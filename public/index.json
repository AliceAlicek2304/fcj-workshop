[
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/1-create-vpc/",
	"title": "Create VPC &amp; Network",
	"tags": [],
	"description": "",
	"content": "Create VPC and Network Infrastructure In this step, we will set up the Virtual Private Cloud (VPC) where our application resources will reside. We will create public subnets for internet-facing resources (like load balancers or NAT gateways) and private subnets for internal resources (like Lambda and RDS).\n1. Create VPC CLI aws ec2 create-vpc \\ --cidr-block 10.10.0.0/16 \\ --tag-specifications \u0026#39;ResourceType=vpc,Tags=[{Key=Name,Value=gametracker-vpc}]\u0026#39; \\ --region ap-southeast-2 AWS Console Open the VPC Dashboard. Click Create VPC. VPC settings: Name tag: gametracker-vpc IPv4 CIDR block: 10.10.0.0/16 Click Create VPC. 2. Create Subnets We will create 2 Public Subnets and 2 Private Subnets across two Availability Zones (AZs) for high availability.\nCLI # Public Subnet 1 (AZ A) aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block 10.10.0.0/24 --availability-zone ap-southeast-2a --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=gametracker-public-1}]\u0026#39; # Public Subnet 2 (AZ B) aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block 10.10.1.0/24 --availability-zone ap-southeast-2b --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=gametracker-public-2}]\u0026#39; # Private Subnet 1 (AZ A) aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block 10.10.2.0/24 --availability-zone ap-southeast-2a --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=gametracker-private-1}]\u0026#39; # Private Subnet 2 (AZ B) aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block 10.10.3.0/24 --availability-zone ap-southeast-2b --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=gametracker-private-2}]\u0026#39; # Enable Auto-assign Public IP for Public Subnets aws ec2 modify-subnet-attribute --subnet-id \u0026lt;SUBNET_PUBLIC_1_ID\u0026gt; --map-public-ip-on-launch aws ec2 modify-subnet-attribute --subnet-id \u0026lt;SUBNET_PUBLIC_2_ID\u0026gt; --map-public-ip-on-launch AWS Console Navigate to Subnets ‚Üí Create subnet. Select your gametracker-vpc. Create the 4 subnets with the CIDRs and AZs listed above. For the Public Subnets: Select the subnet ‚Üí Actions ‚Üí Edit subnet settings ‚Üí Enable Auto-assign public IPv4 address. 3. Internet Gateway CLI # Create IGW aws ec2 create-internet-gateway --tag-specifications \u0026#39;ResourceType=internet-gateway,Tags=[{Key=Name,Value=gametracker-igw}]\u0026#39; # Attach to VPC aws ec2 attach-internet-gateway --internet-gateway-id \u0026lt;IGW_ID\u0026gt; --vpc-id \u0026lt;VPC_ID\u0026gt; AWS Console Navigate to Internet Gateways ‚Üí Create internet gateway. Name: gametracker-igw. Click Create. Select the created IGW ‚Üí Actions ‚Üí Attach to VPC ‚Üí Select gametracker-vpc. 4. Route Tables CLI # Create Public Route Table aws ec2 create-route-table --vpc-id \u0026lt;VPC_ID\u0026gt; --tag-specifications \u0026#39;ResourceType=route-table,Tags=[{Key=Name,Value=public-route-table}]\u0026#39; # Add Route to Internet aws ec2 create-route --route-table-id \u0026lt;RTB_PUBLIC_ID\u0026gt; --destination-cidr-block 0.0.0.0/0 --gateway-id \u0026lt;IGW_ID\u0026gt; # Associate Public Subnets aws ec2 associate-route-table --route-table-id \u0026lt;RTB_PUBLIC_ID\u0026gt; --subnet-id \u0026lt;SUBNET_PUBLIC_1_ID\u0026gt; aws ec2 associate-route-table --route-table-id \u0026lt;RTB_PUBLIC_ID\u0026gt; --subnet-id \u0026lt;SUBNET_PUBLIC_2_ID\u0026gt; # Create Private Route Table aws ec2 create-route-table --vpc-id \u0026lt;VPC_ID\u0026gt; --tag-specifications \u0026#39;ResourceType=route-table,Tags=[{Key=Name,Value=private-route-table-1}]\u0026#39; # Associate Private Subnets aws ec2 associate-route-table --route-table-id \u0026lt;RTB_PRIVATE_ID\u0026gt; --subnet-id \u0026lt;SUBNET_PRIVATE_1_ID\u0026gt; aws ec2 associate-route-table --route-table-id \u0026lt;RTB_PRIVATE_ID\u0026gt; --subnet-id \u0026lt;SUBNET_PRIVATE_2_ID\u0026gt; AWS Console Navigate to Route Tables ‚Üí Create route table. Create public-route-table and private-route-table-1. Public Route Table: Routes ‚Üí Edit routes ‚Üí Add 0.0.0.0/0 targeting gametracker-igw. Subnet associations ‚Üí Edit ‚Üí Select both public subnets. Private Route Table: Subnet associations ‚Üí Edit ‚Üí Select both private subnets. 5. VPC Endpoint for S3 Required for Lambda in private subnets to access S3 without NAT Gateway.\nCLI aws ec2 create-vpc-endpoint \\ --vpc-id \u0026lt;VPC_ID\u0026gt; \\ --service-name com.amazonaws.ap-southeast-2.s3 \\ --route-table-ids \u0026lt;RTB_PUBLIC_ID\u0026gt; \u0026lt;RTB_PRIVATE_ID\u0026gt; AWS Console Navigate to Endpoints ‚Üí Create endpoint. Name: gametracker-s3-endpoint. Service category: AWS services. Service: com.amazonaws.ap-southeast-2.s3 (Gateway type). VPC: gametracker-vpc. Route tables: Select both public and private route tables. Click Create endpoint. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-deploy-app/1-deploy-backend/",
	"title": "Deploy Backend",
	"tags": [],
	"description": "",
	"content": "Deploy Backend (Spring Boot + Lambda) 1. Create ECR Repository CLI aws ecr create-repository \\ --repository-name gametracker-backend \\ --region ap-southeast-2 \\ --image-scanning-configuration scanOnPush=true AWS Console Open ECR Console. Click Create repository. Name: gametracker-backend. Click Create. 2. Build \u0026amp; Push Docker Image Prerequisite: You must have Docker installed and running locally, and AWS CLI configured.\n# 1. Login to ECR aws ecr get-login-password --region ap-southeast-2 | \\ docker login --username AWS --password-stdin \u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.ap-southeast-2.amazonaws.com # 2. Build Spring Boot app (Maven/Gradle) # cd backend # ./mvnw clean package -DskipTests # 3. Build Docker Image docker build -t gametracker-backend . # 4. Tag Image docker tag gametracker-backend:latest \u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.ap-southeast-2.amazonaws.com/gametracker-backend:latest # 5. Push Image docker push \u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.ap-southeast-2.amazonaws.com/gametracker-backend:latest 3. Create Lambda Function CLI aws lambda create-function \\ --function-name gametracker-api \\ --package-type Image \\ --code ImageUri=\u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.ap-southeast-2.amazonaws.com/gametracker-backend:latest \\ --role arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/lambda-execution-role \\ --memory-size 3008 \\ --timeout 60 \\ --region ap-southeast-2 \\ --vpc-config SubnetIds=\u0026lt;SUBNET_PRIVATE_1_ID\u0026gt;,\u0026lt;SUBNET_PRIVATE_2_ID\u0026gt;,SecurityGroupIds=\u0026lt;LAMBDA_SG_ID\u0026gt; \\ --environment Variables=\u0026#39;{ SPRING_PROFILES_ACTIVE=prod, SPRING_DATASOURCE_URL=jdbc:sqlserver://\u0026lt;RDS_ENDPOINT\u0026gt;:1433;databaseName=gametracker, SPRING_DATASOURCE_USERNAME=admin, SPRING_DATASOURCE_PASSWORD=your-password, AWS_S3_BUCKET=gametracker-assets, AWS_S3_REGION=ap-southeast-2 }\u0026#39; AWS Console Open Lambda Console ‚Üí Create function. Select Container image. Name: gametracker-api. Image URI: Select from ECR. Role: Use existing role lambda-execution-role. VPC Settings (Advanced): VPC: gametracker-vpc. Subnets: Private subnets. Security Group: gametracker-lambda-sg. Environment variables: Add SPRING_DATASOURCE_URL, etc. Click Create function. 4. Create API Gateway (HTTP API) CLI # Create API aws apigatewayv2 create-api --name gametracker-api --protocol-type HTTP --target arn:aws:lambda:ap-southeast-2:\u0026lt;ACCOUNT_ID\u0026gt;:function:gametracker-api # Grant permission aws lambda add-permission --function-name gametracker-api --statement-id apigateway --action lambda:InvokeFunction --principal apigateway.amazonaws.com AWS Console API Gateway Console ‚Üí Create API ‚Üí HTTP API (Build). Integrations: Add integration -\u0026gt; Lambda -\u0026gt; gametracker-api. Name: gametracker-api. Stages: $default (Auto-deploy). Click Create. Note the Invoke URL. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Deploy GameTracker on AWS üéØ Workshop Objective In this workshop, we will practice deploying GameTracker‚Äîa full-stack web application‚Äîto AWS completely from scratch. You will build a standard serverless architecture using industry-popular services.\nThe goal is to create a production-like environment in the Sydney (ap-southeast-2) region, covering everything from networking setup to application deployment.\nüèóÔ∏è Architecture Overview The solution adopts a modern cloud-native architecture:\nFrontend: React Single Page Application (SPA) hosted on Amazon S3 and distributed via Amazon CloudFront. Backend: Spring Boot application containerized with Docker, stored in Amazon ECR, and running as serverless on AWS Lambda, exposed via API Gateway. Database: SQL Server Express running on Amazon RDS for managing structured game data. Network: A custom VPC with public/private subnets and security groups for isolation and security. üìã Services Used We will configure the following AWS services:\nCategory Service Networking VPC, Subnets, Internet Gateway, NAT Gateway (Optional), Route53 Compute AWS Lambda, API Gateway (HTTP API) Storage Amazon S3 (Frontend \u0026amp; Assets) Database Amazon RDS (SQL Server) Container Amazon ECR (Elastic Container Registry) CDN \u0026amp; Security Amazon CloudFront, AWS WAF, IAM, Security Groups üöÄ Deployment Steps This workshop is divided into logical stages:\nNetwork Setup: Create VPC, Subnets, and Routing. Database Provisioning: Set up RDS SQL Server. Backend Deployment: Create ECR Repository. Build and push Docker image. Create Lambda function and connect to the database. Expose backend via API Gateway. Frontend Deployment: Create S3 buckets. Set up CloudFront distribution. Deploy React application. Cleanup: Instructions to remove resources to avoid costs. üõ†Ô∏è Prerequisites Before starting, ensuring you have the following:\nAWS Account: Active account with admin permissions. Tools Installed: AWS CLI (Configured with aws configure). Docker Desktop. Node.js and Java 17. Source Code: Clone the workshop repository: git clone https://github.com/your-repo/gametracker-workshop.git cd gametracker-workshop Note: Some services like RDS and NAT Gateway charge hourly rates. Please follow the Cleanup section immediately after completing the workshop to avoid unexpected charges.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Part I: Introduction to Kiro.dev (AWS Agentic IDE) Kiro.dev (pronounced ‚Äúkeer-oh‚Äù) is an agentic AI-powered IDE (Integrated Development Environment) developed by a small team at Amazon Web Services (AWS). It is currently in public preview.\n1. Goal \u0026amp; Key Features Goal: Bridge the gap between \u0026ldquo;vibe coding\u0026rdquo; (rapid prototyping) and structured, tested, documented, and maintainable real-world software development. Key Features: TaSpec-Driven Development: Converts requirements into user stories, acceptance criteria, design documents, and task lists. Agent Hooks / Automation: Automatically triggers tasks like documentation generation, testing, and optimization upon events (file save, commit). Steering: Uses Markdown files to clearly define project structure, standards, and architecture. Multi-file Capability: Understands functional goals and executes necessary changes across multiple files. 2. Pros and Cons Type Details Pros Increased transparency \u0026amp; control (spec review before code), Reduced boilerplate burden, Security \u0026amp; privacy (local code generation), Flexibility (external tool integration via Model Context Protocol). Cons Still in preview (incomplete features), May struggle with \u0026ldquo;context understanding\u0026rdquo; in complex projects (requires user supervision), Costs associated with agent interaction. 3. Usage Recommendations Kiro is suitable if you want to maintain a professional, structured workflow while leveraging AI for rapid prototyping, or if you want AI to be a \u0026ldquo;coding partner\u0026rdquo; rather than just a code suggestion tool.\nPart II: Important Notes on Effective AI Usage Here are core experiences and principles when working with AI in software development projects:\n1. Control Principles User must be in control: AI is just an assistant; do not rely on it to control the project. Plan First \u0026amp; Review Frequently: You MUST CREATE A PLAN FIRST (e.g., ask AI to create a plan file) to have a working framework and must frequently review (don\u0026rsquo;t expect it to do everything). Code Manager Role: Your value lies in code validation. You must be the one managing the code generated by AI. 2. Prompt Engineering \u0026amp; Requirement Definition Specific Role: Assign a specific role to the AI (e.g., suggest using Amazon Q/CodeWhisperer). Prompt Templates: Ask AI to create a plan, then filter and refine it. Request it to export files for storage and direct editing later. Clear Definition: Create a new section when writing user stories. Define technology clearly for AI to follow. Instead of saying \u0026ldquo;don\u0026rsquo;t implement this,\u0026rdquo; say \u0026ldquo;implement this part\u0026rdquo; to increase success probability. Detailed Collaboration: Collaborate with AI to create clear requirements and detailed information before asking AI to design and proceed. 3. Project Management Break Down Scope: Divide large scope into scope units; each unit is a small project for easier implementation. Collaborate \u0026amp; Verify: Ask AI to write user stories, then base project time estimation on them. Must have a team working together and verify all output code. Basic Process: Requirement $\\implies$ Unit $\\implies$ Data Model $\\implies$ \u0026hellip; $\\implies$ Verify. Part III: Software Development Process Diagram (Agile/Scrum) The software development process typically follows Agile/Scrum, involving roles such as Product Owner, Developer, and Tester.\n1. Development Cycle Starts from the Backlog (list of requirements), requirements are prioritized and assigned to each Sprint. In each Sprint, the development team performs key activities: Design, Code, and Test. 2. Deliverables \u0026amp; Deployment Environments Artifacts: Design documents, content maps, service interfaces, source code, configurations. Deployment Environments: Products are sequentially deployed through environments: Development (Dev) Testing (QA) User Acceptance Testing (UAT) Production (Prod) ‚Äì Official running environment. This process ensures the relationship between roles, activities, results, and deployment environments, helping control product quality before release to users.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "AWS CLOUD MASTERY SERIES #1 EVENT SUMMARY (15/11/2025) PART I: GENERATIVE AI WITH AMAZON BEDROCK 1. Foundation Model and Prompt Engineering The event posed a key question: What knowledge should be learned to fit the external cloud environment?\nFoundation Model: Definition of foundation models. Prompt Engineering Techniques: Techniques to optimize input for AI models. Few-shot Prompting: Providing a few specific examples to guide the model. Chain of Thought (CoT): Asking the model to show reasoning steps to arrive at the final answer, helping to increase accuracy. 2. Retrieval Augmented Generation (RAG) The event delved into RAG and the concept of Embedding.\nWhat is Embedding? (Concept introduced). Tools supporting RAG: Amazon Titan Embedding (tool introduced). RAG in Action: Illustration of the RAG workflow, including steps: User Query Embedding Model (converts question into vector) Vector Store / Knowledge Source (searches for relevant information) Prompt Template / Prompt Alignment Model (inserts found information into prompt) Large Language Model (LLM) Response (answer supplemented with external knowledge). RetrieveAndGenerate API: An API introduced to implement RAG. 3. Amazon Bedrock AgentCore Amazon Bedrock AgentCore is a platform that helps realize your AI applications by taking AI agents from the experimental stage to production.\nFunction: Strong support for runtime, memory, tools, security, and monitoring for agents. PART II: OTHER PRETRAINED AI SERVICES The event also introduced a range of specialized, pre-trained AI services from Amazon, along with their corresponding costs:\nAI Service Main Function Reference Price Amazon Rekognition Image and video analysis, labeling, automatic sensitive content censoring. $0.0013/image (\u0026lt;1M images) Amazon Translate Real-time text recognition and translation. $15/1M characters Amazon Textract Extract texts and layouts from documents. $0.05/page (\u0026lt;1M pages) ‚üπ needs consideration. Amazon Transcribe Speech to text. Supports sensitive word censoring, voice recognition, and automatic translation. $0.024/minute (\u0026lt;250k minutes) Amazon Polly Text-to-speech. $4 per 1M characters (first 1M free). Amazon Comprehend Natural Language Processing (NLP). Understands text, video, keyword filtering. $0.0001/100 characters + $0.35/1h Amazon Kendra SEARCH function (or question answering). $30/index/month + $0.35/1h (extremely expensive) Amazon Personalize Personalize user experience (like TikTok, Shopee, Facebook). $0.24/training hour + $0.05/GB/recommendation Amazon Lookout Family Includes Lookout for Equipment and Lookout for Vision (for monitoring and anomaly detection). (No specific price in notes) Pipecat (Introduced, no functional details). (No specific price in notes) "
},
{
	"uri": "http://localhost:1313/fcj-workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguy·ªÖn VƒÉn C∆∞·ªùng\nPhone Number: 0349079940\nEmail: cuongnvse183645@fpt.edu.vn\nUniversity: FPT university HCM\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: No. Task Start Date Completion Date Reference Material 1 Learn about AWS Free Tier, create AWS account 08/09/2025 08/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Billing Dashboard, create Budget and cost alerts 09/09/2025 09/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Study IAM: User, Group, Role, Policy 10/09/2025 10/09/2025 AWS Docs, AWS Training, FCJ Playlist 4 Configure MFA for root and user accounts 11/09/2025 11/09/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create 2 IAM users (Dev \u0026amp; Admin) with different permissions 12/09/2025 12/09/2025 AWS Docs, AWS Training, FCJ Playlist 6 Launch EC2 instance (Linux) 15/09/2025 15/09/2025 AWS Docs, AWS Training, FCJ Playlist Week 1 Achievements: Learned about AWS Free Tier and successfully created an AWS account.\nConfigured Billing Dashboard, set up Budget and cost alerts to monitor expenses.\nStudied IAM concepts: User, Group, Role, Policy.\nConfigured MFA for both root and user accounts to enhance security.\nCompleted a mini project: Created 2 IAM users (Dev \u0026amp; Admin) with different permissions.\nLaunched an EC2 instance (Linux) and practiced basic management tasks.\nBecame familiar with the AWS Management Console and AWS CLI for managing resources.\nDeveloped the ability to connect and manage AWS resources using both the web interface and CLI in parallel.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand AWS service groups (Compute, Storage, Networking, Database). Learn to use AWS Console and AWS CLI for managing resources. Practice EC2 and EBS setup and SSH connection. Tasks to be carried out this week: No. Task Start Date Completion Date Reference Material 1 Get acquainted with FCJ members; read and note internship regulations. 11/08/2025 11/08/2025 AWS Docs, AWS Training, FCJ Playlist 2 Learn about AWS and its types of services (Compute, Storage, Networking, Database, etc.) 12/08/2025 12/08/2025 AWS Docs, AWS Training, FCJ Playlist 3 Create AWS Free Tier account; learn AWS Console and CLI setup. 13/08/2025 13/08/2025 AWS Docs, AWS Training, FCJ Playlist 4 Study basic EC2: instance types, AMI, EBS, Elastic IP, and SSH connection methods. 14/08/2025 15/08/2025 AWS Docs, AWS Training, FCJ Playlist 5 Practice launching EC2 instance, connecting via SSH, and attaching EBS volume. 15/08/2025 15/08/2025 AWS Docs, AWS Training, FCJ Playlist Week 2 Achievements: Understood what AWS is and mastered core service groups (Compute, Storage, Networking, Database, \u0026hellip;). Created and configured AWS Free Tier account successfully. Learned how to navigate AWS Management Console and locate services efficiently. Installed and configured AWS CLI with Access Key, Secret Key, and Default Region. Practiced using AWS CLI for: Checking account \u0026amp; configuration Listing regions Viewing EC2 services Creating and managing key pairs Viewing active resources Gained the ability to manage AWS resources in parallel via Console and CLI. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Continue building foundational AWS knowledge. Practice with IAM, EC2, and basic networking setup. Begin deploying a simple website on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Explore AWS Free Tier and create an AWS account 08/09/2025 08/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Billing Dashboard, create Budget \u0026amp; Cost Alerts 09/09/2025 09/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Learn IAM: User, Group, Role, Policy 10/09/2025 10/09/2025 AWS Docs, AWS Training, FCJ Playlist 4 Enable MFA for Root and User accounts 11/09/2025 11/09/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create 2 IAM Users (Dev \u0026amp; Admin) with different permissions 12/09/2025 12/09/2025 AWS Docs, AWS Training, FCJ Playlist 6 Create an EC2 instance (Linux) 15/09/2025 15/09/2025 AWS Docs, AWS Training, FCJ Playlist 7 SSH into EC2 and install Apache/Nginx 16/09/2025 16/09/2025 AWS Docs, AWS Training, FCJ Playlist 8 Learn Security Group, assign Elastic IP 17/09/2025 17/09/2025 AWS Docs, AWS Training, FCJ Playlist 9 Deploy a small application on EC2 18/09/2025 18/09/2025 AWS Docs, AWS Training, FCJ Playlist 10 Mini Project: Deploy a static website on EC2 19/09/2025 19/09/2025 AWS Docs, AWS Training, FCJ Playlist Week 3 Achievements: Understood IAM structure and how to manage users, groups, and roles. Configured MFA and budget alerts for account security and cost control. Created, launched, and connected to EC2 instances. Learned to manage Elastic IPs and security groups. Successfully deployed a static website on EC2. Practiced hands-on with AWS Console and CLI. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand AWS networking concepts: VPC, Subnet, Internet Gateway, Route Table. Learn the difference between Security Groups and NACLs. Deploy basic Load Balancer (ALB) and EC2 instances inside a custom VPC. Complete a mini project combining VPC, EC2, and ALB. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Launch a custom VPC and create Subnets 29/09/2025 29/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Internet Gateway and Route Table for the VPC 30/09/2025 30/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Learn the difference between Security Groups and NACLs 01/10/2025 01/10/2025 AWS Docs, AWS Training, FCJ Playlist 4 Deploy a basic Application Load Balancer (ALB) 02/10/2025 02/10/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create a private VPC and deploy 2 EC2 instances behind the ALB 03/10/2025 03/10/2025 AWS Docs, AWS Training, FCJ Playlist Week 4 Achievements: Successfully created a custom VPC and configured Subnets.\nConfigured Internet Gateway and Route Tables to allow proper routing in the VPC.\nLearned the differences between Security Groups (stateful) and NACLs (stateless) and applied them.\nDeployed a basic Application Load Balancer (ALB) to distribute traffic to multiple EC2 instances.\nCompleted mini project: created a private VPC and deployed 2 EC2 instances behind the ALB, gaining hands-on experience in networking and load balancing.\nBecame more confident in managing AWS networking resources and deploying a simple high-availability architecture.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn advanced AWS networking concepts and practice with Security Groups. Understand Elastic Load Balancer (ALB) and Auto Scaling basics. Deploy a mini project combining EC2, VPC, ALB, and Security Groups. Practice monitoring and basic troubleshooting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review VPC, Subnet, and Route Table configuration 06/10/2025 06/10/2025 AWS Docs, FCJ Playlist 2 Learn and configure Security Groups for EC2 instances 07/10/2025 07/10/2025 AWS Docs, FCJ Playlist 3 Deploy Application Load Balancer (ALB) and connect EC2 instances 08/10/2025 08/10/2025 AWS Docs, FCJ Playlist 4 Understand and practice Auto Scaling Groups 09/10/2025 09/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy 2 EC2 instances behind an ALB with proper Security Groups and basic Auto Scaling 10/10/2025 10/10/2025 AWS Docs, FCJ Playlist Week 5 Achievements: Reviewed VPC, Subnet, and Route Table setups and ensured proper connectivity.\nConfigured Security Groups to allow necessary traffic while keeping instances secure.\nSuccessfully deployed Application Load Balancer (ALB) and connected EC2 instances.\nLearned the basics of Auto Scaling Groups and applied them to EC2 instances.\nCompleted mini project: 2 EC2 instances deployed behind ALB with Security Groups, gaining hands-on experience with networking, load balancing, and scaling.\nImproved skills in monitoring and troubleshooting basic AWS networking issues.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn VPC, Subnet, and Route Table concepts for EC2 networking. Practice configuring Security Groups and Elastic IPs. Deploy EC2 instances in a VPC with proper networking and access. Complete a mini project integrating EC2, Security Groups, and Elastic IP. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review VPC, Subnet, and Route Table concepts 13/10/2025 13/10/2025 AWS Docs, FCJ Playlist 2 Configure Security Groups and test inbound/outbound rules 14/10/2025 14/10/2025 AWS Docs, FCJ Playlist 3 Assign Elastic IPs to EC2 instances and test connectivity 15/10/2025 15/10/2025 AWS Docs, FCJ Playlist 4 Launch EC2 instances within the VPC, assign Subnets, Security Groups, and Elastic IP 16/10/2025 16/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy a basic web application on EC2 instances with proper networking settings 17/10/2025 17/10/2025 AWS Docs, FCJ Playlist Week 6 Achievements: Reviewed and understood VPC, Subnet, and Route Table configurations.\nConfigured Security Groups correctly to allow necessary traffic and secure EC2 instances.\nAssigned Elastic IPs to EC2 instances and verified public accessibility.\nLaunched EC2 instances inside the VPC with correct networking, Security Groups, and Elastic IP setup.\nCompleted mini project: deployed a basic web application on EC2 with proper networking and security.\nStrengthened hands-on skills with EC2 networking, access control, and public IP management.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn AWS S3 concepts: buckets, objects, and storage classes. Practice creating S3 buckets, uploading files, and setting permissions. Understand S3 bucket policies and public/private access. Complete a mini project: deploy static files to S3 and test access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review S3 concepts: bucket, object, storage classes 20/10/2025 20/10/2025 AWS Docs, FCJ Playlist 2 Create S3 buckets and practice uploading files 21/10/2025 21/10/2025 AWS Docs, FCJ Playlist 3 Configure bucket permissions and test public/private access 22/10/2025 22/10/2025 AWS Docs, FCJ Playlist 4 Apply S3 bucket policies for specific access control 23/10/2025 23/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy static website files to S3 and test access 24/10/2025 24/10/2025 AWS Docs, FCJ Playlist Week 7 Achievements: Learned S3 storage concepts and usage.\nSuccessfully created S3 buckets, uploaded files, and organized objects.\nApplied bucket permissions and policies for secure and controlled access.\nCompleted mini project: deployed static files to S3 and verified accessibility.\nEnhanced hands-on skills with AWS S3, file management, and access control.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Learn advanced AWS S3 features: versioning, lifecycle, and bucket policies. Practice setting up S3 bucket policies and permissions. Understand S3 event notifications and integration with Lambda. Introduction to CloudFront for content delivery. Complete a mini project: host a static website on S3 + CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review basic S3 and upload files 27/10/2025 27/10/2025 AWS Docs, FCJ Playlist 2 Enable versioning on S3 buckets and test file versioning 28/10/2025 28/10/2025 AWS Docs, FCJ Playlist 3 Configure bucket policies and permissions 29/10/2025 29/10/2025 AWS Docs, FCJ Playlist 4 Learn about S3 event notifications and trigger Lambda functions 30/10/2025 30/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy a static website on S3 and distribute via CloudFront 31/10/2025 31/10/2025 AWS Docs, FCJ Playlist Week 8 Achievements: Understood advanced S3 features including versioning, lifecycle rules, and policies.\nSuccessfully configured bucket policies for secure and controlled access.\nPracticed triggering Lambda functions via S3 event notifications.\nDeployed a static website on S3 and distributed it via CloudFront for faster content delivery.\nImproved hands-on skills with AWS S3, security policies, version control, and content delivery.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn about AWS RDS (Relational Database Service) and database management. Understand different database engines: MySQL, PostgreSQL, SQL Server. Practice creating RDS instances and connecting from local/EC2. Learn backup, snapshot, and restore methods in RDS. Understand security best practices for database access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review basic database concepts and AWS RDS documentation 03/11/2025 03/11/2025 AWS Docs, FCJ Playlist 2 Create RDS instances (MySQL \u0026amp; PostgreSQL) 04/11/2025 04/11/2025 AWS Docs, FCJ Playlist 3 Connect RDS instances from local machine and EC2 05/11/2025 05/11/2025 AWS Docs, FCJ Playlist 4 Configure automatic backups and manual snapshots 06/11/2025 06/11/2025 AWS Docs, FCJ Playlist 5 Test restore from snapshot and review database security \u0026amp; parameter groups 07/11/2025 07/11/2025 AWS Docs, FCJ Playlist Week 9 Achievements: Understood AWS RDS and its advantages for managed databases.\nSuccessfully created MySQL and PostgreSQL RDS instances.\nConnected to RDS instances from local machines and EC2 instances.\nConfigured automatic backups, created snapshots, and tested restore procedures.\nLearned best practices for database security including IAM roles, security groups, and password management.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Learn about AWS Free Tier, Billing Dashboard, IAM, MFA, EC2 and complete a mini IAM project\nWeek 2: Get familiar with AWS CLI, IAM roles, and basic S3 operations\nWeek 3: Learn EC2 in detail: Instance types, AMI, EBS, Security Groups, and SSH connection\nWeek 4: Practice launching EC2 instances, attaching EBS, and connecting via SSH\nWeek 5: Understand AWS networking: VPC, Subnets, Internet Gateway, and Elastic IPs\nWeek 6: Practice configuring VPC, subnet, security group rules, and EC2 networking setup\nWeek 7: Explore RDS, DynamoDB, and basic database configuration on AWS\nWeek 8: Learn S3 advanced features: versioning, lifecycle rules, bucket policies, and static website hosting\nWeek 9: Connect with FCJ members, review AWS basics, create AWS account, practice EC2 launch and EBS attachment\nWeek 10: Consolidate AWS skills: EC2, EBS, CLI commands, and Management Console usage\nWeek 11: Practice advanced AWS CLI commands, automate EC2 creation, and integrate basic services\nWeek 12: Learn CloudFormation, deploy EC2 + S3 stacks, integrate Lambda \u0026amp; API Gateway, test automation, and finalize internship report\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/2-create-iam/",
	"title": "Create IAM Roles",
	"tags": [],
	"description": "",
	"content": "Create IAM Roles We need to create an IAM Role for our Lambda function. This role gives Lambda permission to access other AWS services like S3 (for assets), SES (for emails), and RDS (for data).\n1. Create Lambda Execution Role CLI # 1. Create trust policy file echo \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }\u0026#39; \u0026gt; lambda-trust-policy.json # 2. Create the role aws iam create-role \\ --role-name lambda-execution-role \\ --assume-role-policy-document file://lambda-trust-policy.json # 3. Attach standard policies aws iam attach-role-policy \\ --role-name lambda-execution-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole aws iam attach-role-policy \\ --role-name lambda-execution-role \\ --policy-arn arn:aws:iam::aws:policy/CloudWatchLogsFullAccess AWS Console Open the IAM Console. Go to Roles ‚Üí Create role. Trusted entity type: AWS service. Service: Lambda. Click Next. Add permissions: Search and select: AWSLambdaVPCAccessExecutionRole CloudWatchLogsFullAccess Click Next. Role name: lambda-execution-role. Click Create role. 2. Add Custom Permissions We need to give Lambda specific access to our S3 buckets and RDS.\nCLI # 1. Create policy document echo \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::gametracker-assets/*\u0026#34;, \u0026#34;arn:aws:s3:::gametracker-assets\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rds:DescribeDBInstances\u0026#34;, \u0026#34;rds:DescribeDBProxies\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] }\u0026#39; \u0026gt; lambda-custom-policy.json # 2. Attach inline policy aws iam put-role-policy \\ --role-name lambda-execution-role \\ --policy-name lambda-custom-permissions \\ --policy-document file://lambda-custom-policy.json AWS Console Go to Roles and select lambda-execution-role. In the Permissions tab, click Add permissions ‚Üí Create inline policy. Select JSON editor and paste the policy JSON above. Click Next. Policy name: lambda-custom-permissions. Click Create policy. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-deploy-app/2-deploy-frontend/",
	"title": "Deploy Frontend",
	"tags": [],
	"description": "",
	"content": "Deploy Frontend (React + S3 + CloudFront) 1. Create S3 Buckets We need two buckets: one for the frontend code and one for game assets.\nCLI # Frontend Bucket aws s3 mb s3://gametracker-frontend --region ap-southeast-2 # Assets Bucket aws s3 mb s3://gametracker-assets --region ap-southeast-2 AWS Console S3 Console ‚Üí Create bucket. Create gametracker-frontend (disable public access if using CloudFront OAC). Create gametracker-assets (configure CORS as needed). 2. Create CloudFront Distribution This ensures fast delivery and HTTPS.\nAWS Console CloudFront Console ‚Üí Create distribution. Origin domain: Select gametracker-frontend.s3.... Origin access: Select Origin access control settings (recommended) -\u0026gt; Create control setting. Important: You must update the S3 bucket policy to allow CloudFront access (copy the policy AWS provides after creation). Viewer protocol policy: Redirect HTTP to HTTPS. Default root object: index.html. Error pages: Create custom error response for 403 and 404 -\u0026gt; Path /index.html -\u0026gt; Status 200. This is crucial for SPA routing to work. Click Create. 3. Configure and Build React App Before building, you must point the Frontend to your new API Gateway.\nOpen the frontend folder in your code editor. Find the configuration file (usually .env or src/config.js). Update the API_URL to your API Gateway Invoke URL (from step 5.4.1). // Example .env REACT_APP_API_URL=https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-2.amazonaws.com Build the application: npm install npm run build 4. Deploy to S3 Upload the build artifacts to your S3 bucket.\n# Sync build folder to S3 aws s3 sync build/ s3://gametracker-frontend # Invalidate Cache (to see changes immediately) aws cloudfront create-invalidation --distribution-id \u0026lt;DISTRIBUTION_ID\u0026gt; --paths \u0026#34;/*\u0026#34; üéâ Success! Your GameTracker application is now live at the CloudFront Domain Name! Access it via the browser to test.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Prerequisites Before deploying the GameTracker application, we need to set up the foundational infrastructure. This includes the network layer (VPC), security configurations (IAM Roles, Security Groups), and necessary permissions.\nIn this section, we will:\nCreate Network Infrastructure: VPC, Subnets, Internet Gateway, and Route Tables. Create IAM Roles: Execution roles for AWS Lambda. Create Security Groups: Firewall rules for our components. These steps ensure a secure and isolated environment for our application.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Context The gaming industry is rapidly growing, with players seeking deeper engagement through data tracking and simulation tools. GameTracker is a cloud-native web application designed to empower players and administrators to manage, track, and share simplified information about game entities (characters, weapons, banners, events). The solution addresses the fragmentation of game data across wikis and spreadsheets by providing a centralized, localized platform.\nCore Features (MVP)\nGame Information Hub: Centralized database for Character, Weapon, and Item attributes. Gacha Simulator: Realistic simulation of entry/wish systems to test drop rates. Gacha History Tracker: Tools to import pull history and analyze luck/pity. Event Timeline: Interactive roadmap of past, current, and upcoming events. In Development\nReal-time Resource Tracking: Integration for real-time in-game currency/energy checks. Notification System: Alerts for resource overflow or event endings. 1.2 PROJECT SUCCESS CRITERIA Performance: API latency \u0026lt; 200ms for 95% of requests. Availability: 99.9% uptime during the first month of operation. Cost Efficiency: Monthly AWS infrastructure cost remains under $150. 1.3 ASSUMPTIONS Access: Development team has full administrative access to the AWS account. Third-Party Services: Google OAuth2 services and Game APIs remain available and stable. Data Availability: Game asset data (images, stats) can be manually sourced or imported without strict copyright blockers for educational/workshop purposes. Budget: The project operates within the AWS Free Tier limitations where possible, with a budget cap for RDS and NAT Gateway. 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM The solution utilizes a serverless-first approach on AWS to ensure scalability and low maintenance.\nComponents:\nFrontend: React (Vite) hosted on S3 + CloudFront. Backend: Spring Boot on AWS Lambda + API Gateway. Database: SQL Server on Amazon RDS. Security: AWS WAF, IAM, Security Groups. 2.2 TECHNICAL PLAN We will implement the solution using industry-standard DevOps practices:\nFrontend: Developed with React and TypeScript. deployed to S3/CloudFront. Backend: Spring Boot 3 Java application, containerized with Docker, pushed to ECR, and deployed to Lambda. Infrastructure: Managed via AWS Console and CLI scripts for reproducibility. Testing: Unit testing for backend logic and manual UAT for frontend flows. 2.3 PROJECT PLAN The project follows a 4-week Agile timeline:\nWeek 1: Analysis, DB Schema Design, VPC Setup. Week 2: Backend API Development \u0026amp; Database Connectivity. Week 3: Frontend UI Implementation \u0026amp; Gacha Logic. Week 4: Integration, CI/CD Pipeline, Deployment, and Documentation. 2.4 SECURITY CONSIDERATIONS Authentication: Stateless authentication using JWT and Google OAuth2. Network Security: VPC with Public/Private subnets; Database in private subnet; WAF protecting CloudFront. Encryption: TLS 1.2+ for data in transit; RDS instances encrypted at rest. Access Control: Least Privilege IAM roles for Lambda functions. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Phase Activities Deliverables Setup VPC creation, IAM setup, Git repo init AWS Environment, Architecture Doc Development Coding API endpoints, UI, DB migration Source Code, Docker Images Deployment S3 sync, Lambda deployment, CloudFront config Live URL, API Endpoint 3.2 OUT OF SCOPE Mobile App: Native iOS/Android versions are not included. Multiplayer: Real-time multiplayer game servers or lobbies. Payment Processing: No integration with payment gateways for real money. 3.3 PATH TO PRODUCTION Scalability: Architecture supports 0-10,000 users via Lambda auto-scaling. Monitoring: Amazon CloudWatch Dashboards for error rates and latency. Backup: Automated RDS backups and S3 versioning enabled for disaster recovery. 4. EXPECTED AWS COST BREAKDOWN BY SERVICES Estimated Monthly Cost: ~$123.00\nService Estimated Usage Cost (Approx) RDS (SQL Server) db.t3.micro (Single AZ) ~$60.00 NAT Gateway 1 Unit (if required) ~$32.00 AWS WAF Web ACL + Request fees ~$10.00 CloudFront 100GB Data Out ~$8.50 AWS Lambda 1M Invocations ~$5.00 Other S3, logs, Route53 ~$7.50 5. TEAM Name Role Email Responsibility Nguyen Van Cuong Full Stack Developer cuongnvse183645@fpt.edu.vn End-to-end Implementation 6. RESOURCES \u0026amp; COST ESTIMATES Labor costs (Educational/Workshop context)\nResource Responsibility Effort Estimate Full Stack Developer Design, Code, Deploy ~160 Hours (4 Weeks) Technical Mentor Review, Guidance ~20 Hours Total Estimated Effort: 4 Man-weeks.\n7. ACCEPTANCE The product is accepted upon demonstration of:\nUser Login: Successful Google OAuth2 login. Data Management: CRUD operations for characters/weapons working. Simulation: Gacha simulator functioning with correct logic. Deployment: Application is publicly accessible via HTTPS. Documentation: Complete User Guide and Deployment Guide provided. üîó Project Website: https://trackerplus.site/\rüì• Download Proposal: Proposal-TeamOne.docx\r"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Learn about AWS S3 (Simple Storage Service) and CloudFront (CDN) services. Understand S3 bucket creation, permissions, and storage classes. Practice uploading files to S3 and configuring public/private access. Learn about CloudFront distributions to deliver content globally. Understand versioning, lifecycle policies, and basic security best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review S3 concepts: bucket, object, storage classes, permissions 10/11/2025 10/11/2025 AWS Docs, FCJ Playlist 2 Create S3 buckets and upload test files 11/11/2025 11/11/2025 AWS Docs, FCJ Playlist 3 Configure S3 permissions, public/private access, and versioning 12/11/2025 12/11/2025 AWS Docs, FCJ Playlist 4 Set up CloudFront distribution with S3 as origin 13/11/2025 13/11/2025 AWS Docs, FCJ Playlist 5 Test global content delivery and implement lifecycle policies \u0026amp; security best practices 14/11/2025 14/11/2025 AWS Docs, FCJ Playlist Week 10 Achievements: Understood AWS S3 and CloudFront, their use cases, and advantages.\nSuccessfully created S3 buckets and uploaded files with correct permissions.\nConfigured CloudFront distribution to deliver content globally with caching.\nLearned about versioning, lifecycle policies, and basic security best practices for S3.\nGained hands-on experience in managing storage and content delivery on AWS.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Learn about AWS Lambda and serverless computing. Understand how to create Lambda functions, triggers, and manage versions. Learn about API Gateway to expose Lambda functions as HTTP endpoints. Practice integrating Lambda with S3 and other AWS services. Understand basic monitoring and logging with CloudWatch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review serverless concepts and Lambda architecture 17/11/2025 17/11/2025 AWS Docs, FCJ Playlist 2 Create simple Lambda functions and test execution 18/11/2025 18/11/2025 AWS Docs, FCJ Playlist 3 Set up triggers for Lambda (S3 event, API Gateway, CloudWatch event) 19/11/2025 19/11/2025 AWS Docs, FCJ Playlist 4 Configure API Gateway to expose Lambda functions as REST endpoints 20/11/2025 20/11/2025 AWS Docs, FCJ Playlist 5 Monitor Lambda execution with CloudWatch logs and metrics 21/11/2025 21/11/2025 AWS Docs, FCJ Playlist Week 11 Achievements: Understood serverless architecture and AWS Lambda concepts. Created Lambda functions and tested execution successfully. Set up triggers from S3, API Gateway, and CloudWatch events. Exposed Lambda functions via API Gateway endpoints. Monitored Lambda executions with CloudWatch and learned basic troubleshooting. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Review and consolidate all AWS knowledge learned during the internship. Learn basic automation using AWS CloudFormation and Infrastructure as Code (IaC). Deploy simple stacks with CloudFormation. Review Lambda, S3, EC2, and API Gateway integration. Evaluate internship outcomes and prepare final report. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review all AWS services and concepts studied during internship 24/11/2025 24/11/2025 AWS Docs, FCJ materials 2 Learn CloudFormation basics and template syntax 25/11/2025 25/11/2025 AWS Docs, FCJ Playlist 3 Deploy a simple CloudFormation stack to create EC2 + S3 resources 26/11/2025 26/11/2025 AWS Docs, FCJ Playlist 4 Integrate Lambda and API Gateway into CloudFormation stack 27/11/2025 27/11/2025 AWS Docs, FCJ Playlist 5 Test automation, troubleshoot, and prepare summary of all accomplishments 28/11/2025 28/11/2025 AWS Docs, FCJ Playlist Week 12 Achievements: Consolidated AWS knowledge and practical skills gained during internship. Learned basic automation and Infrastructure as Code (IaC) using CloudFormation. Deployed a CloudFormation stack with EC2, S3, Lambda, and API Gateway. Tested automation, monitored resources, and practiced troubleshooting. Completed internship report and evaluation. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-create-rds/",
	"title": "Create RDS Database",
	"tags": [],
	"description": "",
	"content": "Create RDS SQL Server In this step, we will create the Microsoft SQL Server database instance that will store our game data. We will use the SQL Server Express edition which is eligible for the AWS Free Tier.\n1. Create RDS Instance CLI # Create RDS Instance aws rds create-db-instance \\ --db-instance-identifier gametracker-mssql \\ --db-instance-class db.t3.micro \\ --engine sqlserver-ex \\ --master-username admin \\ --master-user-password YourSecurePassword123 \\ --allocated-storage 20 \\ --vpc-security-group-ids \u0026lt;RDS_SG_ID\u0026gt; \\ --db-subnet-group-name gametracker-db-subnet-group \\ --backup-retention-period 7 \\ --no-publicly-accessible \\ --region ap-southeast-2 AWS Console Open the RDS Console. Click Create database. Choose a database creation method: Standard create. Engine options: Microsoft SQL Server. Edition: SQL Server Express Edition. Templates: Select Free tier. Settings: DB instance identifier: gametracker-mssql. Master username: admin (or your preferred username). Master password: Enter a strong password. Instance configuration: db.t3.micro. Storage: 20 GiB (General Purpose SSD gp2/gp3). Connectivity: VPC: gametracker-vpc. Subnet group: gametracker-db-subnet-group. Public access: No. VPC security group: Choose existing -\u0026gt; gametracker-rds-sg. Additional configuration: Initial database name: (SQL Server doesn\u0026rsquo;t support this via console, we create it later via query). Click Create database. 2. Verify creation Wait for the status to change from Creating to Available.\nEndpoint: Copy the endpoint (e.g., gametracker-mssql.xxxx.ap-southeast-2.rds.amazonaws.com). Port: 1433. We will use these details when deploying the backend application.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/3-create-sg/",
	"title": "Create Security Groups",
	"tags": [],
	"description": "",
	"content": "Create Security Groups Security Groups allow us to control traffic. We need two groups: one for our Compute layer (Lambda) and one for our Database layer (RDS).\n1. Create Lambda Security Group CLI # Create Security Group aws ec2 create-security-group \\ --group-name gametracker-lambda-sg \\ --description \u0026#34;SG for Lambda functions\u0026#34; \\ --vpc-id \u0026lt;VPC_ID\u0026gt; # Note: By default, SGs allow all outbound traffic, which is what we want for Lambda. AWS Console Navigate to EC2 Dashboard ‚Üí Security Groups ‚Üí Create security group. Name: gametracker-lambda-sg. Description: SG for Lambda functions. VPC: gametracker-vpc. Inbound rules: None (Lambda doesn\u0026rsquo;t listen to incoming ports directly in this setup). Outbound rules: Allow all traffic (Default). Click Create security group. 2. Create RDS Security Group This group allows the Lambda function to talk to the SQL Server database on port 1433.\nCLI # Create Security Group aws ec2 create-security-group \\ --group-name gametracker-rds-sg \\ --description \u0026#34;SG for RDS\u0026#34; \\ --vpc-id \u0026lt;VPC_ID\u0026gt; # Add Inbound Rule for SQL Server (Port 1433) from Lambda SG aws ec2 authorize-security-group-ingress \\ --group-id \u0026lt;RDS_SG_ID\u0026gt; \\ --protocol tcp \\ --port 1433 \\ --source-group \u0026lt;LAMBDA_SG_ID\u0026gt; AWS Console Security Groups ‚Üí Create security group. Name: gametracker-rds-sg. Description: SG for RDS. VPC: gametracker-vpc. Inbound rules: Type: MS SQL. Port: 1433. Source: Custom ‚Üí Select gametracker-lambda-sg (start typing the name to find it). Outbound rules: Allow all traffic (Default). Click Create security group. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/4-create-db-subnetgroup/",
	"title": "Create DB Subnet Group",
	"tags": [],
	"description": "",
	"content": "Create DB Subnet Group The DB Subnet Group tells RDS which subnets it can use to deploy the database instances. For high availability, we must select subnets in at least two different Availability Zones.\n1. Create DB Subnet Group We will use our two Private Subnets for this group to ensure the database is not directly accessible from the internet.\nCLI aws rds create-db-subnet-group \\ --db-subnet-group-name gametracker-db-subnet-group \\ --db-subnet-group-description \u0026#34;Private subnets for gametracker RDS\u0026#34; \\ --subnet-ids \u0026lt;SUBNET_PRIVATE_1_ID\u0026gt; \u0026lt;SUBNET_PRIVATE_2_ID\u0026gt; AWS Console Open the RDS Console. In the navigation pane, click Subnet groups. Click Create DB subnet group. Name: gametracker-db-subnet-group. Description: Private subnets for GameTracker. VPC: Select gametracker-vpc. Add subnets: Availability Zones: Select ap-southeast-2a and ap-southeast-2b. Subnets: Select the CIDRs corresponding to your Private Subnets (e.g., 10.10.2.0/24 and 10.10.3.0/24). Click Create. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-deploy-app/",
	"title": "Deploy Application",
	"tags": [],
	"description": "",
	"content": "Deploy Application Now that our infrastructure is ready (Network, Database, IAM), we can proceed to deploy the application components.\nWe will split this into two parts:\nBackend Deployment: Containerizing the Spring Boot app, pushing to ECR, and running securely on AWS Lambda with API Gateway. Frontend Deployment: Hosting the React SPA on S3 and distributing it globally via CloudFront. Let\u0026rsquo;s start with the Backend.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Introduction to Kiro.dev (AWS Agentic IDE)\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/5-create-nat/",
	"title": "Create NAT Gateway",
	"tags": [],
	"description": "",
	"content": "Create NAT Gateway (Optional) If your Lambda function in the private subnet needs to access the internet (e.g., for installing packages, calling external APIs, or pulling Docker images if endpoints aren\u0026rsquo;t used), you need a NAT Gateway.\nNote: NAT Gateway incurs hourly costs. Only create if necessary or just before deployment.\n1. Allocate Elastic IP (EIP) CLI aws ec2 allocate-address --domain vpc --region ap-southeast-2 # Save the AllocationId AWS Console Navigate to VPC Dashboard ‚Üí Elastic IPs. Click Allocate Elastic IP address. Region: ap-southeast-2. Click Allocate. 2. Create NAT Gateway We will create the NAT Gateway in one of our Public Subnets.\nCLI aws ec2 create-nat-gateway \\ --subnet-id \u0026lt;SUBNET_PUBLIC_1_ID\u0026gt; \\ --allocation-id \u0026lt;EIP_ALLOCATION_ID\u0026gt; \\ --region ap-southeast-2 AWS Console Navigate to NAT Gateways ‚Üí Create NAT gateway. Name: gametracker-nat. Subnet: Select gametracker-public-1. Elastic IP allocation ID: Select the EIP created above. Click Create NAT gateway. 3. Update Private Route Table Limit internet traffic from private subnets to go through the NAT Gateway.\nCLI aws ec2 create-route \\ --route-table-id \u0026lt;RTB_PRIVATE_ID\u0026gt; \\ --destination-cidr-block 0.0.0.0/0 \\ --nat-gateway-id \u0026lt;NAT_GATEWAY_ID\u0026gt; AWS Console Navigate to Route Tables. Select private-route-table-1. Go to Routes tab ‚Üí Edit routes. Add route: Destination: 0.0.0.0/0. Target: Select NAT Gateway ‚Üí gametracker-nat. Click Save changes. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/",
	"title": "Workshop: Deploy GameTracker on AWS",
	"tags": [],
	"description": "",
	"content": "Workshop: Deploy GameTracker on AWS Overview Welcome to the GameTracker Deployment Workshop. In this hands-on session, you will learn how to deploy a complete 3-tier web application on AWS using modern cloud practices.\nYou will move from a local development environment to a production-ready cloud architecture.\nWhat you will build You will deploy GameTracker, a web application that tracks player statistics. The architecture includes:\nFrontend: React Single Page Application (SPA) hosted on Amazon S3 and distributed via Amazon CloudFront. Backend: Spring Boot REST API containerized with Docker, running on AWS Lambda (Serverless). Database: Amazon RDS for SQL Server to store structured game data. Network: A custom VPC with public/private subnets and security groups. Workshop Structure This workshop is divided into the following modules:\nIntroduction: Overview of the project, architecture, and AWS services used. Prerequisites: Setting up the foundation (VPC, IAM, Security Groups, NAT Gateway). Create Database: Provisioning the Amazon RDS SQL Server instance. Deploy Application: Containerizing and deploying the Backend (ECR, Lambda, API Gateway). Hosting and distributing the Frontend (S3, CloudFront). Backup \u0026amp; Restore: Strategies for protecting your data. Cleanup: Removing resources to avoid costs. Prerequisites Before starting, ensure you have:\nAn active AWS Account. AWS CLI installed and configured. Docker installed locally. Java 17+ and Node.js installed (for building the app). Let\u0026rsquo;s get started!\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/6-backup/",
	"title": "Backup &amp; Restore",
	"tags": [],
	"description": "",
	"content": "Backup and Recovery Strategy For the GameTracker application, data is our most critical asset. We need to protect:\nStructured Game Data: Stored in Amazon RDS (SQL Server). Game Assets: Images and files stored in Amazon S3. 1. RDS Backup (SQL Server) Amazon RDS provides two different methods for backing up your DB instances:\nA. Automated Backups When we created the RDS instance, we enabled convenient Automated backups.\nRetention Period: We set it to 7 days. Function: AWS creates a storage volume snapshot of your DB instance, backing up the entire DB instance to S3. Point-in-Time Recovery: You can restore your database to any specific second during your retention period. B. Manual Snapshots You can take a snapshot of your database at any time. Unlike automated backups, manual snapshots are kept until you explicitly delete them.\nSteps to create a Manual Snapshot:\nOpen RDS Console. Go to Databases -\u0026gt; Select gametracker-mssql. Click Actions -\u0026gt; Take snapshot. Info: Snapshot name: gametracker-manual-backup-v1. Click Take snapshot. 2. S3 Data Protection For our gametracker-assets bucket, we should enable Versioning. This allows us to preserve, retrieve, and restore every version of every object stored in the bucket.\nSteps to enable Versioning:\nOpen S3 Console. Select the gametracker-assets bucket. Go to the Properties tab. Under Bucket Versioning, click Edit. Select Enable. Click Save changes. Now, if an admin accidentally deletes or overwrites a character image, you can easily restore the previous version.\n3. Restore Strategy In case of a failure (e.g., accidental data deletion in SQL Server):\nGo to RDS Console -\u0026gt; Snapshots. Select the latest snapshot (Automated or Manual). Click Actions -\u0026gt; Restore snapshot. New DB Instance Identifier: e.g., gametracker-mssql-restore. Once restored and available, update your Lambda Environment Variables (SPRING_DATASOURCE_URL) to point to the new endpoint. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at First Cloud Journey (FCJ) from August 1, 2025 to December 15, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the GameTracker Platform project, developing a serverless solution for the game community. Through this, I improved my skills in AWS Cloud services (Lambda, S3, RDS, WAF), React frontend development, Spring Boot backend, and System Architecture design.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚úÖ ‚òê ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Advanced Troubleshooting: Improve capability to debug complex distributed system issues independently. Communication: Enhance technical communication skills to explain complex architectural decisions more clearly to non-technical stakeholders. Optimization: Deepen understanding of cost optimization and performance tuning for serverless architectures. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/7-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "Cleanup Resources Important: To avoid unexpected charges, you must delete all resources created during this workshop. Follow the steps below in the specified order.\n1. Delete Application Resources Frontend (S3 \u0026amp; CloudFront):\nCloudFront: Disable the distribution -\u0026gt; Wait for it to deploy (stop) -\u0026gt; Delete it. S3 Buckets: Empty gametracker-frontend and gametracker-assets, then delete them. Backend (Lambda \u0026amp; API Gateway \u0026amp; ECR):\nAPI Gateway: Delete the gametracker-api API. Lambda: Delete the gametracker-api function. ECR: Delete the gametracker-backend repository. 2. Delete Database RDS Instance: Go to Amazon RDS \u0026gt; Databases. Select gametracker-mssql. Action \u0026gt; Delete. Uncheck Create final snapshot and confirm deletion. DB Subnet Group: Go to Subnet groups -\u0026gt; Delete gametracker-db-subnet-group. 3. Delete Network Resources NAT Gateway (Expensive!): Go to VPC \u0026gt; NAT Gateways. Delete gametracker-nat. Wait for it to be Deleted. Elastic IP: Go to Elastic IPs -\u0026gt; Release the allocated IP. VPC Endpoint: Go to Endpoints -\u0026gt; Delete gametracker-s3-endpoint. VPC: Go to Your VPCs -\u0026gt; Select gametracker-vpc -\u0026gt; Actions -\u0026gt; Delete VPC. This will automatically delete Subnets, Route Tables, Internet Gateway, and Security Groups associated with it. 4. Verify Check your Billing Dashboard the next day to ensure no active resources remain.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Below is my formal assessment regarding my internship experience with the First Cloud Journey program. I hope these insights contribute to the continuous improvement of the training program.\nDetailed Evaluation 1. Professional Environment\nI appreciate the professional yet open working environment in the office. There is a strong culture of mutual support, where colleagues are always willing to answer technical questions despite their busy schedules.\n2. Training \u0026amp; Mentorship Quality\nI would like to express my special thanks to the Mentors. They have been very patient and provided excellent guidance. Thanks to this, my programming mindset and self-learning skills have improved significantly. The support from the team regarding office procedures and event participation was also excellent.\n3. Practical Applicability\nThis internship helped me systematize the theoretical knowledge from university and apply it to real-world business problems. Being directly involved in the development and deployment process on the Cloud is an invaluable experience, helping me better understand software industry standards.\n4. Skill Development\nBeyond improving technical capabilities (AWS, Docker, CI/CD), I have learned professional industrial working styles. Soft skills such as progress reporting, working in Agile/Scrum teams, and writing standard technical documentation are important assets for my future career.\n5. Company Culture\nA culture of respect and responsibility is what I felt most clearly at FCJ.\nConclusion Most Satisfying: Completing the construction and deployment of a real-world system on AWS with close guidance from Mentors. Recommendation: I will definitely recommend this program to friends, as it is an ideal environment for honing skills and professional work attitude. Acknowledgments I sincerely thank the Company and the FCJ Team for creating the conditions for me to have a meaningful internship. I hope to have the opportunity to continue accompanying and contributing to the company in the future.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/fcj-workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]