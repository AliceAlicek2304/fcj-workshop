[
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/1-create-vpc/",
	"title": "Create VPC &amp; Network",
	"tags": [],
	"description": "",
	"content": "Create VPC and Network Infrastructure In this step, we will set up the Virtual Private Cloud (VPC) where our application resources will reside. We will create public subnets for internet-facing resources (like load balancers or NAT gateways) and private subnets for internal resources (like Lambda and RDS).\n1. Create VPC CLI aws ec2 create-vpc \\ --cidr-block 10.10.0.0/16 \\ --tag-specifications \u0026#39;ResourceType=vpc,Tags=[{Key=Name,Value=gametracker-vpc}]\u0026#39; \\ --region ap-southeast-2 AWS Console Open the VPC Dashboard. Click Create VPC. VPC settings: Name tag: gametracker-vpc IPv4 CIDR block: 10.10.0.0/16 Click Create VPC. 2. Create Subnets We will create 2 Public Subnets and 2 Private Subnets across two Availability Zones (AZs) for high availability.\nCLI # Public Subnet 1 (AZ A) aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block 10.10.0.0/24 --availability-zone ap-southeast-2a --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=gametracker-public-1}]\u0026#39; # Public Subnet 2 (AZ B) aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block 10.10.1.0/24 --availability-zone ap-southeast-2b --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=gametracker-public-2}]\u0026#39; # Private Subnet 1 (AZ A) aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block 10.10.2.0/24 --availability-zone ap-southeast-2a --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=gametracker-private-1}]\u0026#39; # Private Subnet 2 (AZ B) aws ec2 create-subnet --vpc-id \u0026lt;VPC_ID\u0026gt; --cidr-block 10.10.3.0/24 --availability-zone ap-southeast-2b --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=gametracker-private-2}]\u0026#39; # Enable Auto-assign Public IP for Public Subnets aws ec2 modify-subnet-attribute --subnet-id \u0026lt;SUBNET_PUBLIC_1_ID\u0026gt; --map-public-ip-on-launch aws ec2 modify-subnet-attribute --subnet-id \u0026lt;SUBNET_PUBLIC_2_ID\u0026gt; --map-public-ip-on-launch AWS Console Navigate to Subnets ‚Üí Create subnet. Select your gametracker-vpc. Create the 4 subnets with the CIDRs and AZs listed above. For the Public Subnets: Select the subnet ‚Üí Actions ‚Üí Edit subnet settings ‚Üí Enable Auto-assign public IPv4 address. 3. Internet Gateway CLI # Create IGW aws ec2 create-internet-gateway --tag-specifications \u0026#39;ResourceType=internet-gateway,Tags=[{Key=Name,Value=gametracker-igw}]\u0026#39; # Attach to VPC aws ec2 attach-internet-gateway --internet-gateway-id \u0026lt;IGW_ID\u0026gt; --vpc-id \u0026lt;VPC_ID\u0026gt; AWS Console Navigate to Internet Gateways ‚Üí Create internet gateway. Name: gametracker-igw. Click Create. Select the created IGW ‚Üí Actions ‚Üí Attach to VPC ‚Üí Select gametracker-vpc. 4. Route Tables CLI # Create Public Route Table aws ec2 create-route-table --vpc-id \u0026lt;VPC_ID\u0026gt; --tag-specifications \u0026#39;ResourceType=route-table,Tags=[{Key=Name,Value=public-route-table}]\u0026#39; # Add Route to Internet aws ec2 create-route --route-table-id \u0026lt;RTB_PUBLIC_ID\u0026gt; --destination-cidr-block 0.0.0.0/0 --gateway-id \u0026lt;IGW_ID\u0026gt; # Associate Public Subnets aws ec2 associate-route-table --route-table-id \u0026lt;RTB_PUBLIC_ID\u0026gt; --subnet-id \u0026lt;SUBNET_PUBLIC_1_ID\u0026gt; aws ec2 associate-route-table --route-table-id \u0026lt;RTB_PUBLIC_ID\u0026gt; --subnet-id \u0026lt;SUBNET_PUBLIC_2_ID\u0026gt; # Create Private Route Table aws ec2 create-route-table --vpc-id \u0026lt;VPC_ID\u0026gt; --tag-specifications \u0026#39;ResourceType=route-table,Tags=[{Key=Name,Value=private-route-table-1}]\u0026#39; # Associate Private Subnets aws ec2 associate-route-table --route-table-id \u0026lt;RTB_PRIVATE_ID\u0026gt; --subnet-id \u0026lt;SUBNET_PRIVATE_1_ID\u0026gt; aws ec2 associate-route-table --route-table-id \u0026lt;RTB_PRIVATE_ID\u0026gt; --subnet-id \u0026lt;SUBNET_PRIVATE_2_ID\u0026gt; AWS Console Navigate to Route Tables ‚Üí Create route table. Create public-route-table and private-route-table-1. Public Route Table: Routes ‚Üí Edit routes ‚Üí Add 0.0.0.0/0 targeting gametracker-igw. Subnet associations ‚Üí Edit ‚Üí Select both public subnets. Private Route Table: Subnet associations ‚Üí Edit ‚Üí Select both private subnets. 5. VPC Endpoint for S3 Required for Lambda in private subnets to access S3 without NAT Gateway.\nCLI aws ec2 create-vpc-endpoint \\ --vpc-id \u0026lt;VPC_ID\u0026gt; \\ --service-name com.amazonaws.ap-southeast-2.s3 \\ --route-table-ids \u0026lt;RTB_PUBLIC_ID\u0026gt; \u0026lt;RTB_PRIVATE_ID\u0026gt; AWS Console Navigate to Endpoints ‚Üí Create endpoint. Name: gametracker-s3-endpoint. Service category: AWS services. Service: com.amazonaws.ap-southeast-2.s3 (Gateway type). VPC: gametracker-vpc. Route tables: Select both public and private route tables. Click Create endpoint. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-deploy-app/1-deploy-backend/",
	"title": "Deploy Backend",
	"tags": [],
	"description": "",
	"content": "Deploy Backend (Spring Boot + Lambda) 1. Create ECR Repository CLI aws ecr create-repository \\ --repository-name gametracker-backend \\ --region ap-southeast-2 \\ --image-scanning-configuration scanOnPush=true AWS Console Open ECR Console. Click Create repository. Name: gametracker-backend. Click Create. 2. Build \u0026amp; Push Docker Image Prerequisite: You must have Docker installed and running locally, and AWS CLI configured.\n# 1. Login to ECR aws ecr get-login-password --region ap-southeast-2 | \\ docker login --username AWS --password-stdin \u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.ap-southeast-2.amazonaws.com # 2. Build Spring Boot app (Maven/Gradle) # cd backend # ./mvnw clean package -DskipTests # 3. Build Docker Image docker build -t gametracker-backend . # 4. Tag Image docker tag gametracker-backend:latest \u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.ap-southeast-2.amazonaws.com/gametracker-backend:latest # 5. Push Image docker push \u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.ap-southeast-2.amazonaws.com/gametracker-backend:latest 3. Create Lambda Function CLI aws lambda create-function \\ --function-name gametracker-api \\ --package-type Image \\ --code ImageUri=\u0026lt;ACCOUNT_ID\u0026gt;.dkr.ecr.ap-southeast-2.amazonaws.com/gametracker-backend:latest \\ --role arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/lambda-execution-role \\ --memory-size 3008 \\ --timeout 60 \\ --region ap-southeast-2 \\ --vpc-config SubnetIds=\u0026lt;SUBNET_PRIVATE_1_ID\u0026gt;,\u0026lt;SUBNET_PRIVATE_2_ID\u0026gt;,SecurityGroupIds=\u0026lt;LAMBDA_SG_ID\u0026gt; \\ --environment Variables=\u0026#39;{ SPRING_PROFILES_ACTIVE=prod, SPRING_DATASOURCE_URL=jdbc:sqlserver://\u0026lt;RDS_ENDPOINT\u0026gt;:1433;databaseName=gametracker, SPRING_DATASOURCE_USERNAME=admin, SPRING_DATASOURCE_PASSWORD=your-password, AWS_S3_BUCKET=gametracker-assets, AWS_S3_REGION=ap-southeast-2 }\u0026#39; AWS Console Open Lambda Console ‚Üí Create function. Select Container image. Name: gametracker-api. Image URI: Select from ECR. Role: Use existing role lambda-execution-role. VPC Settings (Advanced): VPC: gametracker-vpc. Subnets: Private subnets. Security Group: gametracker-lambda-sg. Environment variables: Add SPRING_DATASOURCE_URL, etc. Click Create function. 4. Create API Gateway (HTTP API) CLI # Create API aws apigatewayv2 create-api --name gametracker-api --protocol-type HTTP --target arn:aws:lambda:ap-southeast-2:\u0026lt;ACCOUNT_ID\u0026gt;:function:gametracker-api # Grant permission aws lambda add-permission --function-name gametracker-api --statement-id apigateway --action lambda:InvokeFunction --principal apigateway.amazonaws.com AWS Console API Gateway Console ‚Üí Create API ‚Üí HTTP API (Build). Integrations: Add integration -\u0026gt; Lambda -\u0026gt; gametracker-api. Name: gametracker-api. Stages: $default (Auto-deploy). Click Create. Note the Invoke URL. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Deploy GameTracker on AWS üéØ Workshop Objective In this workshop, we will manually deploy the GameTracker platform‚Äîa full-stack web application‚Äîto AWS from scratch. You will build a serverless-ready architecture using industry-standard services.\nThe goal is to create a production-like environment in the Sydney (ap-southeast-2) region, covering everything from networking to application deployment.\nüèóÔ∏è Architecture Overview The solution adopts a modern cloud-native architecture:\nFrontend: React Single Page Application (SPA) hosted on Amazon S3 and distributed via Amazon CloudFront. Backend: Spring Boot application containerized with Docker, stored in Amazon ECR, and running serverless on AWS Lambda exposed via API Gateway. Database: SQL Server Express running on Amazon RDS for structured game data. Network: A custom VPC with public/private subnets and security groups to ensure isolation and security. üìã Services Used We will configure the following AWS services:\nCategory Services Networking VPC, Subnets, Internet Gateway, NAT Gateway (optional), Route53 Compute AWS Lambda, API Gateway (HTTP API) Storage Amazon S3 (Frontend \u0026amp; Assets) Database Amazon RDS (SQL Server) Container Amazon ECR (Elastic Container Registry) CDN \u0026amp; Security Amazon CloudFront, AWS WAF, IAM, Security Groups üöÄ Deployment Steps This workshop is divided into logical phases:\nNetwork Setup: Creating VPC, Subnets, and Routing. Database Provisioning: Setting up RDS SQL Server. Backend Deployment: Creating an ECR Repository. Building and pushing the Docker image. Creating the Lambda function and connecting it to the database. Exposing the backend via API Gateway. Frontend Deployment: Creating S3 buckets. Setting up CloudFront distribution. Deploying the React application. Clean up: Instructions on how to decommission resources to avoid costs. ÔøΩÔ∏è Prerequisites An active AWS Account. AWS CLI installed and configured. Docker installed locally. Basic knowledge of terminal commands. Note: Some resources like RDS and NAT Gateway incur hourly costs. Please follow the Cleanup section immediately after finishing the workshop to avoid unexpected charges.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Part I: Introduction to Kiro.dev (AWS Agentic IDE) Kiro.dev (pronounced ‚Äúkeer-oh‚Äù) is an agentic AI-powered IDE (Integrated Development Environment) developed by a small team at Amazon Web Services (AWS). It is currently in public preview.\n1. Goal \u0026amp; Key Features Goal: Bridge the gap between \u0026ldquo;vibe coding\u0026rdquo; (rapid prototyping) and structured, tested, documented, and maintainable real-world software development. Key Features: TaSpec-Driven Development: Converts requirements into user stories, acceptance criteria, design documents, and task lists. Agent Hooks / Automation: Automatically triggers tasks like documentation generation, testing, and optimization upon events (file save, commit). Steering: Uses Markdown files to clearly define project structure, standards, and architecture. Multi-file Capability: Understands functional goals and executes necessary changes across multiple files. 2. Pros and Cons Type Details Pros Increased transparency \u0026amp; control (spec review before code), Reduced boilerplate burden, Security \u0026amp; privacy (local code generation), Flexibility (external tool integration via Model Context Protocol). Cons Still in preview (incomplete features), May struggle with \u0026ldquo;context understanding\u0026rdquo; in complex projects (requires user supervision), Costs associated with agent interaction. 3. Usage Recommendations Kiro is suitable if you want to maintain a professional, structured workflow while leveraging AI for rapid prototyping, or if you want AI to be a \u0026ldquo;coding partner\u0026rdquo; rather than just a code suggestion tool.\nPart II: Important Notes on Effective AI Usage Here are core experiences and principles when working with AI in software development projects:\n1. Control Principles User must be in control: AI is just an assistant; do not rely on it to control the project. Plan First \u0026amp; Review Frequently: You MUST CREATE A PLAN FIRST (e.g., ask AI to create a plan file) to have a working framework and must frequently review (don\u0026rsquo;t expect it to do everything). Code Manager Role: Your value lies in code validation. You must be the one managing the code generated by AI. 2. Prompt Engineering \u0026amp; Requirement Definition Specific Role: Assign a specific role to the AI (e.g., suggest using Amazon Q/CodeWhisperer). Prompt Templates: Ask AI to create a plan, then filter and refine it. Request it to export files for storage and direct editing later. Clear Definition: Create a new section when writing user stories. Define technology clearly for AI to follow. Instead of saying \u0026ldquo;don\u0026rsquo;t implement this,\u0026rdquo; say \u0026ldquo;implement this part\u0026rdquo; to increase success probability. Detailed Collaboration: Collaborate with AI to create clear requirements and detailed information before asking AI to design and proceed. 3. Project Management Break Down Scope: Divide large scope into scope units; each unit is a small project for easier implementation. Collaborate \u0026amp; Verify: Ask AI to write user stories, then base project time estimation on them. Must have a team working together and verify all output code. Basic Process: Requirement $\\implies$ Unit $\\implies$ Data Model $\\implies$ \u0026hellip; $\\implies$ Verify. Part III: Software Development Process Diagram (Agile/Scrum) The software development process typically follows Agile/Scrum, involving roles such as Product Owner, Developer, and Tester.\n1. Development Cycle Starts from the Backlog (list of requirements), requirements are prioritized and assigned to each Sprint. In each Sprint, the development team performs key activities: Design, Code, and Test. 2. Deliverables \u0026amp; Deployment Environments Artifacts: Design documents, content maps, service interfaces, source code, configurations. Deployment Environments: Products are sequentially deployed through environments: Development (Dev) Testing (QA) User Acceptance Testing (UAT) Production (Prod) ‚Äì Official running environment. This process ensures the relationship between roles, activities, results, and deployment environments, helping control product quality before release to users.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "AWS CLOUD MASTERY SERIES #1 EVENT SUMMARY (15/11/2025) PART I: GENERATIVE AI WITH AMAZON BEDROCK 1. Foundation Model and Prompt Engineering The event posed a key question: What knowledge should be learned to fit the external cloud environment?\nFoundation Model: Definition of foundation models. Prompt Engineering Techniques: Techniques to optimize input for AI models. Few-shot Prompting: Providing a few specific examples to guide the model. Chain of Thought (CoT): Asking the model to show reasoning steps to arrive at the final answer, helping to increase accuracy. 2. Retrieval Augmented Generation (RAG) The event delved into RAG and the concept of Embedding.\nWhat is Embedding? (Concept introduced). Tools supporting RAG: Amazon Titan Embedding (tool introduced). RAG in Action: Illustration of the RAG workflow, including steps: User Query Embedding Model (converts question into vector) Vector Store / Knowledge Source (searches for relevant information) Prompt Template / Prompt Alignment Model (inserts found information into prompt) Large Language Model (LLM) Response (answer supplemented with external knowledge). RetrieveAndGenerate API: An API introduced to implement RAG. 3. Amazon Bedrock AgentCore Amazon Bedrock AgentCore is a platform that helps realize your AI applications by taking AI agents from the experimental stage to production.\nFunction: Strong support for runtime, memory, tools, security, and monitoring for agents. PART II: OTHER PRETRAINED AI SERVICES The event also introduced a range of specialized, pre-trained AI services from Amazon, along with their corresponding costs:\nAI Service Main Function Reference Price Amazon Rekognition Image and video analysis, labeling, automatic sensitive content censoring. $0.0013/image (\u0026lt;1M images) Amazon Translate Real-time text recognition and translation. $15/1M characters Amazon Textract Extract texts and layouts from documents. $0.05/page (\u0026lt;1M pages) ‚üπ needs consideration. Amazon Transcribe Speech to text. Supports sensitive word censoring, voice recognition, and automatic translation. $0.024/minute (\u0026lt;250k minutes) Amazon Polly Text-to-speech. $4 per 1M characters (first 1M free). Amazon Comprehend Natural Language Processing (NLP). Understands text, video, keyword filtering. $0.0001/100 characters + $0.35/1h Amazon Kendra SEARCH function (or question answering). $30/index/month + $0.35/1h (extremely expensive) Amazon Personalize Personalize user experience (like TikTok, Shopee, Facebook). $0.24/training hour + $0.05/GB/recommendation Amazon Lookout Family Includes Lookout for Equipment and Lookout for Vision (for monitoring and anomaly detection). (No specific price in notes) Pipecat (Introduced, no functional details). (No specific price in notes) "
},
{
	"uri": "http://localhost:1313/fcj-workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguy·ªÖn VƒÉn C∆∞·ªùng\nPhone Number: 0349079940\nEmail: cuongnvse183645@fpt.edu.vn\nUniversity: FPT university HCM\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: No. Task Start Date Completion Date Reference Material 1 Learn about AWS Free Tier, create AWS account 08/09/2025 08/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Billing Dashboard, create Budget and cost alerts 09/09/2025 09/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Study IAM: User, Group, Role, Policy 10/09/2025 10/09/2025 AWS Docs, AWS Training, FCJ Playlist 4 Configure MFA for root and user accounts 11/09/2025 11/09/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create 2 IAM users (Dev \u0026amp; Admin) with different permissions 12/09/2025 12/09/2025 AWS Docs, AWS Training, FCJ Playlist 6 Launch EC2 instance (Linux) 15/09/2025 15/09/2025 AWS Docs, AWS Training, FCJ Playlist Week 1 Achievements: Learned about AWS Free Tier and successfully created an AWS account.\nConfigured Billing Dashboard, set up Budget and cost alerts to monitor expenses.\nStudied IAM concepts: User, Group, Role, Policy.\nConfigured MFA for both root and user accounts to enhance security.\nCompleted a mini project: Created 2 IAM users (Dev \u0026amp; Admin) with different permissions.\nLaunched an EC2 instance (Linux) and practiced basic management tasks.\nBecame familiar with the AWS Management Console and AWS CLI for managing resources.\nDeveloped the ability to connect and manage AWS resources using both the web interface and CLI in parallel.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand AWS service groups (Compute, Storage, Networking, Database). Learn to use AWS Console and AWS CLI for managing resources. Practice EC2 and EBS setup and SSH connection. Tasks to be carried out this week: No. Task Start Date Completion Date Reference Material 1 Get acquainted with FCJ members; read and note internship regulations. 11/08/2025 11/08/2025 AWS Docs, AWS Training, FCJ Playlist 2 Learn about AWS and its types of services (Compute, Storage, Networking, Database, etc.) 12/08/2025 12/08/2025 AWS Docs, AWS Training, FCJ Playlist 3 Create AWS Free Tier account; learn AWS Console and CLI setup. 13/08/2025 13/08/2025 AWS Docs, AWS Training, FCJ Playlist 4 Study basic EC2: instance types, AMI, EBS, Elastic IP, and SSH connection methods. 14/08/2025 15/08/2025 AWS Docs, AWS Training, FCJ Playlist 5 Practice launching EC2 instance, connecting via SSH, and attaching EBS volume. 15/08/2025 15/08/2025 AWS Docs, AWS Training, FCJ Playlist Week 2 Achievements: Understood what AWS is and mastered core service groups (Compute, Storage, Networking, Database, \u0026hellip;). Created and configured AWS Free Tier account successfully. Learned how to navigate AWS Management Console and locate services efficiently. Installed and configured AWS CLI with Access Key, Secret Key, and Default Region. Practiced using AWS CLI for: Checking account \u0026amp; configuration Listing regions Viewing EC2 services Creating and managing key pairs Viewing active resources Gained the ability to manage AWS resources in parallel via Console and CLI. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Continue building foundational AWS knowledge. Practice with IAM, EC2, and basic networking setup. Begin deploying a simple website on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Explore AWS Free Tier and create an AWS account 08/09/2025 08/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Billing Dashboard, create Budget \u0026amp; Cost Alerts 09/09/2025 09/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Learn IAM: User, Group, Role, Policy 10/09/2025 10/09/2025 AWS Docs, AWS Training, FCJ Playlist 4 Enable MFA for Root and User accounts 11/09/2025 11/09/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create 2 IAM Users (Dev \u0026amp; Admin) with different permissions 12/09/2025 12/09/2025 AWS Docs, AWS Training, FCJ Playlist 6 Create an EC2 instance (Linux) 15/09/2025 15/09/2025 AWS Docs, AWS Training, FCJ Playlist 7 SSH into EC2 and install Apache/Nginx 16/09/2025 16/09/2025 AWS Docs, AWS Training, FCJ Playlist 8 Learn Security Group, assign Elastic IP 17/09/2025 17/09/2025 AWS Docs, AWS Training, FCJ Playlist 9 Deploy a small application on EC2 18/09/2025 18/09/2025 AWS Docs, AWS Training, FCJ Playlist 10 Mini Project: Deploy a static website on EC2 19/09/2025 19/09/2025 AWS Docs, AWS Training, FCJ Playlist Week 3 Achievements: Understood IAM structure and how to manage users, groups, and roles. Configured MFA and budget alerts for account security and cost control. Created, launched, and connected to EC2 instances. Learned to manage Elastic IPs and security groups. Successfully deployed a static website on EC2. Practiced hands-on with AWS Console and CLI. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand AWS networking concepts: VPC, Subnet, Internet Gateway, Route Table. Learn the difference between Security Groups and NACLs. Deploy basic Load Balancer (ALB) and EC2 instances inside a custom VPC. Complete a mini project combining VPC, EC2, and ALB. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Launch a custom VPC and create Subnets 29/09/2025 29/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Internet Gateway and Route Table for the VPC 30/09/2025 30/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Learn the difference between Security Groups and NACLs 01/10/2025 01/10/2025 AWS Docs, AWS Training, FCJ Playlist 4 Deploy a basic Application Load Balancer (ALB) 02/10/2025 02/10/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create a private VPC and deploy 2 EC2 instances behind the ALB 03/10/2025 03/10/2025 AWS Docs, AWS Training, FCJ Playlist Week 4 Achievements: Successfully created a custom VPC and configured Subnets.\nConfigured Internet Gateway and Route Tables to allow proper routing in the VPC.\nLearned the differences between Security Groups (stateful) and NACLs (stateless) and applied them.\nDeployed a basic Application Load Balancer (ALB) to distribute traffic to multiple EC2 instances.\nCompleted mini project: created a private VPC and deployed 2 EC2 instances behind the ALB, gaining hands-on experience in networking and load balancing.\nBecame more confident in managing AWS networking resources and deploying a simple high-availability architecture.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn advanced AWS networking concepts and practice with Security Groups. Understand Elastic Load Balancer (ALB) and Auto Scaling basics. Deploy a mini project combining EC2, VPC, ALB, and Security Groups. Practice monitoring and basic troubleshooting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review VPC, Subnet, and Route Table configuration 06/10/2025 06/10/2025 AWS Docs, FCJ Playlist 2 Learn and configure Security Groups for EC2 instances 07/10/2025 07/10/2025 AWS Docs, FCJ Playlist 3 Deploy Application Load Balancer (ALB) and connect EC2 instances 08/10/2025 08/10/2025 AWS Docs, FCJ Playlist 4 Understand and practice Auto Scaling Groups 09/10/2025 09/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy 2 EC2 instances behind an ALB with proper Security Groups and basic Auto Scaling 10/10/2025 10/10/2025 AWS Docs, FCJ Playlist Week 5 Achievements: Reviewed VPC, Subnet, and Route Table setups and ensured proper connectivity.\nConfigured Security Groups to allow necessary traffic while keeping instances secure.\nSuccessfully deployed Application Load Balancer (ALB) and connected EC2 instances.\nLearned the basics of Auto Scaling Groups and applied them to EC2 instances.\nCompleted mini project: 2 EC2 instances deployed behind ALB with Security Groups, gaining hands-on experience with networking, load balancing, and scaling.\nImproved skills in monitoring and troubleshooting basic AWS networking issues.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn VPC, Subnet, and Route Table concepts for EC2 networking. Practice configuring Security Groups and Elastic IPs. Deploy EC2 instances in a VPC with proper networking and access. Complete a mini project integrating EC2, Security Groups, and Elastic IP. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review VPC, Subnet, and Route Table concepts 13/10/2025 13/10/2025 AWS Docs, FCJ Playlist 2 Configure Security Groups and test inbound/outbound rules 14/10/2025 14/10/2025 AWS Docs, FCJ Playlist 3 Assign Elastic IPs to EC2 instances and test connectivity 15/10/2025 15/10/2025 AWS Docs, FCJ Playlist 4 Launch EC2 instances within the VPC, assign Subnets, Security Groups, and Elastic IP 16/10/2025 16/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy a basic web application on EC2 instances with proper networking settings 17/10/2025 17/10/2025 AWS Docs, FCJ Playlist Week 6 Achievements: Reviewed and understood VPC, Subnet, and Route Table configurations.\nConfigured Security Groups correctly to allow necessary traffic and secure EC2 instances.\nAssigned Elastic IPs to EC2 instances and verified public accessibility.\nLaunched EC2 instances inside the VPC with correct networking, Security Groups, and Elastic IP setup.\nCompleted mini project: deployed a basic web application on EC2 with proper networking and security.\nStrengthened hands-on skills with EC2 networking, access control, and public IP management.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn AWS S3 concepts: buckets, objects, and storage classes. Practice creating S3 buckets, uploading files, and setting permissions. Understand S3 bucket policies and public/private access. Complete a mini project: deploy static files to S3 and test access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review S3 concepts: bucket, object, storage classes 20/10/2025 20/10/2025 AWS Docs, FCJ Playlist 2 Create S3 buckets and practice uploading files 21/10/2025 21/10/2025 AWS Docs, FCJ Playlist 3 Configure bucket permissions and test public/private access 22/10/2025 22/10/2025 AWS Docs, FCJ Playlist 4 Apply S3 bucket policies for specific access control 23/10/2025 23/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy static website files to S3 and test access 24/10/2025 24/10/2025 AWS Docs, FCJ Playlist Week 7 Achievements: Learned S3 storage concepts and usage.\nSuccessfully created S3 buckets, uploaded files, and organized objects.\nApplied bucket permissions and policies for secure and controlled access.\nCompleted mini project: deployed static files to S3 and verified accessibility.\nEnhanced hands-on skills with AWS S3, file management, and access control.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Learn advanced AWS S3 features: versioning, lifecycle, and bucket policies. Practice setting up S3 bucket policies and permissions. Understand S3 event notifications and integration with Lambda. Introduction to CloudFront for content delivery. Complete a mini project: host a static website on S3 + CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review basic S3 and upload files 27/10/2025 27/10/2025 AWS Docs, FCJ Playlist 2 Enable versioning on S3 buckets and test file versioning 28/10/2025 28/10/2025 AWS Docs, FCJ Playlist 3 Configure bucket policies and permissions 29/10/2025 29/10/2025 AWS Docs, FCJ Playlist 4 Learn about S3 event notifications and trigger Lambda functions 30/10/2025 30/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy a static website on S3 and distribute via CloudFront 31/10/2025 31/10/2025 AWS Docs, FCJ Playlist Week 8 Achievements: Understood advanced S3 features including versioning, lifecycle rules, and policies.\nSuccessfully configured bucket policies for secure and controlled access.\nPracticed triggering Lambda functions via S3 event notifications.\nDeployed a static website on S3 and distributed it via CloudFront for faster content delivery.\nImproved hands-on skills with AWS S3, security policies, version control, and content delivery.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn about AWS RDS (Relational Database Service) and database management. Understand different database engines: MySQL, PostgreSQL, SQL Server. Practice creating RDS instances and connecting from local/EC2. Learn backup, snapshot, and restore methods in RDS. Understand security best practices for database access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review basic database concepts and AWS RDS documentation 03/11/2025 03/11/2025 AWS Docs, FCJ Playlist 2 Create RDS instances (MySQL \u0026amp; PostgreSQL) 04/11/2025 04/11/2025 AWS Docs, FCJ Playlist 3 Connect RDS instances from local machine and EC2 05/11/2025 05/11/2025 AWS Docs, FCJ Playlist 4 Configure automatic backups and manual snapshots 06/11/2025 06/11/2025 AWS Docs, FCJ Playlist 5 Test restore from snapshot and review database security \u0026amp; parameter groups 07/11/2025 07/11/2025 AWS Docs, FCJ Playlist Week 9 Achievements: Understood AWS RDS and its advantages for managed databases.\nSuccessfully created MySQL and PostgreSQL RDS instances.\nConnected to RDS instances from local machines and EC2 instances.\nConfigured automatic backups, created snapshots, and tested restore procedures.\nLearned best practices for database security including IAM roles, security groups, and password management.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Learn about AWS Free Tier, Billing Dashboard, IAM, MFA, EC2 and complete a mini IAM project\nWeek 2: Get familiar with AWS CLI, IAM roles, and basic S3 operations\nWeek 3: Learn EC2 in detail: Instance types, AMI, EBS, Security Groups, and SSH connection\nWeek 4: Practice launching EC2 instances, attaching EBS, and connecting via SSH\nWeek 5: Understand AWS networking: VPC, Subnets, Internet Gateway, and Elastic IPs\nWeek 6: Practice configuring VPC, subnet, security group rules, and EC2 networking setup\nWeek 7: Explore RDS, DynamoDB, and basic database configuration on AWS\nWeek 8: Learn S3 advanced features: versioning, lifecycle rules, bucket policies, and static website hosting\nWeek 9: Connect with FCJ members, review AWS basics, create AWS account, practice EC2 launch and EBS attachment\nWeek 10: Consolidate AWS skills: EC2, EBS, CLI commands, and Management Console usage\nWeek 11: Practice advanced AWS CLI commands, automate EC2 creation, and integrate basic services\nWeek 12: Learn CloudFormation, deploy EC2 + S3 stacks, integrate Lambda \u0026amp; API Gateway, test automation, and finalize internship report\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/2-create-iam/",
	"title": "Create IAM Roles",
	"tags": [],
	"description": "",
	"content": "Create IAM Roles We need to create an IAM Role for our Lambda function. This role gives Lambda permission to access other AWS services like S3 (for assets), SES (for emails), and RDS (for data).\n1. Create Lambda Execution Role CLI # 1. Create trust policy file echo \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }\u0026#39; \u0026gt; lambda-trust-policy.json # 2. Create the role aws iam create-role \\ --role-name lambda-execution-role \\ --assume-role-policy-document file://lambda-trust-policy.json # 3. Attach standard policies aws iam attach-role-policy \\ --role-name lambda-execution-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole aws iam attach-role-policy \\ --role-name lambda-execution-role \\ --policy-arn arn:aws:iam::aws:policy/CloudWatchLogsFullAccess AWS Console Open the IAM Console. Go to Roles ‚Üí Create role. Trusted entity type: AWS service. Service: Lambda. Click Next. Add permissions: Search and select: AWSLambdaVPCAccessExecutionRole CloudWatchLogsFullAccess Click Next. Role name: lambda-execution-role. Click Create role. 2. Add Custom Permissions We need to give Lambda specific access to our S3 buckets and RDS.\nCLI # 1. Create policy document echo \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::gametracker-assets/*\u0026#34;, \u0026#34;arn:aws:s3:::gametracker-assets\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rds:DescribeDBInstances\u0026#34;, \u0026#34;rds:DescribeDBProxies\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] }\u0026#39; \u0026gt; lambda-custom-policy.json # 2. Attach inline policy aws iam put-role-policy \\ --role-name lambda-execution-role \\ --policy-name lambda-custom-permissions \\ --policy-document file://lambda-custom-policy.json AWS Console Go to Roles and select lambda-execution-role. In the Permissions tab, click Add permissions ‚Üí Create inline policy. Select JSON editor and paste the policy JSON above. Click Next. Policy name: lambda-custom-permissions. Click Create policy. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-deploy-app/2-deploy-frontend/",
	"title": "Deploy Frontend",
	"tags": [],
	"description": "",
	"content": "Deploy Frontend (React + S3 + CloudFront) 1. Create S3 Buckets We need two buckets: one for the frontend code and one for game assets.\nCLI # Frontend Bucket aws s3 mb s3://gametracker-frontend --region ap-southeast-2 # Assets Bucket aws s3 mb s3://gametracker-assets --region ap-southeast-2 AWS Console S3 Console ‚Üí Create bucket. Create gametracker-frontend (disable public access if using CloudFront OAC). Create gametracker-assets (configure CORS as needed). 2. Create CloudFront Distribution This ensures fast delivery and HTTPS.\nAWS Console CloudFront Console ‚Üí Create distribution. Origin domain: Select gametracker-frontend.s3.... Origin access: Select Origin access control settings (recommended) -\u0026gt; Create control setting. Important: You must update the S3 bucket policy to allow CloudFront access (copy the policy AWS provides after creation). Viewer protocol policy: Redirect HTTP to HTTPS. Default root object: index.html. Error pages: Create custom error response for 403 and 404 -\u0026gt; Path /index.html -\u0026gt; Status 200. This is crucial for SPA routing to work. Click Create. 3. Build \u0026amp; Deploy React App Update your React app\u0026rsquo;s API URL to point to the API Gateway Invoke URL you obtained in the previous step.\n# 1. Build npm run build # 2. Sync to S3 aws s3 sync build/ s3://gametracker-frontend # 3. Invalidate Cache (Optional but recommended) aws cloudfront create-invalidation --distribution-id \u0026lt;DISTRIBUTION_ID\u0026gt; --paths \u0026#34;/*\u0026#34; Your GameTracker application is now live at the CloudFront Domain Name!\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Prerequisites Before deploying the GameTracker application, we need to set up the foundational infrastructure. This includes the network layer (VPC), security configurations (IAM Roles, Security Groups), and necessary permissions.\nIn this section, we will:\nCreate Network Infrastructure: VPC, Subnets, Internet Gateway, and Route Tables. Create IAM Roles: Execution roles for AWS Lambda. Create Security Groups: Firewall rules for our components. These steps ensure a secure and isolated environment for our application.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Customer Background GameTracker is a platform designed for game players and admins to manage, track, and share information about characters, weapons, banners, items, and events.\nBusiness and Technical Objectives\nUnified Management: Create a centralized system for managing game data effectively. Accessibility: Provide easy access to information for English-language games which can be a barrier for some users. Efficiency: Reduce manual maintenance time and improve data reliability for admins. Scalability: Build a system scalable to multiple games and community features using serverless architecture. Use Cases\nPlayers: Track gacha history, simulate pulls, view banner/event timelines. Admins: Manage game data (CRUD) with clear access permissions. Partner Professional Services We will deliver a full-stack web application hosted on AWS, utilizing serverless technologies (Lambda, S3, RDS) to ensure low operational costs and high availability.\n1.2 PROJECT SUCCESS CRITERIA System Stability: Stable, auto-scaling system with low maintenance costs. Security: Secure API with centralized data management and role-based access control. User Engagement: Functional gacha tools and timelines that help players track game events conveniently. Scalability: Architecture ready for easy expansion to support more games and features. 1.3 ASSUMPTIONS Prerequisites: AWS account access with necessary permissions for deployment. Dependencies: Third-party authentication (Google OAuth2). Constraints: Budget constraints requiring a low-cost serverless approach. Risks: Potential cold start latency with Lambda (mitigated by warmers), RDS costs (mitigated by instance selection). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM Proposed High-Level Architecture: The solution adopts a modern cloud-native architecture:\nFrontend: React SPA served via S3 + CloudFront, protected by AWS WAF. Backend: Spring Boot serverless deployed on AWS Lambda, using JWT and Google OAuth2 for auth. Database: SQL Server on AWS RDS. Storage: AWS S3 for storing static assets (avatars, backgrounds, weapons). Security: AWS WAF, IAM, and Spring Security. AWS Services Used:\nAWS S3, AWS Lambda, AWS RDS, AWS CloudFront, AWS WAF, AWS SES, AWS IAM. 2.2 TECHNICAL PLAN We will develop scripts using AWS CDK/CloudFormation or manual setup procedures documented for repeatability.\nFrontend: React, TypeScript, Vite. Backend: Spring Boot, Spring Security. DevOps: Docker, CI/CD pipelines. All critical paths including user login, data synchronization, and gacha simulation will include extensive test coverage.\n2.3 PROJECT PLAN The project will follow an Agile methodology over a 1-month timeline.\nWeek 1: Planning, Requirements Analysis, Architecture Design. Week 2: Backend Development (API, Auth, Database). Week 3: Frontend Development (UI/UX, Admin Dashboard, Tools). Week 4: Deployment, Testing, Documentation, and Handover. 2.4 SECURITY CONSIDERATIONS Best Practices Implemented:\nIdentity: Google OAuth2 integration and JWT for secure stateless authentication. Infrastructure: AWS WAF to protect against common web exploits. Access Control: Role-based access control (RBAC) for Admins vs Users. Data Protection: HTTPS encryption in transit; RDS encryption at rest. Monitoring: AWS CloudWatch for logs and metrics. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Project Phase Timeline Activities Deliverables/Milestones Assessment \u0026amp; Setup Week 1 Requirement analysis, architecture design, AWS setup (S3, RDS, Lambda) Architecture Diagram, AWS Environment Ready Backend Implementation Week 2 Build Lambda functions, API endpoints, Auth integration, DB schema Working API, Database connectivity Frontend Implementation Week 3 React SPA development, Dashboard creation, Gacha tools logic Functional Web UI, Admin Dashboard Testing \u0026amp; Go-live Week 4 Integration testing, Security optimization, Deployment to CloudFront Deployed Application, User Guide, Documentation 3.2 OUT OF SCOPE Mobile Application development (iOS/Android native apps). Real-time multiplayer game server features (only web-based data management). Integration with game servers directly (data is manually managed/imported). 3.3 PATH TO PRODUCTION The current proposal outlines the path to a production-ready MVP.\nPOC to Prod: The system is designed to be production-grade from the start using AWS managed services. Gaps: Further load testing and fine-tuning of WAF rules may be needed based on actual traffic patterns. Operations: Error handling and monitoring are implemented via CloudWatch. 4. EXPECTED AWS COST BREAKDOWN BY SERVICES Estimated Monthly Cost: ~$121-123/month\nAWS Lambda: ~$5-7 (Memory: 3008 MB, ~12k invocations). S3 Standard: ~$0.23 (10 GB storage). CloudFront: ~$8.50 (100 GB egress). RDS (SQL Server): ~$60+ (db.t3.medium or similar). AWS WAF: ~$10 (Web ACL + requests). NAT Gateway: ~$32 (if required for Lambda in VPC). Others (SES, Route53, CloudWatch): ~$6. Note: Costs are estimates and depend on actual usage and region.\n5. TEAM Partner Project Team\nName Title Role Email / Contact Info [Name] Delivery Manager Project Manager [Email] [Name] Sr. Solutions Architect Technical Lead [Email] Project Stakeholders\nName Title Stakeholder for Email / Contact Info [Name] [Title] [Role] [Email] 6. RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD) / Hour Solution Architect System Design \u0026amp; Lead - Full-stack Engineer Implementation - Total Estimated Effort: [Total Man-days]\n7. ACCEPTANCE Upon completion of a Phase, the Partner will submit the associated tangible Deliverables to the Customer. The Customer will review, evaluate, and test the Deliverables within eight (8) business days (the ‚ÄúAcceptance Period‚Äù) to determine satisfaction of acceptance criteria.\nIf the Deliverable satisfies its acceptance criteria, Customer will furnish a written acceptance. If rejected, Customer will indicate detailed reasons, and Partner will correct defects. If no rejection is received within the Acceptance Period, Deliverables are deemed accepted.\nüîó Project Website: https://d2eu9it59oopt8.cloudfront.net/\r"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Learn about AWS S3 (Simple Storage Service) and CloudFront (CDN) services. Understand S3 bucket creation, permissions, and storage classes. Practice uploading files to S3 and configuring public/private access. Learn about CloudFront distributions to deliver content globally. Understand versioning, lifecycle policies, and basic security best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review S3 concepts: bucket, object, storage classes, permissions 10/11/2025 10/11/2025 AWS Docs, FCJ Playlist 2 Create S3 buckets and upload test files 11/11/2025 11/11/2025 AWS Docs, FCJ Playlist 3 Configure S3 permissions, public/private access, and versioning 12/11/2025 12/11/2025 AWS Docs, FCJ Playlist 4 Set up CloudFront distribution with S3 as origin 13/11/2025 13/11/2025 AWS Docs, FCJ Playlist 5 Test global content delivery and implement lifecycle policies \u0026amp; security best practices 14/11/2025 14/11/2025 AWS Docs, FCJ Playlist Week 10 Achievements: Understood AWS S3 and CloudFront, their use cases, and advantages.\nSuccessfully created S3 buckets and uploaded files with correct permissions.\nConfigured CloudFront distribution to deliver content globally with caching.\nLearned about versioning, lifecycle policies, and basic security best practices for S3.\nGained hands-on experience in managing storage and content delivery on AWS.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Learn about AWS Lambda and serverless computing. Understand how to create Lambda functions, triggers, and manage versions. Learn about API Gateway to expose Lambda functions as HTTP endpoints. Practice integrating Lambda with S3 and other AWS services. Understand basic monitoring and logging with CloudWatch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review serverless concepts and Lambda architecture 17/11/2025 17/11/2025 AWS Docs, FCJ Playlist 2 Create simple Lambda functions and test execution 18/11/2025 18/11/2025 AWS Docs, FCJ Playlist 3 Set up triggers for Lambda (S3 event, API Gateway, CloudWatch event) 19/11/2025 19/11/2025 AWS Docs, FCJ Playlist 4 Configure API Gateway to expose Lambda functions as REST endpoints 20/11/2025 20/11/2025 AWS Docs, FCJ Playlist 5 Monitor Lambda execution with CloudWatch logs and metrics 21/11/2025 21/11/2025 AWS Docs, FCJ Playlist Week 11 Achievements: Understood serverless architecture and AWS Lambda concepts. Created Lambda functions and tested execution successfully. Set up triggers from S3, API Gateway, and CloudWatch events. Exposed Lambda functions via API Gateway endpoints. Monitored Lambda executions with CloudWatch and learned basic troubleshooting. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Review and consolidate all AWS knowledge learned during the internship. Learn basic automation using AWS CloudFormation and Infrastructure as Code (IaC). Deploy simple stacks with CloudFormation. Review Lambda, S3, EC2, and API Gateway integration. Evaluate internship outcomes and prepare final report. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review all AWS services and concepts studied during internship 24/11/2025 24/11/2025 AWS Docs, FCJ materials 2 Learn CloudFormation basics and template syntax 25/11/2025 25/11/2025 AWS Docs, FCJ Playlist 3 Deploy a simple CloudFormation stack to create EC2 + S3 resources 26/11/2025 26/11/2025 AWS Docs, FCJ Playlist 4 Integrate Lambda and API Gateway into CloudFormation stack 27/11/2025 27/11/2025 AWS Docs, FCJ Playlist 5 Test automation, troubleshoot, and prepare summary of all accomplishments 28/11/2025 28/11/2025 AWS Docs, FCJ Playlist Week 12 Achievements: Consolidated AWS knowledge and practical skills gained during internship. Learned basic automation and Infrastructure as Code (IaC) using CloudFormation. Deployed a CloudFormation stack with EC2, S3, Lambda, and API Gateway. Tested automation, monitored resources, and practiced troubleshooting. Completed internship report and evaluation. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-create-rds/",
	"title": "Create RDS Database",
	"tags": [],
	"description": "",
	"content": "Create RDS SQL Server In this step, we will create the Microsoft SQL Server database instance that will store our game data. We will use the SQL Server Express edition which is eligible for the AWS Free Tier.\n1. Create RDS Instance CLI # Create RDS Instance aws rds create-db-instance \\ --db-instance-identifier gametracker-mssql \\ --db-instance-class db.t3.micro \\ --engine sqlserver-ex \\ --master-username admin \\ --master-user-password YourSecurePassword123 \\ --allocated-storage 20 \\ --vpc-security-group-ids \u0026lt;RDS_SG_ID\u0026gt; \\ --db-subnet-group-name gametracker-db-subnet-group \\ --backup-retention-period 7 \\ --no-publicly-accessible \\ --region ap-southeast-2 AWS Console Open the RDS Console. Click Create database. Choose a database creation method: Standard create. Engine options: Microsoft SQL Server. Edition: SQL Server Express Edition. Templates: Select Free tier. Settings: DB instance identifier: gametracker-mssql. Master username: admin (or your preferred username). Master password: Enter a strong password. Instance configuration: db.t3.micro. Storage: 20 GiB (General Purpose SSD gp2/gp3). Connectivity: VPC: gametracker-vpc. Subnet group: gametracker-db-subnet-group. Public access: No. VPC security group: Choose existing -\u0026gt; gametracker-rds-sg. Additional configuration: Initial database name: (SQL Server doesn\u0026rsquo;t support this via console, we create it later via query). Click Create database. 2. Verify creation Wait for the status to change from Creating to Available.\nEndpoint: Copy the endpoint (e.g., gametracker-mssql.xxxx.ap-southeast-2.rds.amazonaws.com). Port: 1433. We will use these details when deploying the backend application.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/3-create-sg/",
	"title": "Create Security Groups",
	"tags": [],
	"description": "",
	"content": "Create Security Groups Security Groups allow us to control traffic. We need two groups: one for our Compute layer (Lambda) and one for our Database layer (RDS).\n1. Create Lambda Security Group CLI # Create Security Group aws ec2 create-security-group \\ --group-name gametracker-lambda-sg \\ --description \u0026#34;SG for Lambda functions\u0026#34; \\ --vpc-id \u0026lt;VPC_ID\u0026gt; # Note: By default, SGs allow all outbound traffic, which is what we want for Lambda. AWS Console Navigate to EC2 Dashboard ‚Üí Security Groups ‚Üí Create security group. Name: gametracker-lambda-sg. Description: SG for Lambda functions. VPC: gametracker-vpc. Inbound rules: None (Lambda doesn\u0026rsquo;t listen to incoming ports directly in this setup). Outbound rules: Allow all traffic (Default). Click Create security group. 2. Create RDS Security Group This group allows the Lambda function to talk to the SQL Server database on port 1433.\nCLI # Create Security Group aws ec2 create-security-group \\ --group-name gametracker-rds-sg \\ --description \u0026#34;SG for RDS\u0026#34; \\ --vpc-id \u0026lt;VPC_ID\u0026gt; # Add Inbound Rule for SQL Server (Port 1433) from Lambda SG aws ec2 authorize-security-group-ingress \\ --group-id \u0026lt;RDS_SG_ID\u0026gt; \\ --protocol tcp \\ --port 1433 \\ --source-group \u0026lt;LAMBDA_SG_ID\u0026gt; AWS Console Security Groups ‚Üí Create security group. Name: gametracker-rds-sg. Description: SG for RDS. VPC: gametracker-vpc. Inbound rules: Type: MS SQL. Port: 1433. Source: Custom ‚Üí Select gametracker-lambda-sg (start typing the name to find it). Outbound rules: Allow all traffic (Default). Click Create security group. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/4-create-db-subnetgroup/",
	"title": "Create DB Subnet Group",
	"tags": [],
	"description": "",
	"content": "Create DB Subnet Group The DB Subnet Group tells RDS which subnets it can use to deploy the database instances. For high availability, we must select subnets in at least two different Availability Zones.\n1. Create DB Subnet Group We will use our two Private Subnets for this group to ensure the database is not directly accessible from the internet.\nCLI aws rds create-db-subnet-group \\ --db-subnet-group-name gametracker-db-subnet-group \\ --db-subnet-group-description \u0026#34;Private subnets for gametracker RDS\u0026#34; \\ --subnet-ids \u0026lt;SUBNET_PRIVATE_1_ID\u0026gt; \u0026lt;SUBNET_PRIVATE_2_ID\u0026gt; AWS Console Open the RDS Console. In the navigation pane, click Subnet groups. Click Create DB subnet group. Name: gametracker-db-subnet-group. Description: Private subnets for GameTracker. VPC: Select gametracker-vpc. Add subnets: Availability Zones: Select ap-southeast-2a and ap-southeast-2b. Subnets: Select the CIDRs corresponding to your Private Subnets (e.g., 10.10.2.0/24 and 10.10.3.0/24). Click Create. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-deploy-app/",
	"title": "Deploy Application",
	"tags": [],
	"description": "",
	"content": "Deploy Application Now that our infrastructure is ready (Network, Database, IAM), we can proceed to deploy the application components.\nWe will split this into two parts:\nBackend Deployment: Containerizing the Spring Boot app, pushing to ECR, and running securely on AWS Lambda with API Gateway. Frontend Deployment: Hosting the React SPA on S3 and distributing it globally via CloudFront. Let\u0026rsquo;s start with the Backend.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Introduction to Kiro.dev (AWS Agentic IDE)\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/5-create-nat/",
	"title": "Create NAT Gateway",
	"tags": [],
	"description": "",
	"content": "Create NAT Gateway (Optional) If your Lambda function in the private subnet needs to access the internet (e.g., for installing packages, calling external APIs, or pulling Docker images if endpoints aren\u0026rsquo;t used), you need a NAT Gateway.\nNote: NAT Gateway incurs hourly costs. Only create if necessary or just before deployment.\n1. Allocate Elastic IP (EIP) CLI aws ec2 allocate-address --domain vpc --region ap-southeast-2 # Save the AllocationId AWS Console Navigate to VPC Dashboard ‚Üí Elastic IPs. Click Allocate Elastic IP address. Region: ap-southeast-2. Click Allocate. 2. Create NAT Gateway We will create the NAT Gateway in one of our Public Subnets.\nCLI aws ec2 create-nat-gateway \\ --subnet-id \u0026lt;SUBNET_PUBLIC_1_ID\u0026gt; \\ --allocation-id \u0026lt;EIP_ALLOCATION_ID\u0026gt; \\ --region ap-southeast-2 AWS Console Navigate to NAT Gateways ‚Üí Create NAT gateway. Name: gametracker-nat. Subnet: Select gametracker-public-1. Elastic IP allocation ID: Select the EIP created above. Click Create NAT gateway. 3. Update Private Route Table Limit internet traffic from private subnets to go through the NAT Gateway.\nCLI aws ec2 create-route \\ --route-table-id \u0026lt;RTB_PRIVATE_ID\u0026gt; \\ --destination-cidr-block 0.0.0.0/0 \\ --nat-gateway-id \u0026lt;NAT_GATEWAY_ID\u0026gt; AWS Console Navigate to Route Tables. Select private-route-table-1. Go to Routes tab ‚Üí Edit routes. Add route: Destination: 0.0.0.0/0. Target: Select NAT Gateway ‚Üí gametracker-nat. Click Save changes. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/",
	"title": "Workshop: Get Started with Amazon RDS",
	"tags": [],
	"description": "",
	"content": "Amazon Relational Database Service (Amazon RDS) Overview of Amazon RDS ‚ÑπÔ∏è Information: Amazon Relational Database Service (Amazon RDS) is a managed service that allows you to deploy and manage relational databases on AWS. Amazon RDS is designed for online transaction processing (OLTP) and is best suited for structured, relational data storage requirements.\nAmazon RDS provides key benefits:\nEasy replacement for traditional database instances Automated backups and patching during customer-defined maintenance windows One-click scaling, replication, and availability Supported Database Engines Amazon RDS supports the following database engines:\nAmazon Aurora MySQL MariaDB Oracle SQL Server PostgreSQL ‚ö†Ô∏è Warning: RDS is a managed service and you don\u0026rsquo;t have root access to the underlying EC2 server. The exception is Amazon RDS Custom, which allows access to the underlying operating system but is only available for a limited set of DB Engines.\nStorage Options General Purpose SSD (gp3/gp2): Cost-effective storage providing 3 IOPS/GB baseline performance Provisioned IOPS SSD (io1/io2): High-performance storage with customizable IOPS for I/O-intensive workloads Magnetic Storage: Legacy option with limited performance (not recommended for new deployments) üí° Pro Tip: Choose General Purpose SSD for most workloads, and Provisioned IOPS only when you need consistent I/O performance for database-intensive applications.\nHigh Availability and Disaster Recovery Multi-AZ Deployments: Synchronous standby replica in a different Availability Zone for automatic failover Read Replicas: Asynchronous replication for read scaling and potential disaster recovery Global Databases: Cross-region replication with fast local reads and disaster recovery capabilities üîí Security Note: Multi-AZ deployments enhance both availability and data durability, with automatic failover typically completing within 60-120 seconds.\nSecurity Features Encryption at rest: Using AWS KMS keys (applies to DB instances, backups, snapshots, and replicas) Network isolation: Using Amazon VPC for network-level isolation Resource-level permissions: Using IAM policies SSL/TLS encryption: For data in transit Database authentication: Using database engine native authentication or IAM authentication Scalability and Limits Storage can be scaled up (not down) without downtime Compute resources can be modified with a brief downtime during the change Maximum storage: 64 TiB for most engines (16 TiB for SQL Server) Maximum database connections: Varies by engine and instance size üí° Pro Tip: Plan your initial storage carefully as you can only scale up. Consider using Aurora for more flexible scaling options.\nBackup and Recovery Automated backups: Point-in-time recovery for up to 35 days Manual snapshots: User-initiated backups that persist until explicitly deleted Snapshot export to S3: For long-term retention or analysis When to Use Amazon RDS Amazon RDS is ideal for:\nTraditional relational database workloads Applications requiring SQL query capabilities Structured data with well-defined schemas OLTP workloads with predictable scaling needs ‚ÑπÔ∏è Information: For unstructured data, high-scale requirements, or specialized workloads, consider alternative AWS database services like DynamoDB, DocumentDB, or purpose-built databases.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/6-backup/",
	"title": "Backup and Restore",
	"tags": [],
	"description": "",
	"content": "Understanding Amazon RDS Backup and Restore ‚ÑπÔ∏è Information: Amazon RDS provides automated backups and allows manual snapshots to ensure your database data is protected and can be recovered when needed. These capabilities are essential for disaster recovery planning and maintaining business continuity.\nMonitoring Backup Status Access Monitoring:\nNavigate to the Databases section in the AWS Management Console. Select your target DB instance. Click on the Monitoring tab to view performance metrics. Managing Backups View Backup Details:\nSelect your DB instance. Navigate to the Maintenance \u0026amp; backups tab. Here you can view both automated and manual backup information and configure backup settings. View Snapshots:\nIn the left navigation pane, click Snapshots. You will see a list of all manual and automated snapshots. Restoring from a DB Snapshot ‚ÑπÔ∏è Information: Restoring a snapshot creates a new DB instance. It does not overwrite the existing one.\nSelect Snapshot:\nChoose the DB snapshot you want to restore. Click Actions \u0026gt; Restore snapshot. Configure New Instance:\nDB instance identifier: Enter a unique name for the new instance (e.g., workshop-db-restore). Instance specifications: Select the instance class (e.g., db.t3.micro). Connectivity: Select the same VPC and Subnet Group as your original instance. Security: Choose the correct Security Group. üí° Pro Tip: When restoring for testing, you can choose a smaller instance class to save costs.\nInitiate Restore:\nClick Restore DB instance. ‚ö†Ô∏è Warning: The restore process creates a completely new database instance with a new endpoint. You must update your application\u0026rsquo;s connection string to point to this new endpoint.\nVerify:\nWait for the status to change to Available. Test connectivity to the new instance. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at First Cloud Journey (FCJ) from June 17, 2024 to September 10, 2024, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the GameTracker Platform project, developing a serverless solution for the game community. Through this, I improved my skills in AWS Cloud services (Lambda, S3, RDS, WAF), React frontend development, Spring Boot backend, and System Architecture design.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚úÖ ‚òê ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Advanced Troubleshooting: Improve capability to debug complex distributed system issues independently. Communication: Enhance technical communication skills to explain complex architectural decisions more clearly to non-technical stakeholders. Optimization: Deepen understanding of cost optimization and performance tuning for serverless architectures. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/7-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "Resource Cleanup ‚ÑπÔ∏è Information: To avoid unexpected charges, it is crucial to clean up all resources created during this workshop. We will delete resources in the reverse order of their creation.\nStep 1: Delete RDS Resources Delete RDS Instance:\nGo to the Amazon RDS console \u0026gt; Databases. Select your database instance (e.g., workshop-db). Click Actions \u0026gt; Delete. Uncheck Create final snapshot and check I acknowledge\u0026hellip;. Type delete me and click Delete. Delete DB Subnet Group:\nGo to Subnet groups. Select your group (e.g., rds-subnet-group). Click Delete. Step 2: Terminate EC2 Instance Terminate Instance: Go to the Amazon EC2 console \u0026gt; Instances. Select your instance (e.g., Workshop-Web-Server). Click Instance state \u0026gt; Terminate instance. Click Terminate. Step 3: Delete Network Resources Delete Security Groups:\nGo to the VPC console \u0026gt; Security Groups. Select the RDS Security Group and delete it. Select the EC2 Security Group and delete it. üí° Pro Tip: You must delete the RDS Security Group first because the EC2 Security Group might reference it (or vice versa depending on your rules). If you get a dependency error, check your Inbound/Outbound rules.\nDelete VPC:\nGo to Your VPCs. Select your workshop VPC. Click Actions \u0026gt; Delete VPC. Type delete and confirm. ‚ÑπÔ∏è Information: Deleting the VPC will automatically delete associated subnets, route tables, and internet gateways.\nVerification Check your Billing Dashboard the next day to ensure no active resources remain. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/fcj-workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/fcj-workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]