[
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/1-create-vpc/",
	"title": "Create a VPC",
	"tags": [],
	"description": "",
	"content": "Creating a VPC with Subnets and Associated Resources ‚ÑπÔ∏è Information: Amazon Virtual Private Cloud (Amazon VPC) is your private network in the cloud. It allows you to launch AWS resources into a virtual network that you define, giving you complete control over your network environment.\nWe will use the VPC and more wizard to create our VPC, subnets, route tables, and internet gateway in a single workflow.\nStep-by-Step Guide Open the Amazon VPC console at https://console.aws.amazon.com/vpc/.\nOn the VPC dashboard, choose Create VPC.\nUnder Resources to create, select VPC and more. This option automatically provisions related resources like subnets and route tables.\nName tag auto-generation: Enter a name for your project (e.g., workshop-vpc). This will be used as a prefix for all created resources.\nIPv4 CIDR block: Keep the default (e.g., 10.0.0.0/16) or enter your preferred range.\nAvailability Zones (AZs): Select 2. This is critical for High Availability and Multi-AZ deployments.\nNumber of public subnets: Select 2. These will host resources that need direct internet access (like a bastion host or load balancer).\nNumber of private subnets: Select 2. These will host your RDS database instances, keeping them secure from the public internet.\nüîí Security Note: Always place your database instances in private subnets.\nNAT gateways: Select 1 per AZ or 1 in 1 AZ depending on your cost preference. For this workshop, None or 1 in 1 AZ is sufficient if you need outbound internet access for private instances (e.g., for updates).\nüí° Pro Tip: In a production environment, deploying a NAT Gateway in each AZ ensures high availability but incurs higher costs.\nVPC endpoints: Leave as None for this workshop.\nDNS options: Ensure Enable DNS hostnames and Enable DNS resolution are checked. These are required for RDS to function correctly with public access (if needed) and for easier internal resolution.\nReview the Preview pane to visualize your network architecture.\nClick Create VPC.\nConfiguring Public IP Assignment (Optional) ‚ÑπÔ∏è Information: By default, instances in non-default subnets do not get public IP addresses. If you want instances in your Public Subnets to automatically get a public IP, follow these steps:\nGo to Subnets in the left navigation pane.\nSelect one of your Public Subnets.\nClick Actions \u0026gt; Edit subnet settings.\nCheck Enable auto-assign public IPv4 address.\nClick Save.\nRepeat for your other Public Subnet.\n‚ö†Ô∏è Warning: Never enable auto-assign public IP for your Private Subnets where your database resides.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Amazon Relational Database Service (Amazon RDS) ‚ÑπÔ∏è Information: Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.\nOnline Transaction Processing (OLTP) Amazon RDS is optimized for Online Transaction Processing (OLTP) workloads.\nPrimary Use Case It is primarily designed for transactional applications requiring a structured, relational data store, rather than analytical workloads.\nDrop-in Replacement RDS serves as a seamless drop-in replacement for your existing on-premises database instances, allowing you to use the same code, applications, and tools you already use.\nKey Features Automated Maintenance: Backups and software patching are handled automatically during maintenance windows you define. Scalability \u0026amp; Availability: Offers push-button scaling, replication, and high availability options. Supported Database Engines Amazon RDS supports six popular database engines:\nAmazon Aurora MySQL MariaDB Oracle SQL Server PostgreSQL Managed Service Benefits ‚ÑπÔ∏è Information: As a managed service, RDS restricts access to the underlying EC2 instance (no root access) to ensure stability and security.\nüí° Pro Tip: If you need access to the underlying operating system, Amazon RDS Custom is available for Oracle and SQL Server engines.\nThe managed service model includes:\nSecurity: Hardening and patching of DB instances. Reliability: Automated backups and multi-AZ synchronous replication for high availability. Maintenance: Automatic software updates for the DB engine. Scalability: Easy vertical (compute) and horizontal (read replicas) scaling. Resilience: Automatic failover in Multi-AZ configurations. DB Instance A DB instance is an isolated database environment in the cloud. You define the compute and storage resources it uses.\nAccess via Endpoints You connect to your database using endpoints. These can be found in the DB instance details within the AWS Management Console, or retrieved via the DescribeDBInstances API or CLI command.\nInstance Limits ‚ö†Ô∏è Warning: By default, you are limited to 40 Amazon RDS DB instances per account. Of these, up to 10 can be Oracle or SQL Server under the \u0026ldquo;License Included\u0026rdquo; model.\nMaintenance Windows Maintenance windows allow you to control when DB modifications (like scaling or patching) occur. You can specify a weekly time slot, or let AWS assign a 30-minute window randomly.\nWindows Integrated Authentication For SQL Server, Windows Integrated Authentication is supported only when using AWS Directory Service domains. You must establish a trust relationship with your on-premises AD if needed.\nEvents and Notifications Amazon RDS uses Amazon SNS to deliver notifications about important database events.\nAPI: Use DescribeEvents to see events from the past 14 days. CLI: View events from the past 14 days. Console: View events from the past 1 day only. Use Cases, Alternatives, and Anti-Patterns Use the table below to decide if RDS is the right choice for your needs:\nData Store Best Used When\u0026hellip; Database on EC2 - You need full control over the OS and DB configuration.\n- Your preferred DB engine isn\u0026rsquo;t supported by RDS. Amazon RDS - You need a traditional relational database for OLTP.\n- Data is structured and well-formed.\n- Migrating existing apps that require an RDBMS. Amazon DynamoDB - Data is unstructured (name/value pairs) or unpredictable.\n- You need extreme scale and low-latency performance.\n- High I/O throughput is required. Amazon RedShift - You have massive datasets for analytics (OLAP). Amazon Neptune - Data value is derived from relationships between objects (Graph DB). Amazon ElastiCache - You need fast, in-memory caching for frequently accessed data. Amazon S3 - Storing large binary objects (BLOBs) or static website content. Alternative to Amazon RDS:\nIf RDS doesn\u0026rsquo;t meet your specific requirements, running a database on Amazon EC2 is a viable alternative.\nConsider EC2 if:\nYou need maximum flexibility and control. You are willing to manage backups, redundancy, patching, and scaling yourself. You use a database engine not supported by RDS (e.g., IBM DB2, SAP HANA). Anti-Patterns:\nAvoid using RDS for the following scenarios:\nRequirement Better Alternative Storing large binary objects (BLOBs) Amazon S3 Infinite Automated Scalability Amazon DynamoDB Unstructured / Name-Value Data Amazon DynamoDB Complex Graph Relationships Amazon Neptune Complete OS/DB Control Amazon EC2 Encryption üîí Security Note: You can encrypt your RDS instances and snapshots at rest using AWS KMS. This is a best practice for sensitive data.\nEncryption at rest covers:\nDB instance storage Automated backups Read Replicas Snapshots ‚ö†Ô∏è Warning: You cannot encrypt an existing unencrypted DB instance directly. You must create a snapshot, copy it as an encrypted snapshot, and then restore a new DB instance from that encrypted snapshot.\nSSL Encryption: RDS supports SSL for encrypting data in transit between your application and the database.\nDB Subnet Groups ‚ÑπÔ∏è Information: A DB subnet group defines which subnets and IP ranges the RDS instance can use within your VPC.\nüí° Pro Tip: Always include subnets from at least two Availability Zones in your subnet group to enable Multi-AZ deployments.\nBilling and Provisioning You are charged for:\nCompute: DB instance hours (partial hours billed as full). Storage: GB per month. I/O: Requests/month (Magnetic) or Provisioned IOPS/month (SSD). Data Transfer: Outbound data transfer. Backup Storage: Storage for manual snapshots and automated backups (in excess of your DB storage size). Note: Multi-AZ deployments incur costs for the standby instance, storage, and I/O, but data transfer between the primary and standby is free.\nReserved Instances: You can purchase Reserved Instances (RIs) for significant discounts. RIs are tied to specific attributes:\nDB Engine Instance Class Deployment Type (Single-AZ or Multi-AZ) License Model Region Scalability ‚ÑπÔ∏è Information: RDS supports vertical scaling (instance type) and storage scaling.\nStorage: Can be increased while the instance is running (zero downtime, potential performance impact). You cannot decrease storage size. Compute: Changing instance type requires a brief reboot (downtime). ‚ö†Ô∏è Warning: Maximum storage size is 64 TiB for most engines, but 16 TiB for SQL Server.\nPerformance RDS uses EBS volumes for storage. Choose the right type for your workload:\nGeneral Purpose (SSD - gp2/gp3): Balanced performance for most workloads. Cost-effective. Provisioned IOPS (SSD - io1/io2): For I/O intensive, latency-sensitive workloads. You specify the exact IOPS needed. Magnetic: Legacy storage, not recommended for new workloads. Multi-AZ and Read Replicas Feature Multi-AZ Deployments Read Replicas Purpose High Availability (HA) \u0026amp; Disaster Recovery (DR) Read Scalability \u0026amp; Performance Replication Synchronous (Zero data loss) Asynchronous (Eventual consistency) Active Nodes Only Primary is active All replicas are active for reads Backups Taken from Standby (no I/O impact on Primary) Not configured by default Failover Automatic Manual promotion required Multi-AZ Details ‚ÑπÔ∏è Information: Multi-AZ creates a standby replica in a different Availability Zone.\nAutomatic Failover: Triggered by infrastructure failure, network loss, or instance failure. Seamless: DNS endpoint automatically updates to point to the standby. Recommendation: Use Provisioned IOPS for Multi-AZ to ensure consistent replication performance. üí° Pro Tip: Always use the DNS endpoint in your application connection strings, never the IP address, to ensure failover works correctly.\n‚ö†Ô∏è Warning: The standby instance in a Multi-AZ setup cannot be used for read traffic.\nRead Replicas Details ‚ÑπÔ∏è Information: Offload read traffic from your primary instance to Read Replicas.\nScalability: Up to 5 read replicas per master. Flexibility: Can be in the same AZ, different AZ, or even a different Region (Cross-Region). Promotion: A Read Replica can be manually promoted to a standalone master database. üí° Pro Tip: You can promote a Read Replica to become the new master. This process takes a few minutes.\nDB Snapshots ‚ÑπÔ∏è Information: User-initiated backups of your instance.\nStored on S3 indefinitely until you delete them. Restoration: Creates a brand new DB instance with a new endpoint. Sharing: Snapshots can be shared with other AWS accounts. üí° Pro Tip: Always take a final snapshot before deleting a production database.\nMonitoring Use these tools to keep your database healthy:\nAmazon CloudWatch: Metrics (CPU, memory, disk I/O) and Alarms. Enhanced Monitoring: Real-time OS metrics. Performance Insights: Visualizes database load and helps identify bottlenecks. RDS Events: Notifications about configuration changes or failovers. üí° Pro Tip: Combine these tools for a comprehensive view of your database\u0026rsquo;s health and performance.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Part I: Introduction to Kiro.dev (AWS Agentic IDE) Kiro.dev (pronounced ‚Äúkeer-oh‚Äù) is an agentic AI-powered IDE (Integrated Development Environment) developed by a small team at Amazon Web Services (AWS). It is currently in public preview.\n1. Goal \u0026amp; Key Features Goal: Bridge the gap between \u0026ldquo;vibe coding\u0026rdquo; (rapid prototyping) and structured, tested, documented, and maintainable real-world software development. Key Features: TaSpec-Driven Development: Converts requirements into user stories, acceptance criteria, design documents, and task lists. Agent Hooks / Automation: Automatically triggers tasks like documentation generation, testing, and optimization upon events (file save, commit). Steering: Uses Markdown files to clearly define project structure, standards, and architecture. Multi-file Capability: Understands functional goals and executes necessary changes across multiple files. 2. Pros and Cons Type Details Pros Increased transparency \u0026amp; control (spec review before code), Reduced boilerplate burden, Security \u0026amp; privacy (local code generation), Flexibility (external tool integration via Model Context Protocol). Cons Still in preview (incomplete features), May struggle with \u0026ldquo;context understanding\u0026rdquo; in complex projects (requires user supervision), Costs associated with agent interaction. 3. Usage Recommendations Kiro is suitable if you want to maintain a professional, structured workflow while leveraging AI for rapid prototyping, or if you want AI to be a \u0026ldquo;coding partner\u0026rdquo; rather than just a code suggestion tool.\nPart II: Important Notes on Effective AI Usage Here are core experiences and principles when working with AI in software development projects:\n1. Control Principles User must be in control: AI is just an assistant; do not rely on it to control the project. Plan First \u0026amp; Review Frequently: You MUST CREATE A PLAN FIRST (e.g., ask AI to create a plan file) to have a working framework and must frequently review (don\u0026rsquo;t expect it to do everything). Code Manager Role: Your value lies in code validation. You must be the one managing the code generated by AI. 2. Prompt Engineering \u0026amp; Requirement Definition Specific Role: Assign a specific role to the AI (e.g., suggest using Amazon Q/CodeWhisperer). Prompt Templates: Ask AI to create a plan, then filter and refine it. Request it to export files for storage and direct editing later. Clear Definition: Create a new section when writing user stories. Define technology clearly for AI to follow. Instead of saying \u0026ldquo;don\u0026rsquo;t implement this,\u0026rdquo; say \u0026ldquo;implement this part\u0026rdquo; to increase success probability. Detailed Collaboration: Collaborate with AI to create clear requirements and detailed information before asking AI to design and proceed. 3. Project Management Break Down Scope: Divide large scope into scope units; each unit is a small project for easier implementation. Collaborate \u0026amp; Verify: Ask AI to write user stories, then base project time estimation on them. Must have a team working together and verify all output code. Basic Process: Requirement $\\implies$ Unit $\\implies$ Data Model $\\implies$ \u0026hellip; $\\implies$ Verify. Part III: Software Development Process Diagram (Agile/Scrum) The software development process typically follows Agile/Scrum, involving roles such as Product Owner, Developer, and Tester.\n1. Development Cycle Starts from the Backlog (list of requirements), requirements are prioritized and assigned to each Sprint. In each Sprint, the development team performs key activities: Design, Code, and Test. 2. Deliverables \u0026amp; Deployment Environments Artifacts: Design documents, content maps, service interfaces, source code, configurations. Deployment Environments: Products are sequentially deployed through environments: Development (Dev) Testing (QA) User Acceptance Testing (UAT) Production (Prod) ‚Äì Official running environment. This process ensures the relationship between roles, activities, results, and deployment environments, helping control product quality before release to users.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "AWS CLOUD MASTERY SERIES #1 EVENT SUMMARY (15/11/2025) PART I: GENERATIVE AI WITH AMAZON BEDROCK 1. Foundation Model and Prompt Engineering The event posed a key question: What knowledge should be learned to fit the external cloud environment?\nFoundation Model: Definition of foundation models. Prompt Engineering Techniques: Techniques to optimize input for AI models. Few-shot Prompting: Providing a few specific examples to guide the model. Chain of Thought (CoT): Asking the model to show reasoning steps to arrive at the final answer, helping to increase accuracy. 2. Retrieval Augmented Generation (RAG) The event delved into RAG and the concept of Embedding.\nWhat is Embedding? (Concept introduced). Tools supporting RAG: Amazon Titan Embedding (tool introduced). RAG in Action: Illustration of the RAG workflow, including steps: User Query Embedding Model (converts question into vector) Vector Store / Knowledge Source (searches for relevant information) Prompt Template / Prompt Alignment Model (inserts found information into prompt) Large Language Model (LLM) Response (answer supplemented with external knowledge). RetrieveAndGenerate API: An API introduced to implement RAG. 3. Amazon Bedrock AgentCore Amazon Bedrock AgentCore is a platform that helps realize your AI applications by taking AI agents from the experimental stage to production.\nFunction: Strong support for runtime, memory, tools, security, and monitoring for agents. PART II: OTHER PRETRAINED AI SERVICES The event also introduced a range of specialized, pre-trained AI services from Amazon, along with their corresponding costs:\nAI Service Main Function Reference Price Amazon Rekognition Image and video analysis, labeling, automatic sensitive content censoring. $0.0013/image (\u0026lt;1M images) Amazon Translate Real-time text recognition and translation. $15/1M characters Amazon Textract Extract texts and layouts from documents. $0.05/page (\u0026lt;1M pages) ‚üπ needs consideration. Amazon Transcribe Speech to text. Supports sensitive word censoring, voice recognition, and automatic translation. $0.024/minute (\u0026lt;250k minutes) Amazon Polly Text-to-speech. $4 per 1M characters (first 1M free). Amazon Comprehend Natural Language Processing (NLP). Understands text, video, keyword filtering. $0.0001/100 characters + $0.35/1h Amazon Kendra SEARCH function (or question answering). $30/index/month + $0.35/1h (extremely expensive) Amazon Personalize Personalize user experience (like TikTok, Shopee, Facebook). $0.24/training hour + $0.05/GB/recommendation Amazon Lookout Family Includes Lookout for Equipment and Lookout for Vision (for monitoring and anomaly detection). (No specific price in notes) Pipecat (Introduced, no functional details). (No specific price in notes) "
},
{
	"uri": "http://localhost:1313/fcj-workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguy·ªÖn VƒÉn C∆∞·ªùng\nPhone Number: 0349079940\nEmail: cuongnvse183645@fpt.edu.vn\nUniversity: FPT university HCM\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: No. Task Start Date Completion Date Reference Material 1 Learn about AWS Free Tier, create AWS account 08/09/2025 08/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Billing Dashboard, create Budget and cost alerts 09/09/2025 09/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Study IAM: User, Group, Role, Policy 10/09/2025 10/09/2025 AWS Docs, AWS Training, FCJ Playlist 4 Configure MFA for root and user accounts 11/09/2025 11/09/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create 2 IAM users (Dev \u0026amp; Admin) with different permissions 12/09/2025 12/09/2025 AWS Docs, AWS Training, FCJ Playlist 6 Launch EC2 instance (Linux) 15/09/2025 15/09/2025 AWS Docs, AWS Training, FCJ Playlist Week 1 Achievements: Learned about AWS Free Tier and successfully created an AWS account.\nConfigured Billing Dashboard, set up Budget and cost alerts to monitor expenses.\nStudied IAM concepts: User, Group, Role, Policy.\nConfigured MFA for both root and user accounts to enhance security.\nCompleted a mini project: Created 2 IAM users (Dev \u0026amp; Admin) with different permissions.\nLaunched an EC2 instance (Linux) and practiced basic management tasks.\nBecame familiar with the AWS Management Console and AWS CLI for managing resources.\nDeveloped the ability to connect and manage AWS resources using both the web interface and CLI in parallel.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand AWS service groups (Compute, Storage, Networking, Database). Learn to use AWS Console and AWS CLI for managing resources. Practice EC2 and EBS setup and SSH connection. Tasks to be carried out this week: No. Task Start Date Completion Date Reference Material 1 Get acquainted with FCJ members; read and note internship regulations. 11/08/2025 11/08/2025 AWS Docs, AWS Training, FCJ Playlist 2 Learn about AWS and its types of services (Compute, Storage, Networking, Database, etc.) 12/08/2025 12/08/2025 AWS Docs, AWS Training, FCJ Playlist 3 Create AWS Free Tier account; learn AWS Console and CLI setup. 13/08/2025 13/08/2025 AWS Docs, AWS Training, FCJ Playlist 4 Study basic EC2: instance types, AMI, EBS, Elastic IP, and SSH connection methods. 14/08/2025 15/08/2025 AWS Docs, AWS Training, FCJ Playlist 5 Practice launching EC2 instance, connecting via SSH, and attaching EBS volume. 15/08/2025 15/08/2025 AWS Docs, AWS Training, FCJ Playlist Week 2 Achievements: Understood what AWS is and mastered core service groups (Compute, Storage, Networking, Database, \u0026hellip;). Created and configured AWS Free Tier account successfully. Learned how to navigate AWS Management Console and locate services efficiently. Installed and configured AWS CLI with Access Key, Secret Key, and Default Region. Practiced using AWS CLI for: Checking account \u0026amp; configuration Listing regions Viewing EC2 services Creating and managing key pairs Viewing active resources Gained the ability to manage AWS resources in parallel via Console and CLI. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Continue building foundational AWS knowledge. Practice with IAM, EC2, and basic networking setup. Begin deploying a simple website on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Explore AWS Free Tier and create an AWS account 08/09/2025 08/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Billing Dashboard, create Budget \u0026amp; Cost Alerts 09/09/2025 09/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Learn IAM: User, Group, Role, Policy 10/09/2025 10/09/2025 AWS Docs, AWS Training, FCJ Playlist 4 Enable MFA for Root and User accounts 11/09/2025 11/09/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create 2 IAM Users (Dev \u0026amp; Admin) with different permissions 12/09/2025 12/09/2025 AWS Docs, AWS Training, FCJ Playlist 6 Create an EC2 instance (Linux) 15/09/2025 15/09/2025 AWS Docs, AWS Training, FCJ Playlist 7 SSH into EC2 and install Apache/Nginx 16/09/2025 16/09/2025 AWS Docs, AWS Training, FCJ Playlist 8 Learn Security Group, assign Elastic IP 17/09/2025 17/09/2025 AWS Docs, AWS Training, FCJ Playlist 9 Deploy a small application on EC2 18/09/2025 18/09/2025 AWS Docs, AWS Training, FCJ Playlist 10 Mini Project: Deploy a static website on EC2 19/09/2025 19/09/2025 AWS Docs, AWS Training, FCJ Playlist Week 3 Achievements: Understood IAM structure and how to manage users, groups, and roles. Configured MFA and budget alerts for account security and cost control. Created, launched, and connected to EC2 instances. Learned to manage Elastic IPs and security groups. Successfully deployed a static website on EC2. Practiced hands-on with AWS Console and CLI. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand AWS networking concepts: VPC, Subnet, Internet Gateway, Route Table. Learn the difference between Security Groups and NACLs. Deploy basic Load Balancer (ALB) and EC2 instances inside a custom VPC. Complete a mini project combining VPC, EC2, and ALB. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Launch a custom VPC and create Subnets 29/09/2025 29/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Internet Gateway and Route Table for the VPC 30/09/2025 30/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Learn the difference between Security Groups and NACLs 01/10/2025 01/10/2025 AWS Docs, AWS Training, FCJ Playlist 4 Deploy a basic Application Load Balancer (ALB) 02/10/2025 02/10/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create a private VPC and deploy 2 EC2 instances behind the ALB 03/10/2025 03/10/2025 AWS Docs, AWS Training, FCJ Playlist Week 4 Achievements: Successfully created a custom VPC and configured Subnets.\nConfigured Internet Gateway and Route Tables to allow proper routing in the VPC.\nLearned the differences between Security Groups (stateful) and NACLs (stateless) and applied them.\nDeployed a basic Application Load Balancer (ALB) to distribute traffic to multiple EC2 instances.\nCompleted mini project: created a private VPC and deployed 2 EC2 instances behind the ALB, gaining hands-on experience in networking and load balancing.\nBecame more confident in managing AWS networking resources and deploying a simple high-availability architecture.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn advanced AWS networking concepts and practice with Security Groups. Understand Elastic Load Balancer (ALB) and Auto Scaling basics. Deploy a mini project combining EC2, VPC, ALB, and Security Groups. Practice monitoring and basic troubleshooting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review VPC, Subnet, and Route Table configuration 06/10/2025 06/10/2025 AWS Docs, FCJ Playlist 2 Learn and configure Security Groups for EC2 instances 07/10/2025 07/10/2025 AWS Docs, FCJ Playlist 3 Deploy Application Load Balancer (ALB) and connect EC2 instances 08/10/2025 08/10/2025 AWS Docs, FCJ Playlist 4 Understand and practice Auto Scaling Groups 09/10/2025 09/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy 2 EC2 instances behind an ALB with proper Security Groups and basic Auto Scaling 10/10/2025 10/10/2025 AWS Docs, FCJ Playlist Week 5 Achievements: Reviewed VPC, Subnet, and Route Table setups and ensured proper connectivity.\nConfigured Security Groups to allow necessary traffic while keeping instances secure.\nSuccessfully deployed Application Load Balancer (ALB) and connected EC2 instances.\nLearned the basics of Auto Scaling Groups and applied them to EC2 instances.\nCompleted mini project: 2 EC2 instances deployed behind ALB with Security Groups, gaining hands-on experience with networking, load balancing, and scaling.\nImproved skills in monitoring and troubleshooting basic AWS networking issues.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn VPC, Subnet, and Route Table concepts for EC2 networking. Practice configuring Security Groups and Elastic IPs. Deploy EC2 instances in a VPC with proper networking and access. Complete a mini project integrating EC2, Security Groups, and Elastic IP. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review VPC, Subnet, and Route Table concepts 13/10/2025 13/10/2025 AWS Docs, FCJ Playlist 2 Configure Security Groups and test inbound/outbound rules 14/10/2025 14/10/2025 AWS Docs, FCJ Playlist 3 Assign Elastic IPs to EC2 instances and test connectivity 15/10/2025 15/10/2025 AWS Docs, FCJ Playlist 4 Launch EC2 instances within the VPC, assign Subnets, Security Groups, and Elastic IP 16/10/2025 16/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy a basic web application on EC2 instances with proper networking settings 17/10/2025 17/10/2025 AWS Docs, FCJ Playlist Week 6 Achievements: Reviewed and understood VPC, Subnet, and Route Table configurations.\nConfigured Security Groups correctly to allow necessary traffic and secure EC2 instances.\nAssigned Elastic IPs to EC2 instances and verified public accessibility.\nLaunched EC2 instances inside the VPC with correct networking, Security Groups, and Elastic IP setup.\nCompleted mini project: deployed a basic web application on EC2 with proper networking and security.\nStrengthened hands-on skills with EC2 networking, access control, and public IP management.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn AWS S3 concepts: buckets, objects, and storage classes. Practice creating S3 buckets, uploading files, and setting permissions. Understand S3 bucket policies and public/private access. Complete a mini project: deploy static files to S3 and test access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review S3 concepts: bucket, object, storage classes 20/10/2025 20/10/2025 AWS Docs, FCJ Playlist 2 Create S3 buckets and practice uploading files 21/10/2025 21/10/2025 AWS Docs, FCJ Playlist 3 Configure bucket permissions and test public/private access 22/10/2025 22/10/2025 AWS Docs, FCJ Playlist 4 Apply S3 bucket policies for specific access control 23/10/2025 23/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy static website files to S3 and test access 24/10/2025 24/10/2025 AWS Docs, FCJ Playlist Week 7 Achievements: Learned S3 storage concepts and usage.\nSuccessfully created S3 buckets, uploaded files, and organized objects.\nApplied bucket permissions and policies for secure and controlled access.\nCompleted mini project: deployed static files to S3 and verified accessibility.\nEnhanced hands-on skills with AWS S3, file management, and access control.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Learn advanced AWS S3 features: versioning, lifecycle, and bucket policies. Practice setting up S3 bucket policies and permissions. Understand S3 event notifications and integration with Lambda. Introduction to CloudFront for content delivery. Complete a mini project: host a static website on S3 + CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review basic S3 and upload files 27/10/2025 27/10/2025 AWS Docs, FCJ Playlist 2 Enable versioning on S3 buckets and test file versioning 28/10/2025 28/10/2025 AWS Docs, FCJ Playlist 3 Configure bucket policies and permissions 29/10/2025 29/10/2025 AWS Docs, FCJ Playlist 4 Learn about S3 event notifications and trigger Lambda functions 30/10/2025 30/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy a static website on S3 and distribute via CloudFront 31/10/2025 31/10/2025 AWS Docs, FCJ Playlist Week 8 Achievements: Understood advanced S3 features including versioning, lifecycle rules, and policies.\nSuccessfully configured bucket policies for secure and controlled access.\nPracticed triggering Lambda functions via S3 event notifications.\nDeployed a static website on S3 and distributed it via CloudFront for faster content delivery.\nImproved hands-on skills with AWS S3, security policies, version control, and content delivery.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn about AWS RDS (Relational Database Service) and database management. Understand different database engines: MySQL, PostgreSQL, SQL Server. Practice creating RDS instances and connecting from local/EC2. Learn backup, snapshot, and restore methods in RDS. Understand security best practices for database access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review basic database concepts and AWS RDS documentation 03/11/2025 03/11/2025 AWS Docs, FCJ Playlist 2 Create RDS instances (MySQL \u0026amp; PostgreSQL) 04/11/2025 04/11/2025 AWS Docs, FCJ Playlist 3 Connect RDS instances from local machine and EC2 05/11/2025 05/11/2025 AWS Docs, FCJ Playlist 4 Configure automatic backups and manual snapshots 06/11/2025 06/11/2025 AWS Docs, FCJ Playlist 5 Test restore from snapshot and review database security \u0026amp; parameter groups 07/11/2025 07/11/2025 AWS Docs, FCJ Playlist Week 9 Achievements: Understood AWS RDS and its advantages for managed databases.\nSuccessfully created MySQL and PostgreSQL RDS instances.\nConnected to RDS instances from local machines and EC2 instances.\nConfigured automatic backups, created snapshots, and tested restore procedures.\nLearned best practices for database security including IAM roles, security groups, and password management.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Learn about AWS Free Tier, Billing Dashboard, IAM, MFA, EC2 and complete a mini IAM project\nWeek 2: Get familiar with AWS CLI, IAM roles, and basic S3 operations\nWeek 3: Learn EC2 in detail: Instance types, AMI, EBS, Security Groups, and SSH connection\nWeek 4: Practice launching EC2 instances, attaching EBS, and connecting via SSH\nWeek 5: Understand AWS networking: VPC, Subnets, Internet Gateway, and Elastic IPs\nWeek 6: Practice configuring VPC, subnet, security group rules, and EC2 networking setup\nWeek 7: Explore RDS, DynamoDB, and basic database configuration on AWS\nWeek 8: Learn S3 advanced features: versioning, lifecycle rules, bucket policies, and static website hosting\nWeek 9: Connect with FCJ members, review AWS basics, create AWS account, practice EC2 launch and EBS attachment\nWeek 10: Consolidate AWS skills: EC2, EBS, CLI commands, and Management Console usage\nWeek 11: Practice advanced AWS CLI commands, automate EC2 creation, and integrate basic services\nWeek 12: Learn CloudFormation, deploy EC2 + S3 stacks, integrate Lambda \u0026amp; API Gateway, test automation, and finalize internship report\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/2-create-ec2-sg/",
	"title": "Create EC2 Security Group",
	"tags": [],
	"description": "",
	"content": "Creating a Security Group for EC2 Instances ‚ÑπÔ∏è Information: A Security Group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. In this step, we will create a security group that allows traffic to our web application.\nStep-by-Step Guide Navigate to the EC2 Dashboard in the AWS Management Console.\nIn the left navigation pane, under Network \u0026amp; Security, select Security Groups.\nClick Create security group.\nBasic details:\nSecurity group name: Enter a descriptive name (e.g., EC2-Web-SG). Description: Enter a description (e.g., Allow Web and SSH access). VPC: Select the VPC you created in the previous step. Inbound rules: Click Add rule to allow the following traffic:\nType Protocol Port Range Source Description HTTP TCP 80 0.0.0.0/0 Allow HTTP access from anywhere HTTPS TCP 443 0.0.0.0/0 Allow HTTPS access from anywhere Custom TCP TCP 5000 0.0.0.0/0 Allow access to application port SSH TCP 22 My IP Allow SSH access only from your IP üîí Security Note: For SSH (Port 22), always select My IP as the source to restrict administrative access to your current location only. Never open SSH to 0.0.0.0/0 in a production environment.\nOutbound rules: Leave the default rule (Allow all traffic) unchanged.\nClick Create security group.\nNote the Security Group ID (e.g., sg-xxxxxxxx), as you will need it later.\nüí° Pro Tip: Security Groups are stateful. This means if you allow an inbound request, the response is automatically allowed to flow back out, regardless of outbound rules.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Overview ‚ÑπÔ∏è Information: Before we can launch our Amazon RDS database, we need to lay the groundwork. This involves setting up a secure and robust network infrastructure within your AWS environment.\nIn this section, we will walk through the creation of the following essential components:\nVirtual Private Cloud (VPC): The isolated network environment for your resources. Subnets: Network segments distributed across multiple Availability Zones to ensure high availability. Security Groups: Virtual firewalls to control traffic for both your application (EC2) and database (RDS). DB Subnet Group: A collection of subnets that tells RDS where it can provision your database instances. Why is this important? Setting up these components correctly is crucial for:\nSecurity: Isolating your database from public internet access. Availability: Ensuring your database can survive data center failures (Multi-AZ). Connectivity: Allowing your application servers to communicate securely with your database. üí° Pro Tip: Always design your network with High Availability in mind. By creating subnets in at least two different Availability Zones now, you enable the option for Multi-AZ deployments later.\nüîí Security Note: We will follow the principle of Least Privilege. Our Security Groups will be configured to allow only necessary traffic on specific ports from authorized sources.\n‚ö†Ô∏è Warning: You cannot create a Multi-AZ RDS deployment if your DB Subnet Group does not span at least two Availability Zones.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Customer Background GameTracker is a platform designed for game players and admins to manage, track, and share information about characters, weapons, banners, items, and events.\nBusiness and Technical Objectives\nUnified Management: Create a centralized system for managing game data effectively. Accessibility: Provide easy access to information for English-language games which can be a barrier for some users. Efficiency: Reduce manual maintenance time and improve data reliability for admins. Scalability: Build a system scalable to multiple games and community features using serverless architecture. Use Cases\nPlayers: Track gacha history, simulate pulls, view banner/event timelines. Admins: Manage game data (CRUD) with clear access permissions. Partner Professional Services We will deliver a full-stack web application hosted on AWS, utilizing serverless technologies (Lambda, S3, RDS) to ensure low operational costs and high availability.\n1.2 PROJECT SUCCESS CRITERIA System Stability: Stable, auto-scaling system with low maintenance costs. Security: Secure API with centralized data management and role-based access control. User Engagement: Functional gacha tools and timelines that help players track game events conveniently. Scalability: Architecture ready for easy expansion to support more games and features. 1.3 ASSUMPTIONS Prerequisites: AWS account access with necessary permissions for deployment. Dependencies: Third-party authentication (Google OAuth2). Constraints: Budget constraints requiring a low-cost serverless approach. Risks: Potential cold start latency with Lambda (mitigated by warmers), RDS costs (mitigated by instance selection). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM Proposed High-Level Architecture: The solution adopts a modern cloud-native architecture:\nFrontend: React SPA served via S3 + CloudFront, protected by AWS WAF. Backend: Spring Boot serverless deployed on AWS Lambda, using JWT and Google OAuth2 for auth. Database: SQL Server on AWS RDS. Storage: AWS S3 for storing static assets (avatars, backgrounds, weapons). Security: AWS WAF, IAM, and Spring Security. AWS Services Used:\nAWS S3, AWS Lambda, AWS RDS, AWS CloudFront, AWS WAF, AWS SES, AWS IAM. 2.2 TECHNICAL PLAN We will develop scripts using AWS CDK/CloudFormation or manual setup procedures documented for repeatability.\nFrontend: React, TypeScript, Vite. Backend: Spring Boot, Spring Security. DevOps: Docker, CI/CD pipelines. All critical paths including user login, data synchronization, and gacha simulation will include extensive test coverage.\n2.3 PROJECT PLAN The project will follow an Agile methodology over a 1-month timeline.\nWeek 1: Planning, Requirements Analysis, Architecture Design. Week 2: Backend Development (API, Auth, Database). Week 3: Frontend Development (UI/UX, Admin Dashboard, Tools). Week 4: Deployment, Testing, Documentation, and Handover. 2.4 SECURITY CONSIDERATIONS Best Practices Implemented:\nIdentity: Google OAuth2 integration and JWT for secure stateless authentication. Infrastructure: AWS WAF to protect against common web exploits. Access Control: Role-based access control (RBAC) for Admins vs Users. Data Protection: HTTPS encryption in transit; RDS encryption at rest. Monitoring: AWS CloudWatch for logs and metrics. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Project Phase Timeline Activities Deliverables/Milestones Assessment \u0026amp; Setup Week 1 Requirement analysis, architecture design, AWS setup (S3, RDS, Lambda) Architecture Diagram, AWS Environment Ready Backend Implementation Week 2 Build Lambda functions, API endpoints, Auth integration, DB schema Working API, Database connectivity Frontend Implementation Week 3 React SPA development, Dashboard creation, Gacha tools logic Functional Web UI, Admin Dashboard Testing \u0026amp; Go-live Week 4 Integration testing, Security optimization, Deployment to CloudFront Deployed Application, User Guide, Documentation 3.2 OUT OF SCOPE Mobile Application development (iOS/Android native apps). Real-time multiplayer game server features (only web-based data management). Integration with game servers directly (data is manually managed/imported). 3.3 PATH TO PRODUCTION The current proposal outlines the path to a production-ready MVP.\nPOC to Prod: The system is designed to be production-grade from the start using AWS managed services. Gaps: Further load testing and fine-tuning of WAF rules may be needed based on actual traffic patterns. Operations: Error handling and monitoring are implemented via CloudWatch. 4. EXPECTED AWS COST BREAKDOWN BY SERVICES Estimated Monthly Cost: ~$121-123/month\nAWS Lambda: ~$5-7 (Memory: 3008 MB, ~12k invocations). S3 Standard: ~$0.23 (10 GB storage). CloudFront: ~$8.50 (100 GB egress). RDS (SQL Server): ~$60+ (db.t3.medium or similar). AWS WAF: ~$10 (Web ACL + requests). NAT Gateway: ~$32 (if required for Lambda in VPC). Others (SES, Route53, CloudWatch): ~$6. Note: Costs are estimates and depend on actual usage and region.\n5. TEAM Partner Project Team\nName Title Role Email / Contact Info [Name] Delivery Manager Project Manager [Email] [Name] Sr. Solutions Architect Technical Lead [Email] Project Stakeholders\nName Title Stakeholder for Email / Contact Info [Name] [Title] [Role] [Email] 6. RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD) / Hour Solution Architect System Design \u0026amp; Lead - Full-stack Engineer Implementation - Total Estimated Effort: [Total Man-days]\n7. ACCEPTANCE Upon completion of a Phase, the Partner will submit the associated tangible Deliverables to the Customer. The Customer will review, evaluate, and test the Deliverables within eight (8) business days (the ‚ÄúAcceptance Period‚Äù) to determine satisfaction of acceptance criteria.\nIf the Deliverable satisfies its acceptance criteria, Customer will furnish a written acceptance. If rejected, Customer will indicate detailed reasons, and Partner will correct defects. If no rejection is received within the Acceptance Period, Deliverables are deemed accepted.\nüîó Project Website: https://d2eu9it59oopt8.cloudfront.net/\r"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Learn about AWS S3 (Simple Storage Service) and CloudFront (CDN) services. Understand S3 bucket creation, permissions, and storage classes. Practice uploading files to S3 and configuring public/private access. Learn about CloudFront distributions to deliver content globally. Understand versioning, lifecycle policies, and basic security best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review S3 concepts: bucket, object, storage classes, permissions 10/11/2025 10/11/2025 AWS Docs, FCJ Playlist 2 Create S3 buckets and upload test files 11/11/2025 11/11/2025 AWS Docs, FCJ Playlist 3 Configure S3 permissions, public/private access, and versioning 12/11/2025 12/11/2025 AWS Docs, FCJ Playlist 4 Set up CloudFront distribution with S3 as origin 13/11/2025 13/11/2025 AWS Docs, FCJ Playlist 5 Test global content delivery and implement lifecycle policies \u0026amp; security best practices 14/11/2025 14/11/2025 AWS Docs, FCJ Playlist Week 10 Achievements: Understood AWS S3 and CloudFront, their use cases, and advantages.\nSuccessfully created S3 buckets and uploaded files with correct permissions.\nConfigured CloudFront distribution to deliver content globally with caching.\nLearned about versioning, lifecycle policies, and basic security best practices for S3.\nGained hands-on experience in managing storage and content delivery on AWS.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Learn about AWS Lambda and serverless computing. Understand how to create Lambda functions, triggers, and manage versions. Learn about API Gateway to expose Lambda functions as HTTP endpoints. Practice integrating Lambda with S3 and other AWS services. Understand basic monitoring and logging with CloudWatch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review serverless concepts and Lambda architecture 17/11/2025 17/11/2025 AWS Docs, FCJ Playlist 2 Create simple Lambda functions and test execution 18/11/2025 18/11/2025 AWS Docs, FCJ Playlist 3 Set up triggers for Lambda (S3 event, API Gateway, CloudWatch event) 19/11/2025 19/11/2025 AWS Docs, FCJ Playlist 4 Configure API Gateway to expose Lambda functions as REST endpoints 20/11/2025 20/11/2025 AWS Docs, FCJ Playlist 5 Monitor Lambda execution with CloudWatch logs and metrics 21/11/2025 21/11/2025 AWS Docs, FCJ Playlist Week 11 Achievements: Understood serverless architecture and AWS Lambda concepts. Created Lambda functions and tested execution successfully. Set up triggers from S3, API Gateway, and CloudWatch events. Exposed Lambda functions via API Gateway endpoints. Monitored Lambda executions with CloudWatch and learned basic troubleshooting. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Review and consolidate all AWS knowledge learned during the internship. Learn basic automation using AWS CloudFormation and Infrastructure as Code (IaC). Deploy simple stacks with CloudFormation. Review Lambda, S3, EC2, and API Gateway integration. Evaluate internship outcomes and prepare final report. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review all AWS services and concepts studied during internship 24/11/2025 24/11/2025 AWS Docs, FCJ materials 2 Learn CloudFormation basics and template syntax 25/11/2025 25/11/2025 AWS Docs, FCJ Playlist 3 Deploy a simple CloudFormation stack to create EC2 + S3 resources 26/11/2025 26/11/2025 AWS Docs, FCJ Playlist 4 Integrate Lambda and API Gateway into CloudFormation stack 27/11/2025 27/11/2025 AWS Docs, FCJ Playlist 5 Test automation, troubleshoot, and prepare summary of all accomplishments 28/11/2025 28/11/2025 AWS Docs, FCJ Playlist Week 12 Achievements: Consolidated AWS knowledge and practical skills gained during internship. Learned basic automation and Infrastructure as Code (IaC) using CloudFormation. Deployed a CloudFormation stack with EC2, S3, Lambda, and API Gateway. Tested automation, monitored resources, and practiced troubleshooting. Completed internship report and evaluation. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-create-ec2/",
	"title": "Create EC2 instance",
	"tags": [],
	"description": "",
	"content": "Creating an EC2 Instance ‚ÑπÔ∏è Information: Amazon EC2 (Elastic Compute Cloud) provides scalable computing capacity in the AWS Cloud. It allows you to launch virtual servers (instances) in minutes, eliminating the need to invest in upfront hardware.\nIn this step, we will launch a Linux EC2 instance that will serve as our application server (or bastion host) to connect to our RDS database.\nStep-by-Step Guide Access the Console: Open the Amazon EC2 console.\nLaunch Instance: On the dashboard, click the orange Launch instance button.\nName and Tags:\nName: Enter a descriptive name (e.g., Workshop-Web-Server). Application and OS Images (AMI):\nQuick Start: Select Amazon Linux. AMI: Select Amazon Linux 2023 AMI (Free tier eligible). üí° Pro Tip: Always look for the \u0026ldquo;Free tier eligible\u0026rdquo; tag to avoid unexpected costs during learning or testing.\nInstance Type:\nSelect t2.micro (or t3.micro if t2 is unavailable in your region). ‚ÑπÔ∏è Information: These instance types are low-cost and often covered by the AWS Free Tier.\nKey Pair (Login):\nSelect the Key pair you created earlier. ‚ö†Ô∏è Warning: Do not proceed without a key pair. You will not be able to SSH into your instance without it.\nNetwork Settings:\nClick Edit. VPC: Select your workshop VPC. Subnet: Select a Public Subnet. Auto-assign Public IP: Ensure this is Enabled. Firewall (security groups): Select Select existing security group. Choose the EC2 Security Group you created in step 5.2.2. üîí Security Note: By reusing the security group we created earlier, we ensure our instance has exactly the permissions it needs‚Äîno more, no less.\nLaunch:\nReview your summary. Click Launch instance. Verify:\nClick View all instances. Wait for the Instance state to change to Running and Status check to pass. üí° Pro Tip: If you don\u0026rsquo;t see the Public DNS column, click the settings gear icon and enable Public IPv4 DNS.\nConnecting via SSH (MobaXterm) ‚ÑπÔ∏è Information: MobaXterm is a powerful terminal for Windows that makes SSH connections easy.\nDownload \u0026amp; Install: Get MobaXterm from the official website.\nCreate Session:\nClick Session \u0026gt; SSH. Remote host: Enter your EC2 instance\u0026rsquo;s Public IPv4 DNS. Specify username: Enter ec2-user. Port: 22. Authentication:\nGo to the Advanced SSH settings tab. Check Use private key. Browse and select your .pem key file. Connect:\nClick OK. You should now be connected to your Linux server! üîí Security Note: Keep your .pem key file secure. Never share it or commit it to public repositories.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/3-create-db-sg/",
	"title": "Create RDS Security Group",
	"tags": [],
	"description": "",
	"content": "Creating a Security Group for Amazon RDS ‚ÑπÔ∏è Information: Just like our EC2 instance, our RDS database needs a Security Group to control traffic. This security group will act as a firewall, allowing access only from our application servers.\nStep-by-Step Guide In the VPC Dashboard, select Security Groups from the left navigation pane.\nClick Create security group.\nBasic details:\nSecurity group name: Enter a descriptive name (e.g., RDS-MySQL-SG). Description: Enter a description (e.g., Allow MySQL access from EC2). VPC: Select the same VPC as before. Inbound rules: Click Add rule to allow database connections:\nType Protocol Port Range Source Description MySQL/Aurora TCP 3306 sg-xxxxxxxx (EC2-Web-SG) Allow MySQL access from EC2 SG üîí Security Note: Instead of entering an IP address (like 0.0.0.0/0), select the Security Group ID of the EC2 Security Group you created in the previous step. This ensures that only instances belonging to that specific security group can connect to your database. This is a best practice for security.\nOutbound rules: Leave the default rule (Allow all traffic) unchanged.\nClick Create security group.\nYou now have a dedicated security group for your database layer.\n‚ö†Ô∏è Warning: Never use the same Security Group for both your EC2 instances and your RDS instances. Separation of duties is key to a secure architecture.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/4-create-db-subnetgroup/",
	"title": "Create DB Subnet Group",
	"tags": [],
	"description": "",
	"content": "Creating a DB Subnet Group for Amazon RDS ‚ÑπÔ∏è Information: A DB Subnet Group is a collection of subnets (typically private) that you designate for your RDS instances. It tells RDS which subnets and IP ranges it can use in your VPC.\n‚ö†Ô∏è Warning: To enable Multi-AZ deployments (High Availability), your DB Subnet Group must include subnets in at least two different Availability Zones.\nStep-by-Step Guide Navigate to the Amazon RDS console.\nIn the left navigation pane, select Subnet groups.\nClick Create DB Subnet Group.\nSubnet group details:\nName: Enter a name (e.g., rds-subnet-group). Description: Enter a description (e.g., Subnet group for RDS). VPC: Select your VPC. Add subnets:\nAvailability Zones: Select the Availability Zones where you created your private subnets (e.g., us-east-1a and us-east-1b). Subnets: Select the specific Private Subnet IDs associated with those AZs. üîí Security Note: Always select Private Subnets for your database to ensure it is not directly accessible from the internet.\nClick Create.\nYour DB Subnet Group is now ready.\nüí° Pro Tip: If you are using AWS Local Zones, you can also include them here to extend your database closer to your end-users.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-create-rds/",
	"title": "Create RDS database instance",
	"tags": [],
	"description": "",
	"content": "Preparing the EC2 Environment ‚ÑπÔ∏è Information: Before creating our database, let\u0026rsquo;s prepare our EC2 instance with the necessary tools (Git and Node.js) to run our application and connect to the database.\nConnect to your EC2 instance via SSH (as done in the previous step).\nInstall Git: Update your system and install Git to clone the application repository.\nsudo dnf update -y sudo dnf install git -y git --version Install Node.js: We will use a script to install Node.js (LTS version) and necessary dependencies.\nCreate a script file:\nnano install_node.sh Paste the following content:\n#!/bin/bash # Install nvm (Node Version Manager) curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash . ~/.nvm/nvm.sh # Install Node.js LTS nvm install --lts nvm use --lts # Verify installation node -v npm -v # Install global development tools npm install -g nodemon echo \u0026#34;Node.js installation complete.\u0026#34; Save and exit (Ctrl+O, Enter, Ctrl+X).\nRun the script:\nbash install_node.sh Creating the RDS Database Instance ‚ÑπÔ∏è Information: Now we will create a MySQL database instance using Amazon RDS.\nStep-by-Step Guide Navigate to the Amazon RDS console.\nClick Create database.\nChoose a database creation method: Select Standard create.\nEngine options:\nEngine type: Select MySQL. Edition: Select MySQL Community. Version: Select the latest available version (or MySQL 8.0.x). Templates:\nSelect Free tier (if available and applicable) or Dev/Test for this workshop. üí° Pro Tip: Selecting Free tier automatically hides options that would incur costs, such as Multi-AZ and Provisioned IOPS.\nSettings:\nDB instance identifier: Enter a name (e.g., workshop-db). Master username: admin (or your preferred username). Master password: Enter a strong password and confirm it. ‚ö†Ô∏è Warning: Do not use Auto generate a password unless you store it immediately. It\u0026rsquo;s easier to set your own for this workshop.\nInstance configuration:\nDB instance class: Select Burstable classes (includes t classes) -\u0026gt; db.t3.micro (Free tier eligible). Storage:\nStorage type: General Purpose SSD (gp2 or gp3). Allocated storage: 20 GiB. Connectivity:\nCompute resource: Don\u0026rsquo;t connect to an EC2 compute resource. VPC: Select your workshop VPC. DB Subnet Group: Select the group you created in step 5.2.4. Public access: No (Best practice for security). VPC security group: Select Choose existing and pick the RDS Security Group created in step 5.2.3. Remove the default security group. üîí Security Note: Ensuring Public access is No and using the correct Security Group prevents unauthorized internet access to your database.\nAdditional configuration:\nInitial database name: Enter workshopdb (This allows RDS to create the schema for you automatically). Leave other settings as default. Click Create database.\nVerifying the Database Wait for the Status to change from Creating to Available.\nClick on the DB identifier (workshop-db) to view details.\nNote the Endpoint (e.g., workshop-db.xxxxxx.us-east-1.rds.amazonaws.com). You will need this to connect your application.\nMonitoring and Maintenance ‚ÑπÔ∏è Information: RDS provides built-in tools for monitoring and maintenance.\nLogs \u0026amp; Events: Check the Logs \u0026amp; events tab to see error logs, slow query logs, and administrative events. Maintenance \u0026amp; backups: Check this tab to see your backup window and any pending maintenance updates. üí° Pro Tip: Enable Enhanced Monitoring for granular, real-time metrics if you need to debug performance issues.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Introduction to Kiro.dev (AWS Agentic IDE)\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.5-deploy-app/",
	"title": "Application Deployment",
	"tags": [],
	"description": "",
	"content": "Deploying the Node.js Application ‚ÑπÔ∏è Information: Now that our infrastructure (EC2 and RDS) is ready, we will deploy our sample Node.js application.\nStep 1: Clone the Repository Connect to your EC2 instance via SSH (if not already connected).\nClone the workshop repository:\ngit clone https://github.com/AWS-First-Cloud-Journey/AWS-FCJ-Management cd AWS-FCJ-Management Step 2: Install Dependencies Initialize the project and install required packages:\nnpm install Step 3: Configure Database Connection Create a .env file to store your database credentials:\nnano .env Paste the following content, replacing the placeholders with your actual RDS details:\nDB_HOST=your-rds-endpoint.us-east-1.rds.amazonaws.com DB_USER=admin DB_PASSWORD=your-password DB_NAME=workshopdb DB_PORT=3306 DB_HOST: The Endpoint you copied from the RDS console. DB_USER: The Master username you set (e.g., admin). DB_PASSWORD: The Master password you set. DB_NAME: The database name (e.g., workshopdb). Save and exit (Ctrl+O, Enter, Ctrl+X).\nStep 4: Initialize the Database ‚ÑπÔ∏è Information: We need to create the necessary tables for our application. We will use a simple script or SQL commands.\nCreate a file named init_db.js (or use the provided SQL script if available in the repo). If you need to manually create the table, you can connect using a MySQL client or use a Node.js script like this:\nconst mysql = require(\u0026#39;mysql\u0026#39;); require(\u0026#39;dotenv\u0026#39;).config(); const connection = mysql.createConnection({ host: process.env.DB_HOST, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); connection.connect(); const createTableQuery = ` CREATE TABLE IF NOT EXISTS users ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, email VARCHAR(255) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; connection.query(createTableQuery, (error, results, fields) =\u0026gt; { if (error) throw error; console.log(\u0026#39;Table created successfully\u0026#39;); }); connection.end(); Note: The actual application might have its own migration script. Ensure you follow the repository\u0026rsquo;s specific instructions if they differ.\nStep 5: Start the Application Start the application using npm:\nnpm start You should see a message indicating the server is running (e.g., Server running on port 3000 or 5000).\nStep 6: Verify Deployment Open your web browser.\nNavigate to http://\u0026lt;EC2-Public-IP\u0026gt;:5000 (or the port your app uses).\nYou should see your application running and connected to the RDS database.\nüí° Pro Tip: If you cannot access the application, verify that your EC2 Security Group allows inbound traffic on the application port (e.g., 5000) from your IP address.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/",
	"title": "Workshop: Get Started with Amazon RDS",
	"tags": [],
	"description": "",
	"content": "Amazon Relational Database Service (Amazon RDS) Overview of Amazon RDS ‚ÑπÔ∏è Information: Amazon Relational Database Service (Amazon RDS) is a managed service that allows you to deploy and manage relational databases on AWS. Amazon RDS is designed for online transaction processing (OLTP) and is best suited for structured, relational data storage requirements.\nAmazon RDS provides key benefits:\nEasy replacement for traditional database instances Automated backups and patching during customer-defined maintenance windows One-click scaling, replication, and availability Supported Database Engines Amazon RDS supports the following database engines:\nAmazon Aurora MySQL MariaDB Oracle SQL Server PostgreSQL ‚ö†Ô∏è Warning: RDS is a managed service and you don\u0026rsquo;t have root access to the underlying EC2 server. The exception is Amazon RDS Custom, which allows access to the underlying operating system but is only available for a limited set of DB Engines.\nStorage Options General Purpose SSD (gp3/gp2): Cost-effective storage providing 3 IOPS/GB baseline performance Provisioned IOPS SSD (io1/io2): High-performance storage with customizable IOPS for I/O-intensive workloads Magnetic Storage: Legacy option with limited performance (not recommended for new deployments) üí° Pro Tip: Choose General Purpose SSD for most workloads, and Provisioned IOPS only when you need consistent I/O performance for database-intensive applications.\nHigh Availability and Disaster Recovery Multi-AZ Deployments: Synchronous standby replica in a different Availability Zone for automatic failover Read Replicas: Asynchronous replication for read scaling and potential disaster recovery Global Databases: Cross-region replication with fast local reads and disaster recovery capabilities üîí Security Note: Multi-AZ deployments enhance both availability and data durability, with automatic failover typically completing within 60-120 seconds.\nSecurity Features Encryption at rest: Using AWS KMS keys (applies to DB instances, backups, snapshots, and replicas) Network isolation: Using Amazon VPC for network-level isolation Resource-level permissions: Using IAM policies SSL/TLS encryption: For data in transit Database authentication: Using database engine native authentication or IAM authentication Scalability and Limits Storage can be scaled up (not down) without downtime Compute resources can be modified with a brief downtime during the change Maximum storage: 64 TiB for most engines (16 TiB for SQL Server) Maximum database connections: Varies by engine and instance size üí° Pro Tip: Plan your initial storage carefully as you can only scale up. Consider using Aurora for more flexible scaling options.\nBackup and Recovery Automated backups: Point-in-time recovery for up to 35 days Manual snapshots: User-initiated backups that persist until explicitly deleted Snapshot export to S3: For long-term retention or analysis When to Use Amazon RDS Amazon RDS is ideal for:\nTraditional relational database workloads Applications requiring SQL query capabilities Structured data with well-defined schemas OLTP workloads with predictable scaling needs ‚ÑπÔ∏è Information: For unstructured data, high-scale requirements, or specialized workloads, consider alternative AWS database services like DynamoDB, DocumentDB, or purpose-built databases.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/6-backup/",
	"title": "Backup and Restore",
	"tags": [],
	"description": "",
	"content": "Understanding Amazon RDS Backup and Restore ‚ÑπÔ∏è Information: Amazon RDS provides automated backups and allows manual snapshots to ensure your database data is protected and can be recovered when needed. These capabilities are essential for disaster recovery planning and maintaining business continuity.\nMonitoring Backup Status Access Monitoring:\nNavigate to the Databases section in the AWS Management Console. Select your target DB instance. Click on the Monitoring tab to view performance metrics. Managing Backups View Backup Details:\nSelect your DB instance. Navigate to the Maintenance \u0026amp; backups tab. Here you can view both automated and manual backup information and configure backup settings. View Snapshots:\nIn the left navigation pane, click Snapshots. You will see a list of all manual and automated snapshots. Restoring from a DB Snapshot ‚ÑπÔ∏è Information: Restoring a snapshot creates a new DB instance. It does not overwrite the existing one.\nSelect Snapshot:\nChoose the DB snapshot you want to restore. Click Actions \u0026gt; Restore snapshot. Configure New Instance:\nDB instance identifier: Enter a unique name for the new instance (e.g., workshop-db-restore). Instance specifications: Select the instance class (e.g., db.t3.micro). Connectivity: Select the same VPC and Subnet Group as your original instance. Security: Choose the correct Security Group. üí° Pro Tip: When restoring for testing, you can choose a smaller instance class to save costs.\nInitiate Restore:\nClick Restore DB instance. ‚ö†Ô∏è Warning: The restore process creates a completely new database instance with a new endpoint. You must update your application\u0026rsquo;s connection string to point to this new endpoint.\nVerify:\nWait for the status to change to Available. Test connectivity to the new instance. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at First Cloud Journey (FCJ) from June 17, 2024 to September 10, 2024, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the GameTracker Platform project, developing a serverless solution for the game community. Through this, I improved my skills in AWS Cloud services (Lambda, S3, RDS, WAF), React frontend development, Spring Boot backend, and System Architecture design.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚úÖ ‚òê ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Advanced Troubleshooting: Improve capability to debug complex distributed system issues independently. Communication: Enhance technical communication skills to explain complex architectural decisions more clearly to non-technical stakeholders. Optimization: Deepen understanding of cost optimization and performance tuning for serverless architectures. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/7-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "Resource Cleanup ‚ÑπÔ∏è Information: To avoid unexpected charges, it is crucial to clean up all resources created during this workshop. We will delete resources in the reverse order of their creation.\nStep 1: Delete RDS Resources Delete RDS Instance:\nGo to the Amazon RDS console \u0026gt; Databases. Select your database instance (e.g., workshop-db). Click Actions \u0026gt; Delete. Uncheck Create final snapshot and check I acknowledge\u0026hellip;. Type delete me and click Delete. Delete DB Subnet Group:\nGo to Subnet groups. Select your group (e.g., rds-subnet-group). Click Delete. Step 2: Terminate EC2 Instance Terminate Instance: Go to the Amazon EC2 console \u0026gt; Instances. Select your instance (e.g., Workshop-Web-Server). Click Instance state \u0026gt; Terminate instance. Click Terminate. Step 3: Delete Network Resources Delete Security Groups:\nGo to the VPC console \u0026gt; Security Groups. Select the RDS Security Group and delete it. Select the EC2 Security Group and delete it. üí° Pro Tip: You must delete the RDS Security Group first because the EC2 Security Group might reference it (or vice versa depending on your rules). If you get a dependency error, check your Inbound/Outbound rules.\nDelete VPC:\nGo to Your VPCs. Select your workshop VPC. Click Actions \u0026gt; Delete VPC. Type delete and confirm. ‚ÑπÔ∏è Information: Deleting the VPC will automatically delete associated subnets, route tables, and internet gateways.\nVerification Check your Billing Dashboard the next day to ensure no active resources remain. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/fcj-workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/fcj-workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]