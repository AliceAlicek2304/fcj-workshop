[{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.2-prerequiste/1-create-vpc/","title":"Create a VPC","tags":[],"description":"","content":"Creating a VPC with Subnets and Associated Resources **‚ÑπÔ∏è\\ Information**: Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you\u0026rsquo;ve defined. This virtual network closely resembles a traditional network in your own data center, with the benefits of using the scalable infrastructure of AWS.\nFollow these steps to create a VPC with all necessary components for your Amazon RDS deployment:\nOpen the Amazon VPC console at https://console.aws.amazon.com/vpc/. On the VPC dashboard, choose Create VPC.\nFor Resources to create, select VPC and more to create a complete VPC environment.\nConfigure the Name tag auto-generation option based on your preference. This allows AWS to automatically create consistent naming for all VPC resources.\nEnter an IPv4 CIDR block range for your VPC (e.g., 10.0.0.0/16). This defines the IP address range available within your VPC.\n(Optional) To enable IPv6 support, select IPv6 CIDR block and choose an Amazon-provided IPv6 range.\nSelect the appropriate Tenancy option:\nDefault: EC2 instances use the tenancy attribute specified during launch Dedicated: All EC2 instances run on hardware dedicated to your account √∞≈∏‚Äô¬° Pro Tip: Most workloads should use Default tenancy for cost efficiency. Only select Dedicated when you have specific compliance or licensing requirements.\nFor Number of Availability Zones (AZs), select at least two AZs for high availability.\nConfigure your Number of public subnets and Number of private subnets. For Amazon RDS deployments, you\u0026rsquo;ll typically need private subnets for your database instances and public subnets for resources that need internet access.\n√∞≈∏‚Äù‚Äô Security Note: Place your RDS instances in private subnets to enhance security by preventing direct internet access to your databases.\n(Optional) If resources in private subnets need internet access, configure NAT gateways in each AZ where you have resources requiring outbound connectivity.\n√∞≈∏‚Äô¬° Pro Tip: For production environments, deploy NAT gateways in each AZ to eliminate cross-AZ dependencies and improve fault tolerance.\n(Optional) For IPv6 outbound connectivity from private subnets, select Yes for Egress-only Internet Gateway.\n(Optional) To enable private access to Amazon S3, select VPC endpoints, S3 Gateway. This creates a gateway endpoint that allows resources in your VPC to access S3 without using the public internet.\nFor DNS options, the default settings enable both DNS resolution and DNS hostnames, which are recommended for most deployments.\n(Optional) Add tags to your VPC by expanding Additional tags and entering key-value pairs.\nReview the Preview pane to see a visual representation of the VPC architecture you\u0026rsquo;ve configured.\nChoose Create VPC to provision all the configured resources.\nConfiguring Public IP Address Assignment for Subnets **‚ÑπÔ∏è\\ Information**: The auto-assign public IPv4 address setting determines whether instances launched in a subnet automatically receive public IP addresses. For RDS deployments, your database subnets should typically have this setting disabled.\nTo modify the public IP address assignment behavior for a subnet:\nOpen the Amazon VPC console at https://console.aws.amazon.com/vpc/.\nIn the navigation pane, choose Subnets.\nSelect your subnet and choose Actions, then Edit subnet settings. Configure the Auto-assign public IPv4 address setting:\nChecked: Instances launched in this subnet automatically receive a public IPv4 address Unchecked: Instances launched in this subnet do not receive a public IPv4 address unless specifically requested during launch √∞≈∏‚Äù‚Äô Security Note: For subnets that will host your RDS instances, ensure this setting is unchecked to prevent accidental public IP assignment.\nChoose Save to apply your changes.\n**‚ö†Ô∏è\\ Warning**: Default subnets have auto-assign public IPv4 addresses enabled by default. Always verify this setting when using default subnets for database deployments.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.1-introduce/","title":"Introduction","tags":[],"description":"","content":"Amazon Relational Database Service (Amazon RDS) **‚ÑπÔ∏è\\ Information**: Amazon Relational Database Service (Amazon RDS) is a managed service that you can use to launch and manage relational databases on AWS.\nOnline Transaction Processing (OLTP) Amazon RDS is an Online Transaction Processing (OLTP) type of database.\nPrimary Use Case The primary use case is a transactional database (rather than an analytical database). It is best suited to structured, relational data store requirements.\nDrop-in Replacement It aims to be a drop-in replacement for existing on-premises instances of the same databases.\nFeatures Automated backups and patching are applied in customer-defined maintenance windows. Push-button scaling, replication, and redundancy. Supported Database Engines Amazon RDS supports the following database engines:\nAmazon Aurora MySQL MariaDB Oracle SQL Server PostgreSQL Managed Service **‚ÑπÔ∏è\\ Information**: RDS is a managed service, and you do not have access to the underlying EC2 instance (no root access).\nüí° Pro Tip: The exception to the above rule is Amazon RDS Custom, which allows access to the underlying operating system. This is available for Oracle and SQL Server database engines.\nManaged Service Includes The Amazon RDS managed service includes the following:\nSecurity and patching of the DB instances. Automated backup for the DB instances. Software updates for the DB engine. Easy scaling for storage and compute. Multi-AZ option with synchronous replication. Automatic failover for Multi-AZ option. Read replicas option for read-heavy workloads. DB Instance A DB instance is a database environment in the cloud with the compute and storage resources you specify.\nAccess via Endpoints Database instances are accessed via endpoints. Endpoints can be retrieved via the DB instance description in the AWS Management Console, DescribeDBInstances API, or describe-db-instances command.\nInstance Limits **‚ö†Ô∏è\\ Warning**: By default, customers are allowed to have up to a total of 40 Amazon RDS DB instances (only 10 of these can be Oracle or MS SQL unless you have your own licenses).\nMaintenance Windows Maintenance windows are configured to allow DB instances modifications to take place, such as scaling and software patching (some operations require the DB instance to be taken offline briefly). You can define the maintenance window, or AWS will schedule a 30-minute window.\nWindows Integrated Authentication Windows integrated authentication for SQL only works with domains created using the AWS directory service ‚Äì need to establish a trust with an on-premises AD directory.\nEvents and Notifications Amazon RDS uses AWS SNS to send RDS events via SNS notifications. You can use API calls to the Amazon RDS service to list the RDS events in the last 14 days (DescribeEvents API). You can view events from the last 14 days using the CLI. Using the AWS Console, you can only view RDS events for the last 1 day.\nUse Cases, Alternatives, and Anti-Patterns The table below provides guidance on when best to use RDS and several other AWS database/data store services:\nData Store When to Use Database on EC2 - Ultimate control over the database - Preferred DB not available under RDS Amazon RDS - Need traditional relational database for OLTP - Your data is well-formed and structured - Existing apps requiring RDBMS Amazon DynamoDB - Name/value pair data or unpredictable data structure - In-memory performance with persistence - High I/O needs - Scale dynamically Amazon RedShift - Massive amounts of data - Primarily OLAP workloads Amazon Neptune - Relationships between objects a major portion of data value Amazon Elasticache - Fast temporary storage for small amounts of data - Highly volatile data Amazon S3 - BLOBs - Static Websites Alternative to Amazon RDS:\nIf your use case isn\u0026rsquo;t supported on RDS, you can run databases on Amazon EC2.\nConsider the following points when considering a DB on EC2:\nYou can run any database you like with full control and ultimate flexibility. You must manage everything like backups, redundancy, patching, and scaling. Good option if you require a database not yet supported by RDS, such as IBM DB2 or SAP HANA. Good option if it is not feasible to migrate to an AWS-managed database. Anti-Patterns:\nAnti-patterns are certain patterns in architecture or development that are considered bad or sub-optimal practices ‚Äì i.e. there may be a better service or method to produce the best result.\nThe following table describes requirements that are not a good fit for RDS:\nRequirement More Suitable Service Lots of large binary objects (BLOBs) S3 Automated Scalability DynamoDB Name/Value Data Structure DynamoDB Data is not well structured or unpredictable DynamoDB Other database platforms like IBM DB2 or SAP HANA EC2 Complete control over the database EC2 Encryption üîí Security Note: You can encrypt your Amazon RDS instances and snapshots at rest by enabling the encryption option for your Amazon RDS DB instance.\nEncryption at rest is supported for all DB types and uses AWS KMS.\nWhen using encryption at rest, the following elements are also encrypted:\nAll DB snapshots. Backups. DB instance storage. Read Replicas. **‚ö†Ô∏è\\ Warning**: You cannot encrypt an existing DB; you need to create a snapshot, copy it, encrypt the copy, then build an encrypted DB from the snapshot.\nData that is encrypted at rest includes the underlying storage for a DB instance, its automated backups, Read Replicas, and snapshots.\nA Read Replica of an Amazon RDS encrypted instance is also encrypted using the same key as the master instance when both are in the same region.\nIf the master and Read Replica are in different regions, you encrypt using the encryption key for that region.\n**‚ö†Ô∏è\\ Warning**: You can\u0026rsquo;t have an encrypted Read Replica of an unencrypted DB instance or an unencrypted Read Replica of an encrypted DB instance.\nEncryption/decryption is handled transparently.\nRDS supports SSL encryption between applications and RDS DB instances.\nRDS generates a certificate for the instance.\nDB Subnet Groups A DB subnet group is a collection of subnets (typically private) that you create in a VPC and that you then designate for your DB instances.\nüí° Pro Tip: Each DB subnet group should have subnets in at least two Availability Zones in a given region.\nIt is recommended to configure a subnet group with subnets in each AZ (even for standalone instances).\nDuring the creation of an RDS instance, you can select the DB subnet group and the AZ within the group to place the RDS DB instance in.\nYou cannot pick the IP within the subnet that is allocated.\nBilling and Provisioning AWS Charge for:\nDB instance hours (partial hours are charged as full hours). Storage GB/month. I/O requests/month ‚Äì for magnetic storage. Provisioned IOPS/month ‚Äì for RDS provisioned IOPS SSD. Egress data transfer. Backup storage (DB backups and manual snapshots). Backup storage for the automated RDS backup is free of charge up to the provisioned EBS volume size. However, AWS replicates data across multiple AZs, so you are charged for the extra storage space on S3.\nFor multi-AZ, you are charged for:\nMulti-AZ DB hours. Provisioned storage. Double write I/Os. For multi-AZ, you are not charged for DB data transfer during replication from primary to standby.\nOracle and Microsoft SQL licenses are included, or you can bring your own (BYO).\nOn-demand and reserved instance pricing available.\nReserved instances are defined based on the following attributes which must not be changed:\nDB engine. DB instance class. Deployment type (standalone, multi-AZ). License model. Region. Reserved instances:\nCan be moved between AZs in the same region. Are available for multi-AZ deployments. Can be applied to Read Replicas if DB instance class and region are the same. Scaling is achieved through changing the instance class for compute and modifying storage capacity for additional storage allocation.\nScalability You can only scale RDS up (compute and storage).\nYou cannot decrease the allocated storage for an RDS instance.\nYou can scale storage and change the storage type for all DB engines except MS SQL.\nFor MS SQL, the workaround is to create a new instance from a snapshot with the new configuration.\nScaling storage can happen while the RDS instance is running without outage; however, there may be performance degradation.\nScaling compute will cause downtime.\nYou can choose to have changes take effect immediately, however, the default is within the maintenance window.\nScaling requests are applied during the specified maintenance window unless ‚Äúapply immediately‚Äù is used.\nAll RDS DB types support a maximum DB size of 64 TiB except for Microsoft SQL Server (16 TiB).\nPerformance Amazon RDS uses EBS volumes (never uses instance store) for DB and log storage.\nThere are three storage types available: General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic.\nGeneral Purpose (SSD):\nUse for Database workloads with moderate I/O requirement. Cost-effective. Also called gp2. 3 IOPS/GB. Burst up to 3000 IOPS. Provisioned IOPS (SSD):\nUse for I/O intensive workloads. Low latency and consistent I/O. User-specified IOPS (see table below). For provisioned IOPS storage, the table below shows the range of Provisioned IOPS and storage size range for each database engine.\nDatabase Engine Range of Provisioned IOPS Range of Storage MariaDB 1,000-80,000 IOPS 100 GiB-64TiB SQL Server 1,000-64,000 IOPS 20 GiB-16TiB MySQL 1,000-80,000 IOPS 100 GiB-64TiB Oracle 1,000-256,000 IOPS 100 GiB-64TiB PostgreSQL 1,000-80,000 IOPS 100 GiB-64TiB Magnetic:\nNot recommended anymore, available for backward compatibility. Doesn‚Äôt allow you to scale storage when using the SQL Server database engine. Doesn‚Äôt support elastic volumes. Limited to a maximum size of 4 TiB. Limited to a maximum of 1,000 IOPS. Multi-AZ and Read Replicas Multi-AZ and Read Replicas are used for high availability, fault tolerance, and performance scaling.\nThe table below compares multi-AZ deployments to Read Replicas:\nMulti-AZ Deployments Read Replicas Synchronous Replication ‚Äì highly durable Asynchronous replication ‚Äì highly scalable Only database engine on primary instance is active All read replicas are accessible and can be used for read scaling Automated backups are taken from standby No backups configured by default Always span two availability zones within a single region Can be within an Availability Zone, Cross-AZ, or Cross-Region Database engine version upgrades happen on primary Database engine version upgrade is independent from the source instance Automatic failover to standby when a problem is detected Can be manually promoted to a standalone database instance Multi-AZ Multi-AZ RDS creates a replica in another AZ and synchronously replicates to it (DR only).\nThere is an option to choose multi-AZ during the launch wizard.\nAWS recommends the use of provisioned IOPS storage for multi-AZ RDS DB instances.\nEach AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.\nYou cannot choose which AZ in the region will be chosen to create the standby DB instance.\nYou can view which AZ the standby DB instance is created in.\nA failover may be triggered in the following circumstances:\nLoss of primary AZ or primary DB instance failure. Loss of network connectivity on primary. Compute (EC2) unit failure on primary. Storage (EBS) unit failure on primary. The primary DB instance is changed. Patching of the OS on the primary DB instance. Manual failover (reboot with failover selected on primary). During failover, RDS automatically updates configuration (including DNS endpoint) to use the second node.\nDepending on the instance class, it can take 1 to a few minutes to failover to a standby DB instance.\nIt is recommended to implement DB connection retries in your application.\nRecommended to use the endpoint rather than the IP address to point applications to the RDS DB.\nThe method to initiate a manual RDS DB instance failover is to reboot selecting the option to failover.\nA DB instance reboot is required for changes to take effect when you change the DB parameter group or when you change a static DB parameter.\nThe DB parameter group is a configuration container for the DB engine configuration.\nYou will be alerted by a DB instance event when a failover occurs.\nThe secondary DB in a multi-AZ configuration cannot be used as an independent read node (read or write).\nThere is no charge for data transfer between primary and secondary RDS instances.\nSystem upgrades like OS patching, DB Instance scaling, and system upgrades are applied first on the standby before failing over and modifying the other DB Instance.\nIn multi-AZ configurations, snapshots and automated backups are performed on the standby to avoid I/O suspension on the primary instance.\nRead Replica Support for Multi-AZ: Amazon RDS Read Replicas for MySQL, MariaDB, PostgreSQL, and Oracle support Multi-AZ deployments.\nCombining Read Replicas with Multi-AZ enables you to build a resilient disaster recovery strategy and simplify your database engine upgrade process.\nA Read Replica in a different region than the source database can be used as a standby database and promoted to become the new production database in case of a regional disruption.\nThis allows you to scale reads whilst also having multi-AZ for DR.\nThe process for implementing maintenance activities is as follows:\nPerform operations on standby. Promote standby to primary. Perform operations on new standby (demoted primary). You can manually upgrade a DB instance to a supported DB engine version from the AWS Console.\nBy default, upgrades will take effect during the next maintenance window.\nYou can optionally force an immediate upgrade.\nIn multi-AZ deployments, version upgrades will be conducted on both the primary and standby at the same time, causing an outage of both DB instances.\nEnsure security groups and NACLs will allow your application servers to communicate with both the primary and standby instances.\nRead Replicas Read replicas are used for read-heavy DBs, and replication is asynchronous.\nRead replicas are for workload sharing and offloading.\nRead replicas provide read-only DR.\nRead replicas are created from a snapshot of the master instance.\nMust have automated backups enabled on the primary (retention period \u0026gt; 0).\nOnly supported for transactional database storage engines (InnoDB not InnoDB).\nRead replicas are available for MySQL, PostgreSQL, MariaDB, Oracle, Aurora, and SQL Server.\nFor the MySQL, MariaDB, PostgreSQL, and Oracle database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines‚Äô native asynchronous replication to update the read replica whenever there is a change to the source DB instance.\nAmazon Aurora employs an SSD-backed virtualized storage layer purpose-built for database workloads.\nYou can take snapshots of PostgreSQL read replicas but cannot enable automated backups.\nYou can enable automatic backups on MySQL and MariaDB read replicas.\nYou can enable writes to the MySQL and MariaDB Read Replicas.\nYou can have 5 read replicas of a production DB.\nYou cannot have more than four instances involved in a replication chain.\nYou can have read replicas of read replicas for MySQL and MariaDB but not for PostgreSQL.\nRead replicas can be configured from the AWS Console or the API.\nYou can specify the AZ the read replica is deployed in.\nThe read replica\u0026rsquo;s storage type and instance class can be different from the source but the compute should be at least the performance of the source.\nYou cannot change the DB engine.\nIn a multi-AZ failover, the read replicas are switched to the new primary.\nRead replicas must be explicitly deleted.\nIf a source DB instance is deleted without deleting the replicas, each replica becomes a standalone single-AZ DB instance.\nYou can promote a read replica to primary.\nPromotion of read replicas takes several minutes.\nPromoted read replicas retain:\nBackup retention window. Backup window. DB parameter group. Existing read replicas continue to function as normal.\nEach read replica has its own DNS endpoint.\nRead replicas can have multi-AZ enabled, and you can create read replicas of multi-AZ source DBs.\nRead replicas can be in another region (uses asynchronous replication).\nThis configuration can be used for centralizing data from across different regions for analytics.\nDB Snapshots DB Snapshots are user-initiated and enable you to back up your DB instance in a known state as frequently as you wish, and then restore to that specific state.\nCannot be used for point-in-time recovery. Snapshots are stored on S3. Snapshots remain on S3 until manually deleted. Backups are taken within a defined window. I/O is briefly suspended while backups initialize and may increase latency (applicable to single-AZ RDS). DB snapshots that are performed manually will be stored even after the RDS instance is deleted. Restored DBs will always be a new RDS instance with a new DNS endpoint. Can restore up to the last 5 minutes. Only default DB parameters and security groups are restored ‚Äì you must manually associate all other DB parameters and SGs. It is recommended to take a final snapshot before deleting an RDS instance. Snapshots can be shared with other AWS accounts. High Availability Approaches for Databases If possible, choose DynamoDB over RDS because of inherent fault tolerance.\nIf DynamoDB can‚Äôt be used, choose Aurora because of redundancy and automatic recovery features.\nIf Aurora can‚Äôt be used, choose Multi-AZ RDS.\nFrequent RDS snapshots can protect against data corruption or failure, and they won‚Äôt impact the performance of Multi-AZ deployment.\nRegional replication is also an option but will not be strongly consistent.\nIf the database runs on EC2, you must design the HA yourself.\nMigration AWS Database Migration Service helps you migrate databases to AWS quickly and securely.\nUse along with the Schema Conversion Tool (SCT) to migrate databases to AWS RDS or EC2-based databases.\nThe source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.\nThe AWS Database Migration Service can migrate your data to and from most widely used commercial and open-source databases.\nSchema Conversion Tool can copy database schemas for homogenous migrations (same database) and convert schemas for heterogeneous migrations (different database).\nDMS is used for smaller, simpler conversions and supports MongoDB and DynamoDB.\nSCT is used for larger, more complex datasets like data warehouses.\nDMS has replication functions for on-premises to AWS or to Snowball or S3.\nMonitoring, Logging, and Reporting You can use the following automated monitoring tools to watch Amazon RDS and report when something is wrong:\nAmazon RDS Events ‚Äì Subscribe to Amazon RDS events to be notified when changes occur with a DB instance, DB snapshot, DB parameter group, or DB security group. Database log files ‚Äì View, download, or watch database log files using the Amazon RDS console or Amazon RDS API operations. You can also query some database log files that are loaded into database tables. Amazon RDS Enhanced Monitoring ‚Äî Look at metrics in real time for the operating system. Amazon RDS Performance Insights ‚Äî Assess the load on your database and determine when and where to act. Amazon RDS Recommendations ‚Äî Look at automated recommendations for database resources, such as DB instances, read replicas, and DB parameter groups. In addition, Amazon RDS integrates with Amazon CloudWatch, Amazon EventBridge, and AWS CloudTrail for additional monitoring capabilities:\nAmazon CloudWatch Metrics ‚Äì Amazon RDS automatically sends metrics to CloudWatch every minute for each active database. You don‚Äôt get additional charges for Amazon RDS metrics in CloudWatch. Amazon CloudWatch Alarms ‚Äì You can watch a single Amazon RDS metric over a specific time period. You can then perform one or more actions based on the value of the metric relative to a threshold that you set. Amazon CloudWatch Logs ‚Äì Most DB engines enable you to monitor, store, and access your database log files in CloudWatch Logs. Amazon CloudWatch Events and Amazon EventBridge ‚Äì You can automate AWS services and respond to system events such as application availability issues or resource changes. Events from AWS services are delivered to CloudWatch Events and EventBridge nearly in real time. You can write simple rules to indicate which events interest you and what automated actions to take when an event matches a rule. AWS CloudTrail ‚Äì You can view a record of actions taken by a user, role, or an AWS service in Amazon RDS. CloudTrail captures all API calls for Amazon RDS as events. These captures include calls from the Amazon RDS console and from code calls to the Amazon RDS API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon RDS. If you don‚Äôt configure a trail, you can still view the most recent events in the CloudTrail console in Event history. "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguy·ªÖn VƒÉn C∆∞·ªùng\nPhone Number: 0349079940\nEmail: cuongnvse183645@fpt.edu.vn\nUniversity: FPT university HCM\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: No. Task Start Date Completion Date Reference Material 1 Learn about AWS Free Tier, create AWS account 08/09/2025 08/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Billing Dashboard, create Budget and cost alerts 09/09/2025 09/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Study IAM: User, Group, Role, Policy 10/09/2025 10/09/2025 AWS Docs, AWS Training, FCJ Playlist 4 Configure MFA for root and user accounts 11/09/2025 11/09/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create 2 IAM users (Dev \u0026amp; Admin) with different permissions 12/09/2025 12/09/2025 AWS Docs, AWS Training, FCJ Playlist 6 Launch EC2 instance (Linux) 15/09/2025 15/09/2025 AWS Docs, AWS Training, FCJ Playlist Week 1 Achievements: Learned about AWS Free Tier and successfully created an AWS account.\nConfigured Billing Dashboard, set up Budget and cost alerts to monitor expenses.\nStudied IAM concepts: User, Group, Role, Policy.\nConfigured MFA for both root and user accounts to enhance security.\nCompleted a mini project: Created 2 IAM users (Dev \u0026amp; Admin) with different permissions.\nLaunched an EC2 instance (Linux) and practiced basic management tasks.\nBecame familiar with the AWS Management Console and AWS CLI for managing resources.\nDeveloped the ability to connect and manage AWS resources using both the web interface and CLI in parallel.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand AWS service groups (Compute, Storage, Networking, Database). Learn to use AWS Console and AWS CLI for managing resources. Practice EC2 and EBS setup and SSH connection. Tasks to be carried out this week: No. Task Start Date Completion Date Reference Material 1 Get acquainted with FCJ members; read and note internship regulations. 11/08/2025 11/08/2025 AWS Docs, AWS Training, FCJ Playlist 2 Learn about AWS and its types of services (Compute, Storage, Networking, Database, etc.) 12/08/2025 12/08/2025 AWS Docs, AWS Training, FCJ Playlist 3 Create AWS Free Tier account; learn AWS Console and CLI setup. 13/08/2025 13/08/2025 AWS Docs, AWS Training, FCJ Playlist 4 Study basic EC2: instance types, AMI, EBS, Elastic IP, and SSH connection methods. 14/08/2025 15/08/2025 AWS Docs, AWS Training, FCJ Playlist 5 Practice launching EC2 instance, connecting via SSH, and attaching EBS volume. 15/08/2025 15/08/2025 AWS Docs, AWS Training, FCJ Playlist Week 2 Achievements: Understood what AWS is and mastered core service groups (Compute, Storage, Networking, Database, \u0026hellip;). Created and configured AWS Free Tier account successfully. Learned how to navigate AWS Management Console and locate services efficiently. Installed and configured AWS CLI with Access Key, Secret Key, and Default Region. Practiced using AWS CLI for: Checking account \u0026amp; configuration Listing regions Viewing EC2 services Creating and managing key pairs Viewing active resources Gained the ability to manage AWS resources in parallel via Console and CLI. "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Continue building foundational AWS knowledge. Practice with IAM, EC2, and basic networking setup. Begin deploying a simple website on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Explore AWS Free Tier and create an AWS account 08/09/2025 08/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Billing Dashboard, create Budget \u0026amp; Cost Alerts 09/09/2025 09/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Learn IAM: User, Group, Role, Policy 10/09/2025 10/09/2025 AWS Docs, AWS Training, FCJ Playlist 4 Enable MFA for Root and User accounts 11/09/2025 11/09/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create 2 IAM Users (Dev \u0026amp; Admin) with different permissions 12/09/2025 12/09/2025 AWS Docs, AWS Training, FCJ Playlist 6 Create an EC2 instance (Linux) 15/09/2025 15/09/2025 AWS Docs, AWS Training, FCJ Playlist 7 SSH into EC2 and install Apache/Nginx 16/09/2025 16/09/2025 AWS Docs, AWS Training, FCJ Playlist 8 Learn Security Group, assign Elastic IP 17/09/2025 17/09/2025 AWS Docs, AWS Training, FCJ Playlist 9 Deploy a small application on EC2 18/09/2025 18/09/2025 AWS Docs, AWS Training, FCJ Playlist 10 Mini Project: Deploy a static website on EC2 19/09/2025 19/09/2025 AWS Docs, AWS Training, FCJ Playlist Week 3 Achievements: Understood IAM structure and how to manage users, groups, and roles. Configured MFA and budget alerts for account security and cost control. Created, launched, and connected to EC2 instances. Learned to manage Elastic IPs and security groups. Successfully deployed a static website on EC2. Practiced hands-on with AWS Console and CLI. "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand AWS networking concepts: VPC, Subnet, Internet Gateway, Route Table. Learn the difference between Security Groups and NACLs. Deploy basic Load Balancer (ALB) and EC2 instances inside a custom VPC. Complete a mini project combining VPC, EC2, and ALB. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Launch a custom VPC and create Subnets 29/09/2025 29/09/2025 AWS Docs, AWS Training, FCJ Playlist 2 Configure Internet Gateway and Route Table for the VPC 30/09/2025 30/09/2025 AWS Docs, AWS Training, FCJ Playlist 3 Learn the difference between Security Groups and NACLs 01/10/2025 01/10/2025 AWS Docs, AWS Training, FCJ Playlist 4 Deploy a basic Application Load Balancer (ALB) 02/10/2025 02/10/2025 AWS Docs, AWS Training, FCJ Playlist 5 Mini Project: Create a private VPC and deploy 2 EC2 instances behind the ALB 03/10/2025 03/10/2025 AWS Docs, AWS Training, FCJ Playlist Week 4 Achievements: Successfully created a custom VPC and configured Subnets.\nConfigured Internet Gateway and Route Tables to allow proper routing in the VPC.\nLearned the differences between Security Groups (stateful) and NACLs (stateless) and applied them.\nDeployed a basic Application Load Balancer (ALB) to distribute traffic to multiple EC2 instances.\nCompleted mini project: created a private VPC and deployed 2 EC2 instances behind the ALB, gaining hands-on experience in networking and load balancing.\nBecame more confident in managing AWS networking resources and deploying a simple high-availability architecture.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn advanced AWS networking concepts and practice with Security Groups. Understand Elastic Load Balancer (ALB) and Auto Scaling basics. Deploy a mini project combining EC2, VPC, ALB, and Security Groups. Practice monitoring and basic troubleshooting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review VPC, Subnet, and Route Table configuration 06/10/2025 06/10/2025 AWS Docs, FCJ Playlist 2 Learn and configure Security Groups for EC2 instances 07/10/2025 07/10/2025 AWS Docs, FCJ Playlist 3 Deploy Application Load Balancer (ALB) and connect EC2 instances 08/10/2025 08/10/2025 AWS Docs, FCJ Playlist 4 Understand and practice Auto Scaling Groups 09/10/2025 09/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy 2 EC2 instances behind an ALB with proper Security Groups and basic Auto Scaling 10/10/2025 10/10/2025 AWS Docs, FCJ Playlist Week 5 Achievements: Reviewed VPC, Subnet, and Route Table setups and ensured proper connectivity.\nConfigured Security Groups to allow necessary traffic while keeping instances secure.\nSuccessfully deployed Application Load Balancer (ALB) and connected EC2 instances.\nLearned the basics of Auto Scaling Groups and applied them to EC2 instances.\nCompleted mini project: 2 EC2 instances deployed behind ALB with Security Groups, gaining hands-on experience with networking, load balancing, and scaling.\nImproved skills in monitoring and troubleshooting basic AWS networking issues.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn VPC, Subnet, and Route Table concepts for EC2 networking. Practice configuring Security Groups and Elastic IPs. Deploy EC2 instances in a VPC with proper networking and access. Complete a mini project integrating EC2, Security Groups, and Elastic IP. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review VPC, Subnet, and Route Table concepts 13/10/2025 13/10/2025 AWS Docs, FCJ Playlist 2 Configure Security Groups and test inbound/outbound rules 14/10/2025 14/10/2025 AWS Docs, FCJ Playlist 3 Assign Elastic IPs to EC2 instances and test connectivity 15/10/2025 15/10/2025 AWS Docs, FCJ Playlist 4 Launch EC2 instances within the VPC, assign Subnets, Security Groups, and Elastic IP 16/10/2025 16/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy a basic web application on EC2 instances with proper networking settings 17/10/2025 17/10/2025 AWS Docs, FCJ Playlist Week 6 Achievements: Reviewed and understood VPC, Subnet, and Route Table configurations.\nConfigured Security Groups correctly to allow necessary traffic and secure EC2 instances.\nAssigned Elastic IPs to EC2 instances and verified public accessibility.\nLaunched EC2 instances inside the VPC with correct networking, Security Groups, and Elastic IP setup.\nCompleted mini project: deployed a basic web application on EC2 with proper networking and security.\nStrengthened hands-on skills with EC2 networking, access control, and public IP management.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn AWS S3 concepts: buckets, objects, and storage classes. Practice creating S3 buckets, uploading files, and setting permissions. Understand S3 bucket policies and public/private access. Complete a mini project: deploy static files to S3 and test access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review S3 concepts: bucket, object, storage classes 20/10/2025 20/10/2025 AWS Docs, FCJ Playlist 2 Create S3 buckets and practice uploading files 21/10/2025 21/10/2025 AWS Docs, FCJ Playlist 3 Configure bucket permissions and test public/private access 22/10/2025 22/10/2025 AWS Docs, FCJ Playlist 4 Apply S3 bucket policies for specific access control 23/10/2025 23/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy static website files to S3 and test access 24/10/2025 24/10/2025 AWS Docs, FCJ Playlist Week 7 Achievements: Learned S3 storage concepts and usage.\nSuccessfully created S3 buckets, uploaded files, and organized objects.\nApplied bucket permissions and policies for secure and controlled access.\nCompleted mini project: deployed static files to S3 and verified accessibility.\nEnhanced hands-on skills with AWS S3, file management, and access control.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn advanced AWS S3 features: versioning, lifecycle, and bucket policies. Practice setting up S3 bucket policies and permissions. Understand S3 event notifications and integration with Lambda. Introduction to CloudFront for content delivery. Complete a mini project: host a static website on S3 + CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review basic S3 and upload files 27/10/2025 27/10/2025 AWS Docs, FCJ Playlist 2 Enable versioning on S3 buckets and test file versioning 28/10/2025 28/10/2025 AWS Docs, FCJ Playlist 3 Configure bucket policies and permissions 29/10/2025 29/10/2025 AWS Docs, FCJ Playlist 4 Learn about S3 event notifications and trigger Lambda functions 30/10/2025 30/10/2025 AWS Docs, FCJ Playlist 5 Mini Project: Deploy a static website on S3 and distribute via CloudFront 31/10/2025 31/10/2025 AWS Docs, FCJ Playlist Week 8 Achievements: Understood advanced S3 features including versioning, lifecycle rules, and policies.\nSuccessfully configured bucket policies for secure and controlled access.\nPracticed triggering Lambda functions via S3 event notifications.\nDeployed a static website on S3 and distributed it via CloudFront for faster content delivery.\nImproved hands-on skills with AWS S3, security policies, version control, and content delivery.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn about AWS RDS (Relational Database Service) and database management. Understand different database engines: MySQL, PostgreSQL, SQL Server. Practice creating RDS instances and connecting from local/EC2. Learn backup, snapshot, and restore methods in RDS. Understand security best practices for database access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review basic database concepts and AWS RDS documentation 03/11/2025 03/11/2025 AWS Docs, FCJ Playlist 2 Create RDS instances (MySQL \u0026amp; PostgreSQL) 04/11/2025 04/11/2025 AWS Docs, FCJ Playlist 3 Connect RDS instances from local machine and EC2 05/11/2025 05/11/2025 AWS Docs, FCJ Playlist 4 Configure automatic backups and manual snapshots 06/11/2025 06/11/2025 AWS Docs, FCJ Playlist 5 Test restore from snapshot and review database security \u0026amp; parameter groups 07/11/2025 07/11/2025 AWS Docs, FCJ Playlist Week 9 Achievements: Understood AWS RDS and its advantages for managed databases.\nSuccessfully created MySQL and PostgreSQL RDS instances.\nConnected to RDS instances from local machines and EC2 instances.\nConfigured automatic backups, created snapshots, and tested restore procedures.\nLearned best practices for database security including IAM roles, security groups, and password management.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Learn about AWS Free Tier, Billing Dashboard, IAM, MFA, EC2 and complete a mini IAM project\nWeek 2: Get familiar with AWS CLI, IAM roles, and basic S3 operations\nWeek 3: Learn EC2 in detail: Instance types, AMI, EBS, Security Groups, and SSH connection\nWeek 4: Practice launching EC2 instances, attaching EBS, and connecting via SSH\nWeek 5: Understand AWS networking: VPC, Subnets, Internet Gateway, and Elastic IPs\nWeek 6: Practice configuring VPC, subnet, security group rules, and EC2 networking setup\nWeek 7: Explore RDS, DynamoDB, and basic database configuration on AWS\nWeek 8: Learn S3 advanced features: versioning, lifecycle rules, bucket policies, and static website hosting\nWeek 9: Connect with FCJ members, review AWS basics, create AWS account, practice EC2 launch and EBS attachment\nWeek 10: Consolidate AWS skills: EC2, EBS, CLI commands, and Management Console usage\nWeek 11: Practice advanced AWS CLI commands, automate EC2 creation, and integrate basic services\nWeek 12: Learn CloudFormation, deploy EC2 + S3 stacks, integrate Lambda \u0026amp; API Gateway, test automation, and finalize internship report\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.2-prerequiste/2-create-ec2-sg/","title":"Create EC2 Security Group","tags":[],"description":"","content":"Creating a Security Group for EC2 Instances **‚ÑπÔ∏è\\ Information**: Security groups act as virtual firewalls for your Amazon EC2 instances to control inbound and outbound traffic. For our RDS deployment, we need to create a security group for EC2 instances that will connect to our database.\nFollow these steps to create a security group with the necessary ports:\nNavigate to the AWS Management Console and sign in to your account.\nIn the AWS Management Console, search for and select EC2 under services.\nIn the EC2 navigation pane, under Network \u0026amp; Security, select Security Groups.\nClick the Create security group button.\nIn the Basic details section: Enter a descriptive Security group name (e.g., \u0026ldquo;EC2-Web-App-SG\u0026rdquo;) Provide a meaningful Description (e.g., \u0026ldquo;Security group for EC2 instances connecting to RDS\u0026rdquo;) Select your VPC from the dropdown menu In the Inbound rules section, click Add rule to configure the following access:\nHTTP (80): Select \u0026ldquo;HTTP\u0026rdquo; from the Type dropdown (automatically sets port 80) HTTPS (443): Select \u0026ldquo;HTTPS\u0026rdquo; from the Type dropdown (automatically sets port 443) Custom TCP (5000): Select \u0026ldquo;Custom TCP\u0026rdquo; and enter \u0026ldquo;5000\u0026rdquo; in the Port range field SSH (22): Select \u0026ldquo;SSH\u0026rdquo; from the Type dropdown (automatically sets port 22) √∞≈∏‚Äù‚Äô Security Note: For production environments, restrict the source IP addresses for SSH access to only trusted IP ranges rather than allowing access from anywhere (0.0.0.0/0).\nReview your settings and click Create security group. Once created, the new security group appears in your security groups list. Note the Security Group ID as you\u0026rsquo;ll need it when launching EC2 instances. √∞≈∏‚Äô¬° Pro Tip: You can modify security group rules at any time, and the changes take effect immediately. This allows you to adjust access controls as your application requirements evolve.\n**‚ö†Ô∏è\\ Warning**: Security groups are stateful √¢‚Ç¨‚Äù if you allow inbound traffic on a specific port, the corresponding outbound response traffic is automatically allowed, regardless of outbound rules.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.2-prerequiste/","title":"Prerequisite Steps","tags":[],"description":"","content":"Overview of Prerequisites **‚ÑπÔ∏è\\ Information**: Before deploying Amazon RDS, we need to set up the necessary network infrastructure and security components to ensure proper connectivity and security for our database environment.\nRequired Network Components Create a Virtual Private Cloud (VPC) Create Subnets across multiple Availability Zones Create Security Group for Amazon EC2 instances Create Security Group for Amazon RDS DB instances Create DB Subnet Group for Amazon RDS √∞≈∏‚Äô¬° Pro Tip: When creating subnets for your RDS deployment, distribute them across at least two Availability Zones to enable Multi-AZ deployments for high availability.\n√∞≈∏‚Äù‚Äô Security Note: Security Groups act as virtual firewalls that control inbound and outbound traffic. For RDS, configure your security group to only allow traffic on the database port from authorized sources.\n**‚ö†Ô∏è\\ Warning**: DB Subnet Groups must include subnets in at least two different Availability Zones to support Multi-AZ deployments. Without this configuration, you won\u0026rsquo;t be able to enable the Multi-AZ feature for your RDS instances.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"GameTracker Platform for Game Community A Unified AWS Serverless Solution for Game Data Management \u0026amp; Sharing 1. Executive Summary GameTracker is a platform designed for game players and admins to manage, track, and share information about characters, weapons, banners, items, and events. The system supports multi-method login (email/password, Google OAuth2), account management, admin dashboard, and file storage on AWS S3.\nFrontend is a React SPA served via S3 + CloudFront, protected by AWS WAF to ensure security and performance. Backend is Spring Boot serverless deployed on AWS Lambda connected to SQL Server (RDS).\nGameTracker provides visual tools:\nGacha history tracker for accounts, easy to view and analyze Gacha simulator to predict pulls Banner/event timeline showing event info and duration These tools help players manage game data effectively, especially for English-language games, which are difficult for some users to understand.\n2. Problem Statement Current Problem\nPlayers face difficulty managing and accessing game data, especially in complex gacha games. Language barriers in English games make information less accessible.\nSolution\nGameTracker offers a centralized, user-friendly system where users can:\nManage and update game data Track gacha history and simulate pulls View banner and event timelines Admins can efficiently control content with CRUD operations and clear access permissions.\nBenefits and ROI\nOptimized data management for users and admins Reduced manual maintenance time, improved data reliability Low operational costs via AWS serverless \u0026amp; S3 Scalable to multiple games and community features 3. Solution Architecture Cloud-based modern architecture:\nFrontend: React SPA, served via S3 + CloudFront, SPA fallback, protected by AWS WAF Backend: Spring Boot serverless on AWS Lambda, JWT auth, Google OAuth2, API protected by WAF Database: SQL Server (AWS RDS) File Storage: AWS S3 (avatars, backgrounds, weapons, banners) Admin Dashboard: Full CRUD, role-based access control Security: Spring Security, CORS, AWS WAF AWS Services Used\nAWS S3: store static files and media AWS Lambda: serverless backend AWS RDS: game data storage AWS CloudFront: SPA distribution AWS WAF: protect frontend \u0026amp; API AWS SES: email verification and password reset AWS IAM: manage access rights Component Design\nUser: register, login, manage account, avatar Admin: manage game data, CRUD, access control Game Data: characters, weapons, banners, echo, setecho, roles, elements, backgrounds Tools: gacha tracker, gacha simulator, banner/event timeline 4. Technical Implementation Implementation Phases\nRequirements analysis and architecture design Backend development with Spring Boot serverless, JWT, Google OAuth2, RDS integration Frontend development with React SPA, admin dashboard, gacha tools and timeline AWS deployment: S3, CloudFront, SPA fallback, WAF Testing, security optimization, documentation Technical Requirements\nFrontend: React, TypeScript, Vite, SPA fallback Backend: Spring Boot, Spring Security, JWT, Google OAuth2, Lambda serverless Database: SQL Server (RDS) Cloud: S3, CloudFront, Lambda, WAF, SES DevOps: Docker, CI/CD, AWS IAM 5. Timeline \u0026amp; Milestones (1 Month) Week Phase Tasks Week 1 Planning \u0026amp; Setup Requirement analysis, architecture, AWS preparation (S3, RDS, Lambda, WAF, CloudFront) Week 2 Backend Development Build backend, JWT, Google OAuth2, API endpoints, connect RDS, deploy Lambda \u0026amp; WAF Week 3 Frontend Development Build React SPA, admin dashboard, gacha tools, timeline banner/event Week 4 Deployment \u0026amp; Testing Deploy SPA to S3 + CloudFront, SPA fallback, WAF, testing, optimize security, complete documentation 6. Budget Estimation (summary) AWS Lambda ‚Äî Memory: 3008 MB ~ $5/month. S3 Standard ‚Äî 10 GB storage ~ $0.23/month. CloudFront ‚Äî 100 GB egress ~ $8.50/month. RDS (SQL Server, production) ~ $60+/month. SES ‚Äî Approx: ~ $0.01/month. CloudWatch ‚Äî Logs/metrics (10 GB logs) ~ $5/month. AWS WAF ‚Äî Web ACL + 1M requests ~ $10/month. Route53 ‚Äî 1 hosted zone + 1M queries ~ $0.90/month. Total = $89.64/month 7. Risk Assessment AWS downtime: Medium impact, low probability Security attack: High impact, low probability Increased costs: Medium impact, medium probability Data errors due to admin input: Medium impact, low probability Mitigation strategies\nUse AWS WAF, IAM for access control, HTTPS, secure JWT Monitor costs with CloudWatch, optimize queries and caching Use RDS Multi-AZ, CloudFront CDN, fast rollback on errors Contingency plan\nRDS failover in case of incidents Rollback backend using Lambda Versioning 8. Expected Outcomes Stable, auto-scaling system with low cost Secure API, centralized and easy-to-manage data Easy to expand for more games and features Gacha tools and timeline help players track conveniently üîó Project Website: https://d2eu9it59oopt8.cloudfront.net/\r"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Learn about AWS S3 (Simple Storage Service) and CloudFront (CDN) services. Understand S3 bucket creation, permissions, and storage classes. Practice uploading files to S3 and configuring public/private access. Learn about CloudFront distributions to deliver content globally. Understand versioning, lifecycle policies, and basic security best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review S3 concepts: bucket, object, storage classes, permissions 10/11/2025 10/11/2025 AWS Docs, FCJ Playlist 2 Create S3 buckets and upload test files 11/11/2025 11/11/2025 AWS Docs, FCJ Playlist 3 Configure S3 permissions, public/private access, and versioning 12/11/2025 12/11/2025 AWS Docs, FCJ Playlist 4 Set up CloudFront distribution with S3 as origin 13/11/2025 13/11/2025 AWS Docs, FCJ Playlist 5 Test global content delivery and implement lifecycle policies \u0026amp; security best practices 14/11/2025 14/11/2025 AWS Docs, FCJ Playlist Week 10 Achievements: Understood AWS S3 and CloudFront, their use cases, and advantages.\nSuccessfully created S3 buckets and uploaded files with correct permissions.\nConfigured CloudFront distribution to deliver content globally with caching.\nLearned about versioning, lifecycle policies, and basic security best practices for S3.\nGained hands-on experience in managing storage and content delivery on AWS.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Learn about AWS Lambda and serverless computing. Understand how to create Lambda functions, triggers, and manage versions. Learn about API Gateway to expose Lambda functions as HTTP endpoints. Practice integrating Lambda with S3 and other AWS services. Understand basic monitoring and logging with CloudWatch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review serverless concepts and Lambda architecture 17/11/2025 17/11/2025 AWS Docs, FCJ Playlist 2 Create simple Lambda functions and test execution 18/11/2025 18/11/2025 AWS Docs, FCJ Playlist 3 Set up triggers for Lambda (S3 event, API Gateway, CloudWatch event) 19/11/2025 19/11/2025 AWS Docs, FCJ Playlist 4 Configure API Gateway to expose Lambda functions as REST endpoints 20/11/2025 20/11/2025 AWS Docs, FCJ Playlist 5 Monitor Lambda execution with CloudWatch logs and metrics 21/11/2025 21/11/2025 AWS Docs, FCJ Playlist Week 11 Achievements: Understood serverless architecture and AWS Lambda concepts. Created Lambda functions and tested execution successfully. Set up triggers from S3, API Gateway, and CloudWatch events. Exposed Lambda functions via API Gateway endpoints. Monitored Lambda executions with CloudWatch and learned basic troubleshooting. "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Review and consolidate all AWS knowledge learned during the internship. Learn basic automation using AWS CloudFormation and Infrastructure as Code (IaC). Deploy simple stacks with CloudFormation. Review Lambda, S3, EC2, and API Gateway integration. Evaluate internship outcomes and prepare final report. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Review all AWS services and concepts studied during internship 24/11/2025 24/11/2025 AWS Docs, FCJ materials 2 Learn CloudFormation basics and template syntax 25/11/2025 25/11/2025 AWS Docs, FCJ Playlist 3 Deploy a simple CloudFormation stack to create EC2 + S3 resources 26/11/2025 26/11/2025 AWS Docs, FCJ Playlist 4 Integrate Lambda and API Gateway into CloudFormation stack 27/11/2025 27/11/2025 AWS Docs, FCJ Playlist 5 Test automation, troubleshoot, and prepare summary of all accomplishments 28/11/2025 28/11/2025 AWS Docs, FCJ Playlist Week 12 Achievements: Consolidated AWS knowledge and practical skills gained during internship. Learned basic automation and Infrastructure as Code (IaC) using CloudFormation. Deployed a CloudFormation stack with EC2, S3, Lambda, and API Gateway. Tested automation, monitored resources, and practiced troubleshooting. Completed internship report and evaluation. "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.3-create-ec2/","title":"Create EC2 instance","tags":[],"description":"","content":"Creating an EC2 Instance √¢‚Äû¬π√Ø¬∏¬è **‚ÑπÔ∏è\\ Information**: Amazon EC2 (Elastic Compute Cloud) provides scalable computing capacity in the AWS Cloud, eliminating the need to invest in hardware upfront.\nTo create a Linux EC2 instance using the AWS Management Console, follow these instructions. This guide helps you quickly launch your first instance with essential configurations. For advanced options, refer to the Launch Instance documentation.\nAccess the AWS Console Open a web browser and navigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/. Launch Your Instance On the EC2 console dashboard, locate the Launch instance box, select Launch instance, then choose Launch instance from the dropdown menu.\nConfigure Instance Details Under the Name and tags section, enter a descriptive name for your instance.\nUnder Application and OS Images (Amazon Machine Image), configure the following:\nSelect Quick Start, then choose Amazon Linux From the Amazon Machine Image (AMI) options, select an HVM version of Amazon Linux 2023 √∞≈∏‚Äô¬° Pro Tip: Look for AMIs marked as Free tier eligible to avoid unexpected charges if you\u0026rsquo;re using the AWS Free Tier.\nUnder Instance type, select t2.micro (pre-selected by default).\n√¢‚Äû¬π√Ø¬∏¬è **‚ÑπÔ∏è\\ Information**: The t2.micro instance type qualifies for the AWS Free Tier. In regions where t2.micro isn\u0026rsquo;t available, you can use t3.micro under the Free Tier. For more details, see AWS Free Tier.\nUnder Key pair (login), select the key pair you created during your AWS setup.\n√¢≈°¬†√Ø¬∏¬è **‚ö†Ô∏è\\ Warning**: Do not select Proceed without a key pair (Not recommended). Without a key pair, you won\u0026rsquo;t be able to connect to your instance.\nUnder Network settings, click Edit and configure your security group:\nYou can use the auto-created security group, or Select Select existing security group and choose a security group you created previously √∞≈∏‚Äù‚Äô Security Note: Security groups act as virtual firewalls that control inbound and outbound traffic to your instance. Ensure your security group allows SSH access (port 22) from your IP address only.\nLaunch and Verify Your Instance Review the instance configuration summary in the Summary panel.\nWhen ready, click Launch instance.\nOn the confirmation page, click View all instances to return to the EC2 console.\nMonitor the instance status on the Instances screen:\nInitial state: pending Running state: running (with assigned public DNS name) √∞≈∏‚Äô¬° Pro Tip: If the Public IPv4 DNS column is hidden, click the gear icon (Settings) in the upper right corner, enable Public IPv4 DNS, and click Confirm.\nWait for the instance to pass all status checks before attempting to connect.\nConnecting to Your EC2 Instance via SSH Using MobaXterm √¢‚Äû¬π√Ø¬∏¬è **‚ÑπÔ∏è\\ Information**: MobaXterm is an enhanced terminal for Windows with an X11 server, tabbed SSH client, and various network tools.\nFollow these steps to connect to your EC2 instance using MobaXterm:\nInstall MobaXterm Download MobaXterm from the official website: MobaXterm Website Install the application on your computer Configure SSH Connection Launch MobaXterm\nClick the Session icon in the upper-left corner\nIn the configuration window, enter:\nRemote Host: Your EC2 instance\u0026rsquo;s public IP address or DNS name Port: 22 (default SSH port) Username: The default user for your AMI (typically ec2-user for Amazon Linux) Advanced SSH settings: Browse and select your private key file (.pem) Connect to Your Instance Click OK to save the configuration Click the connect icon to establish an SSH connection √∞≈∏‚Äù‚Äô Security Note: Ensure your private key file (.pem) has restricted permissions. On Windows, verify the file is not accessible to other users.\nSuccessful Connection Once connected, you\u0026rsquo;ll have terminal access to your EC2 instance and can begin managing your server.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.2-prerequiste/3-create-db-sg/","title":"Create RDS Security Group","tags":[],"description":"","content":"Creating a Security Group for Amazon RDS **‚ÑπÔ∏è\\ Information**: Security groups act as virtual firewalls for your Amazon RDS instances, controlling inbound and outbound traffic at the instance level. Each security group contains a set of rules that filter traffic based on protocol, port, and source or destination.\nFollow these steps to create a dedicated security group for your Amazon RDS database instance:\nNavigate to the Amazon VPC console and select Security Groups from the navigation pane.\nClick Create Security Group to create a new security group specifically for your RDS database instance.\nIn the Basic details section:\nEnter a descriptive Security group name (e.g., \u0026ldquo;RDS-MySQL-SG\u0026rdquo;) Provide a meaningful Description (e.g., \u0026ldquo;Security group for RDS MySQL database instances\u0026rdquo;) Select your VPC from the dropdown menu Ensure the VPC you created earlier is selected to associate the security group with your network environment.\nConfigure Inbound rules to control which traffic sources can access your database:\nSelect MySQL/Aurora from the Type dropdown (automatically sets port 3306) For Source, select the security group ID of your EC2 instances that need to connect to the database √∞≈∏‚Äù‚Äô Security Note: Specifying the EC2 security group as the source rather than an IP range ensures only instances with that security group can connect to your database, enhancing security.\nReview your settings and click Create Security Group to complete the process. √∞≈∏‚Äô¬° Pro Tip: You can modify security group rules at any time, and the changes take effect immediately. This allows you to adjust access controls as your application requirements evolve.\n**‚ö†Ô∏è\\ Warning**: It is a best practice to use separate security groups for your RDS instances and EC2 instances. This separation provides better security isolation and makes it easier to manage permissions for each resource type independently.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.2-prerequiste/4-create-db-subnetgroup/","title":"Create DB Subnet Group","tags":[],"description":"","content":"Creating a DB Subnet Group for Amazon RDS **‚ÑπÔ∏è\\ Information**: A DB subnet group is a collection of subnets that you designate for your Amazon RDS database instances within a VPC. DB subnet groups enable you to specify particular subnets and IP ranges where Amazon RDS can deploy database instances, ensuring proper network isolation and availability.\nFollow these steps to create a DB subnet group:\nNavigate to the AWS Management Console and sign in to your account.\nSearch for and select RDS under services.\nIn the navigation pane, select Subnet groups.\nClick Create DB Subnet Group.\nIn the Create DB Subnet Group page, configure the basic details: Enter a descriptive Name for your subnet group (e.g., \u0026ldquo;rds-subnet-group\u0026rdquo;) Provide a meaningful Description (e.g., \u0026ldquo;Subnet group for RDS instances\u0026rdquo;) Select the VPC you created earlier from the dropdown menu In the Add subnets section:\nSelect at least two different Availability Zones to enable Multi-AZ deployments Choose the appropriate Subnets from each Availability Zone (typically private subnets for production databases) √∞≈∏‚Äù‚Äô Security Note: For enhanced security, place your RDS instances in private subnets that don\u0026rsquo;t have direct internet access.\nClick Create to create your DB subnet group.\n**‚ö†Ô∏è\\ Warning**: A DB subnet group must include subnets in at least two different Availability Zones to support Multi-AZ deployments. Without this configuration, you won\u0026rsquo;t be able to enable the Multi-AZ feature for your RDS instances.\n√∞≈∏‚Äô¬° Pro Tip: If you\u0026rsquo;ve enabled AWS Local Zones in your account, you can also select an Availability Zone group on the Create DB Subnet Group page. In this case, select the Availability Zone group, the corresponding Availability Zones, and appropriate subnets.\nAfter creation, your new DB subnet group will appear in the list of DB subnet groups in the RDS console. You can select it to view detailed information, including all associated subnets, in the details panel at the bottom of the window.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.4-create-rds/","title":"Create RDS database instance","tags":[],"description":"","content":"Install Git on Amazon EC2 2023 Below are instructions for installing Git on an Amazon EC2 virtual machine running Amazon Linux 2023 using basic steps.\nUpdate System Packages First, update your system packages to make sure you\u0026rsquo;re using the latest version:\nsudo dnf update -y Find Git Packages. Use the following command to find Git packages in the repository: sudo dnf search git Install Git. Once you find the Git package, you can install it with the following command: sudo dnf install git -y Verify Git Settings. Finally, check the Git version was successfully installed: git --version If you see the Git version appear, it means the installation is complete.\nInstall Node.js on Amazon EC2 Linux 2023 Below is a Bash script to install Node.js on Amazon EC2 Linux: #!/bin/bash\r# Color for formatting\rGREEN=\u0026#39;\\033[0;32m\u0026#39;\rNC=\u0026#39;\\033[0m\u0026#39; # Colorless\r# Check if NVM is installed\rif ! command -v nvm \u0026amp;\u0026gt; /dev/null; then\r# Step 1: Install nvm\rcurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\rsource ~/.nvm/nvm.sh\rfi\r# Verify nvm installation\rnvm --version\r# Install the LTS version of Node.js\rnvm install --lts\r# Use the installed LTS version\rnvm use --lts\r# Verify Node.js and npm installation\rnode -v\rnpm -v\r# Step 4: Create package.json file (if it doesn\u0026#39;t exist yet)\rif [ ! -f package.json ]; then\rnpm init -y\recho -e **${GREEN}Created file package.json.${NC}**\rfi\r#Step 5: Install necessary npm packages\recho -e **Installing required npm packages...**\rnpm install express dotenv express-handlebars body-parser mysql\r#Step 6: Install nodemon as a development dependency\recho -e **Installing nodemon as a development dependency...**\rnpm install --save-dev nodemon\rnpm install -g nodemon\r# Step 7: Add npm start script to package.json\rif ! grep -q \u0026#39;**start**:\u0026#39; package.json; then\rnpm set-script start **index.js** # Replace **your-app.js** with your entry point file\recho -e **${GREEN}Added npm start script to package.json.${NC}**\rfi\recho -e **${GREEN}Installation completed. You can now start building and running your Node.js application using \u0026#39;npm start\u0026#39;.${NC}** Creating a DB Instance on AWS To create a DB Instance on AWS, you can use the AWS Management Console with the option of Easy create either enabled or disabled. When Easy create is enabled, you only need to specify the DB engine type, DB Instance size, and DB Instance identifier. Easy create uses default settings for other configuration options. When Easy create is disabled, you need to specify more configuration options when creating a database, including options for availability, security, backups, and maintenance.\nNote: In the procedure below, the Standard create option is enabled, and Easy create is not. This procedure uses MySQL as an example.\nFor an example using Easy create to guide you in creating and connecting sample DB Instances for each engine, see Getting Started with Amazon RDS.\nTo create a DB Instance:\nSign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.\nIn the upper right corner of the Amazon RDS console, select the AWS region where you want to create the DB Instance.\nIn the navigation pane, choose Databases.\nChoose Create database, and then select Standard create.\nFor Engine type, choose MariaDB, Microsoft SQL Server, MySQL, Oracle, or PostgreSQL. In this example, we are using Microsoft SQL Server.\nFor Database management type, if you are using Oracle or SQL Server, select Amazon RDS or Amazon RDS Custom.\nFor Edition, if you are using Oracle or SQL Server, select the version of the DB engine you want to use.\nFor Version, select the engine version.\nIn the Templates section, choose a template that matches your use case. If you choose Production, the following options will be pre-selected in the next step:\nMulti-AZ failover option Provisioned IOPS SSD (io1) storage option Protection against deletion option We recommend using these features for a production environment.\nNote: Template choices may vary by version.\nTo enter your master password, follow these steps:\nIn the Settings section, open Credential Settings. If you want to specify a password, uncheck the Auto generate a password box if it\u0026rsquo;s already selected. (Optional) Change the Master username value. Enter the same password in both Master password and Confirm password. (Optional) Set up a connection to a compute resource for this DB Instance. You can configure the connection between an Amazon EC2 instance and the new DB Instance during the DB Instance creation process. For more information, see Configuring Automatic Network Connectivity to an EC2 Instance.\nIn the Connectivity section under VPC security group (firewall), if you choose Create new, a VPC security group with a login rule allowing your local computer\u0026rsquo;s IP address to access the database will be created.\nFor the remaining sections, specify your DB Instance settings. For more information on each setting, see Settings for DB Instances.\nChoose Create database.\nIf you choose to use an automatically generated password, the View credential details button will appear on the Databases page.\nTo view the master username and password for the DB Instance, select View credential details.\nTo connect to the DB Instance using the master username, use the displayed username and password.\nImportant: You cannot review the master user password. If you don\u0026rsquo;t record it, you may need to change it. If you need to change the master user password after the DB Instance is available, you can modify the DB Instance to do this. For more information on modifying a DB Instance, see Modifying an Amazon RDS DB Instance.\nUnder Databases, select the name of the new DB Instance. On the RDS console, information about the new DB Instance will appear. The DB Instance will have a Creating status until it is created and ready for use. Once the status changes to Available, you can connect to the DB Instance. Depending on the DB Instance class and allocated storage, it may take a few minutes for the new DB Instance to be available. Check RDS In the details page of the RDS instance, you can find connection-related information such as Endpoint, Port, and Username. The Endpoint is the URL or IP address you use to connect to the RDS database. Viewing Logs and Events on AWS RDS To monitor Logs and Events on Amazon RDS (Relational Database Service), you can follow these steps:\nSign in to the AWS Management Console.\nChoose the Amazon RDS service from the AWS dashboard.\nSelect the RDS instance you want to view Logs and Events for.\nIn the instance details page, you will see the following tabs:\nDB instance details: Displays basic information about the instance. Configuration: Allows you to view and modify the instance\u0026rsquo;s configuration. Log \u0026amp; events: This is where you can view Logs and Events. Click on the Log \u0026amp; events tab. Here, you can view various logs such as:\nError log: Records errors that occur on the instance. General log: Records general activities on the instance. Slow query log: Records slow queries. Event log: Displays important events related to the instance. You can customize settings for viewing Logs and Events here, such as the time range you want to view logs or setting up email notifications for important events.\nRemember to regularly review logs and events to monitor the status of your Amazon RDS instance and detect any issues early.\nViewing Maintenance and Backups on AWS RDS In Amazon Web Services (AWS), Amazon Relational Database Service (RDS) provides an easy-to-manage relational database with automation for various tasks, including maintenance and backups. Here\u0026rsquo;s how you can view information about maintenance and backups in AWS RDS:\nViewing Maintenance Information To view information about maintenance for a DB instance in RDS, you can follow these steps:\nSign in to the AWS Management Console.\nChoose the Amazon RDS service from the list of services.\nIn the RDS dashboard, select the DB instance you are interested in.\nIn the DB instance management page, navigate to the Maintenance \u0026amp; backups tab.\nHere, you will see information about the maintenance schedule, including the times when the DB instance will be automatically backed up and maintenance tasks will be performed. You can also view the history of previous maintenance events.\nViewing Backup Information To view information about backups of a DB instance in AWS RDS, follow these steps:\nSign in to the AWS Management Console.\nChoose the Amazon RDS service from the list of services.\nIn the RDS dashboard, select the DB instance you want to check.\nIn the DB instance management page, navigate to the Maintenance \u0026amp; backups tab.\nHere, you can view information about automatic backups and manual backups. You can also configure and manage backup settings.\nMake sure to follow relevant rules and policies when performing any tasks on AWS RDS to ensure the security and safety of your data.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/5.5-deploy-app/","title":"Application Deployment","tags":[],"description":"","content":"Deploy the application To clone the repository from GitHub of AWS-First-Cloud-Journey, you can use the following command: git clone https://github.com/AWS-First-Cloud-Journey/AWS-FCJ-Management #Instructions for installing Node.js on Amazon Linux 2023 Below is a Bash script to install Node.js on Amazon Linux. Please copy and execute the following steps:\n#!/bin/bash\r# C√°c m√†u cho ƒë·ªãnh d·∫°ng\rGREEN=\u0026#39;\\033[0;32m\u0026#39;\rNC=\u0026#39;\\033[0m\u0026#39; # Kh√¥ng m√†u\r# Ki·ªÉm tra xem NVM ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ch∆∞a\rif ! command -v nvm \u0026amp;\u0026gt; /dev/null; then\r# B∆∞·ªõc 1: C√†i ƒë·∫∑t nvm\rcurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\rsource ~/.nvm/nvm.sh\rfi\r# X√°c minh vi·ªác c√†i ƒë·∫∑t nvm\rnvm --version\r# C√†i ƒë·∫∑t phi√™n b·∫£n LTS c·ªßa Node.js\rnvm install --lts\r# S·ª≠ d·ª•ng phi√™n b·∫£n LTS ƒë√£ c√†i ƒë·∫∑t\rnvm use --lts\r# X√°c minh c√†i ƒë·∫∑t Node.js v√† npm\rnode -v\rnpm -v\r# B∆∞·ªõc 4: T·∫°o t·ªáp package.json (n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i)\rif [ ! -f package.json ]; then\rnpm init -y\recho -e \u0026#34;${GREEN}ƒê√£ t·∫°o t·ªáp package.json.${NC}\u0026#34;\rfi\r# B∆∞·ªõc 5: C√†i ƒë·∫∑t c√°c g√≥i npm c·∫ßn thi·∫øt\recho -e \u0026#34;ƒêang c√†i ƒë·∫∑t c√°c g√≥i npm c·∫ßn thi·∫øt...\u0026#34;\rnpm install express dotenv express-handlebars body-parser mysql\r# B∆∞·ªõc 6: C√†i ƒë·∫∑t nodemon nh∆∞ m·ªôt ph·∫ßn ph√°t tri·ªÉn\recho -e \u0026#34;ƒêang c√†i ƒë·∫∑t nodemon nh∆∞ m·ªôt ph·∫ßn ph√°t tri·ªÉn...\u0026#34;\rnpm install --save-dev nodemon\rnpm install -g nodemon\r# B∆∞·ªõc 7: Th√™m script npm start v√†o package.json\rif ! grep -q \u0026#39;\u0026#34;start\u0026#34;:\u0026#39; package.json; then\rnpm set-script start \u0026#34;index.js\u0026#34; # Thay th·∫ø \u0026#34;your-app.js\u0026#34; b·∫±ng t·ªáp ƒëi·ªÉm nh·∫≠p c·ªßa b·∫°n\recho -e \u0026#34;${GREEN}ƒê√£ th√™m script npm start v√†o package.json.${NC}\u0026#34;\rfi\recho -e \u0026#34;${GREEN}C√†i ƒë·∫∑t ho√†n t·∫•t. B√¢y gi·ªù b·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu x√¢y d·ª±ng v√† ch·∫°y ·ª©ng d·ª•ng Node.js c·ªßa m√¨nh b·∫±ng \u0026#39;npm start\u0026#39;.${NC}\u0026#34; This is a Bash script used to install and configure MySQL server on a system. This script performs the following steps: Set variables with MySQL RPM path and database information such as RDS address, database name, username and password.\nCheck if the MySQL community repository RPM already exists in the current directory. If it does not exist, it will download the RPM from the specified URL.\nInstall RPM of MySQL community repository and MySQL Server.\nStart the MySQL server and configure it to automatically start with the system.\nCheck the installed MySQL version.\nSecure the MySQL server with the mysql_secure_installation command.\nCreate or update an .env file with database information (address, database name, username, and password).\nConnect to MySQL server with credentials and you can add specific SQL commands here.\nNote: To execute this script, you need to have sudo permissions and make sure you have provided the correct database information (RDS Endpoint, database name, username and password) before run script.\n#!/bin/bash\r# Set variables for MySQL RPM and database information\rMYSQL_RPM_URL=\u0026#34;https://dev.mysql.com/get/mysql80-community-release-el9-1.noarch.rpm\u0026#34;\rDB_HOST=\u0026#34;RDS Endpoint\u0026#34;\rDB_NAME=\u0026#34;Database name\u0026#34;\rDB_USER=\u0026#34;Database username\u0026#34;\rDB_PASS=\u0026#34;Database password\u0026#34;\r# Check if MySQL Community repository RPM already exists\rif [ ! -f mysql80-community-release-el9-1.noarch.rpm ]; then\rsudo wget $MYSQL_RPM_URL\rfi\r# Install MySQL Community repository\rsudo dnf install -y mysql80-community-release-el9-1.noarch.rpm\r# Install MySQL server\rsudo dnf install -y mysql-community-server\r# Start MySQL server\rsudo systemctl start mysqld\r# Enable MySQL to start on boot\rsudo systemctl enable mysqld\r# Check MySQL version\rmysql -V\r# Secure the MySQL server\rsudo mysql_secure_installation\r# Create or update the .env file with database information\recho \u0026#34;DB_HOST=$DB_HOST\u0026#34; \u0026gt;\u0026gt; .env\recho \u0026#34;DB_NAME=$DB_NAME\u0026#34; \u0026gt;\u0026gt; .env\recho \u0026#34;DB_USER=$DB_USER\u0026#34; \u0026gt;\u0026gt; .env\recho \u0026#34;DB_PASS=$DB_PASS\u0026#34; \u0026gt;\u0026gt; .env\r# Connect to MySQL and create a new database (you might want to add specific SQL commands here)\rmysql -h $DB_HOST -P 3306 -u $DB_USER -p$DB_PASS Create Database and Table in AWS RDS After successfully connecting to RDS (Relational Database Service) on AWS, we can create a new database and define a table in it using the following SQL script.\nCreate Database First, we will create a new database if it does not exist yet. Use the following command:\nCREATE DATABASE IF NOT EXISTS first_cloud_users; This command checks whether the database \u0026ldquo;first_cloud_users\u0026rdquo; exists or not. If it does not exist, it will create a new database named \u0026ldquo;first_cloud_users\u0026rdquo;.\nUsing Database Next, we use the \u0026ldquo;first_cloud_users\u0026rdquo; database using the command:\nUSE first_cloud_users; This command indicates that all SQL commands will then be executed in the \u0026ldquo;first_cloud_users\u0026rdquo; database.\nCreate Table \u0026ldquo;user\u0026rdquo; We have created the database and used it. Now, we will define a \u0026ldquo;user\u0026rdquo; table in this database using the following SQL script:\nCREATE TABLE `user`\r(\r`id` INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\r`first_name` VARCHAR(45) NOT NULL,\r`last_name` VARCHAR(45) NOT NULL,\r`email` VARCHAR(100) NOT NULL UNIQUE,\r`phone` VARCHAR(15) NOT NULL,\r`comments` TEXT NOT NULL,\r`status` ENUM(\u0026#39;active\u0026#39;, \u0026#39;inactive\u0026#39;) NOT NULL DEFAULT \u0026#39;active\u0026#39;\r) ENGINE = InnoDB; This command defines the structure of the \u0026ldquo;user\u0026rdquo; table with columns such as \u0026ldquo;id\u0026rdquo;, \u0026ldquo;first_name\u0026rdquo;, \u0026ldquo;last_name\u0026rdquo;, \u0026ldquo;email\u0026rdquo;, \u0026ldquo;phone\u0026rdquo;, \u0026ldquo;comments\u0026rdquo;, and \u0026ldquo;status\u0026rdquo;. These columns represent information about the user, and the \u0026ldquo;id\u0026rdquo; column is set as the auto-incrementing primary key.\nAdd Data to Table \u0026ldquo;user\u0026rdquo; Finally, we can add data to the \u0026ldquo;user\u0026rdquo; table using the INSERT INTO command. Here is an example that adds some records to a table:\nINSERT INTO `user`\r(`first_name`, `last_name`, `email`, `phone`, `comments`, `status`)\rVALUES\r(\u0026#39;Amanda\u0026#39;, \u0026#39;Nunes\u0026#39;, \u0026#39;anunes@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Alexander\u0026#39;, \u0026#39;Volkanovski\u0026#39;, \u0026#39;avolkanovski@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Khabib\u0026#39;, \u0026#39;Nurmagomedov\u0026#39;, \u0026#39;knurmagomedov@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Kamaru\u0026#39;, \u0026#39;Usman\u0026#39;, \u0026#39;kusman@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Israel\u0026#39;, \u0026#39;Adesanya\u0026#39;, \u0026#39;iadesanya@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Henry\u0026#39;, \u0026#39;Cejudo\u0026#39;, \u0026#39;hcejudo@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Valentina\u0026#39;, \u0026#39;Shevchenko\u0026#39;, \u0026#39;vshevchenko@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Tyron\u0026#39;, \u0026#39;Woodley\u0026#39;, \u0026#39;twoodley@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Rose\u0026#39;, \u0026#39;Namajunas\u0026#39;, \u0026#39;rnamajunas@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Tony\u0026#39;, \u0026#39;Ferguson\u0026#39;, \u0026#39;tferguson@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Jorge\u0026#39;, \u0026#39;Masvidal\u0026#39;, \u0026#39;jmasvidal@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Nate\u0026#39;, \u0026#39;Diaz\u0026#39;, \u0026#39;ndiaz@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Conor\u0026#39;, \u0026#39;McGregor\u0026#39;, \u0026#39;cmcGregor@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Cris\u0026#39;, \u0026#39;Cyborg\u0026#39;, \u0026#39;ccyborg@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Tecia\u0026#39;, \u0026#39;Torres\u0026#39;, \u0026#39;ttorres@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Ronda\u0026#39;, \u0026#39;Rousey\u0026#39;, \u0026#39;rrousey@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Holly\u0026#39;, \u0026#39;Holm\u0026#39;, \u0026#39;hholm@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;),\r(\u0026#39;Joanna\u0026#39;, \u0026#39;Jedrzejczyk\u0026#39;, \u0026#39;jjedrzejczyk@ufc.com\u0026#39;, \u0026#39;012345 678910\u0026#39;, \u0026#39;I love AWS FCJ\u0026#39;, \u0026#39;active\u0026#39;); This command adds user records to the \u0026ldquo;user\u0026rdquo; table with information such as name, email, phone number, comments, and a default status of \u0026ldquo;active\u0026rdquo;.\nHere\u0026rsquo;s how to create and manage a database and tables in AWS RDS using SQL script.\nsome SQL commands to check database information in a database management system (DBMS) such as MySQL or PostgreSQL: Display a list of all databases: SHOW DATABASES; This command will list all available databases in the system.\nChoose a specific database to work with: USE database_name; This command will move you from your current database to a database named \u0026ldquo;database_name\u0026rdquo;. After using this command, all subsequent SQL commands will apply to this database.\nDisplay tables in the current database: SHOW TABLES; This command will list all the tables present in the current database.\nShows the structure of a specific table: DESCRIBE table_name; This command will tell you the structure of the table named \u0026ldquo;table_name\u0026rdquo;, including the column name, data type, and other column properties.\nDisplay information about database size: SELECT table_schema \u0026#34;Database Name\u0026#34;, SUM(data_length + index_length) / 1024 / 1024 \u0026#34;Database Size (MB)\u0026#34;\rFROM information_schema.tables\rGROUP BY table_schema; This command will display information about the size of the databases in the system, in Megabytes (MB).\nRemember to replace \u0026ldquo;database_name\u0026rdquo; and \u0026ldquo;table_name\u0026rdquo; with the specific names of the database and table you want to test. These commands help you manage and examine information about your database.\nOnce you are in the application directory, run the following command to start the application using npm start: npm start Check EC2 Instance status: Make sure your EC2 Instance is running and functioning properly. Test the application in the browser: Open a web browser and enter the IP address or domain name of the EC2 Instance, followed by port 5000 (for example: http://\u0026lt;IP address or domain name\u0026gt; :5000). This will make a connection to your application running on port 5000.\nTest results: The browser will display your application if everything is configured correctly and the EC2 Instance is working. If not, you need to recheck the previous steps to identify the problem and fix it.\nhttp://\u0026lt;IP address or domain name\u0026gt;:5000 Monitoring AWS RDS On the AWS RDS interface, you can perform the following steps to monitor:\nSelect Databases. Choose the DB instance you\u0026rsquo;ve created. Select Monitoring. To view information about backups of the DB instance in AWS RDS, follow these steps:\nLog in to the AWS Management Console. Select the \u0026ldquo;Amazon RDS\u0026rdquo; service from the list of services. In the RDS dashboard, choose the DB instance you want to check. On the DB instance management page, navigate to the \u0026ldquo;Maintenance \u0026amp; backups\u0026rdquo; tab. Here, you can view information about automatic and manual backups. You can also configure and manage backup settings. View Snapshot information.\nChoose the DB snapshot you want to restore.\nIn the Actions section, select Restore snapshot. On the Restore snapshot page, enter a name for the DB instance you want to restore in the DB instance identifier field.\nSelect other settings such as allocated memory size.\nFor more information on each setting, refer to Settings for DB instances.\nFinally, select Restore DB instance.\nComplete the restore snapshot process.\nCheck the restored database instance.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/","title":"Workshop: Get Started with Amazon RDS","tags":[],"description":"","content":"Amazon Relational Database Service (Amazon RDS) Overview of Amazon RDS ‚ÑπÔ∏è Information: Amazon Relational Database Service (Amazon RDS) is a managed service that allows you to deploy and manage relational databases on AWS. Amazon RDS is designed for online transaction processing (OLTP) and is best suited for structured, relational data storage requirements.\nAmazon RDS provides key benefits:\nEasy replacement for traditional database instances Automated backups and patching during customer-defined maintenance windows One-click scaling, replication, and availability Supported Database Engines Amazon RDS supports the following database engines:\nAmazon Aurora MySQL MariaDB Oracle SQL Server PostgreSQL ‚ö†Ô∏è Warning: RDS is a managed service and you don\u0026rsquo;t have root access to the underlying EC2 server. The exception is Amazon RDS Custom, which allows access to the underlying operating system but is only available for a limited set of DB Engines.\nStorage Options General Purpose SSD (gp3/gp2): Cost-effective storage providing 3 IOPS/GB baseline performance Provisioned IOPS SSD (io1/io2): High-performance storage with customizable IOPS for I/O-intensive workloads Magnetic Storage: Legacy option with limited performance (not recommended for new deployments) üí° Pro Tip: Choose General Purpose SSD for most workloads, and Provisioned IOPS only when you need consistent I/O performance for database-intensive applications.\nHigh Availability and Disaster Recovery Multi-AZ Deployments: Synchronous standby replica in a different Availability Zone for automatic failover Read Replicas: Asynchronous replication for read scaling and potential disaster recovery Global Databases: Cross-region replication with fast local reads and disaster recovery capabilities üîí Security Note: Multi-AZ deployments enhance both availability and data durability, with automatic failover typically completing within 60-120 seconds.\nSecurity Features Encryption at rest: Using AWS KMS keys (applies to DB instances, backups, snapshots, and replicas) Network isolation: Using Amazon VPC for network-level isolation Resource-level permissions: Using IAM policies SSL/TLS encryption: For data in transit Database authentication: Using database engine native authentication or IAM authentication Scalability and Limits Storage can be scaled up (not down) without downtime Compute resources can be modified with a brief downtime during the change Maximum storage: 64 TiB for most engines (16 TiB for SQL Server) Maximum database connections: Varies by engine and instance size üí° Pro Tip: Plan your initial storage carefully as you can only scale up. Consider using Aurora for more flexible scaling options.\nBackup and Recovery Automated backups: Point-in-time recovery for up to 35 days Manual snapshots: User-initiated backups that persist until explicitly deleted Snapshot export to S3: For long-term retention or analysis When to Use Amazon RDS Amazon RDS is ideal for:\nTraditional relational database workloads Applications requiring SQL query capabilities Structured data with well-defined schemas OLTP workloads with predictable scaling needs ‚ÑπÔ∏è Information: For unstructured data, high-scale requirements, or specialized workloads, consider alternative AWS database services like DynamoDB, DocumentDB, or purpose-built databases.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/6-backup/","title":"Backup and Restore","tags":[],"description":"","content":"Understanding Amazon RDS Backup and Restore ‚ÑπÔ∏è **‚ÑπÔ∏è\\ Information**: Amazon RDS provides automated backups and allows manual snapshots to ensure your database data is protected and can be recovered when needed. These capabilities are essential for disaster recovery planning and maintaining business continuity.\nMonitoring Backup Status in Amazon RDS To access backup monitoring in the Amazon RDS console:\nNavigate to the Databases section in the AWS Management Console Select your target DB instance Click on the Monitoring tab to view performance metrics Viewing Backup Information To review backup details for your Amazon RDS instance:\nSign in to the AWS Management Console Select Amazon RDS from the services menu In the RDS dashboard, select your DB instance Navigate to the Maintenance \u0026amp; backups tab Here you can view both automated and manual backup information and configure backup settings Managing Database Snapshots View your available DB snapshots in the snapshots section:\nRestoring from a DB Snapshot Select the DB snapshot you want to use as your restore point:\nUnder the Actions dropdown menu, select Restore snapshot Configure your restored database instance:\nProvide a unique DB instance identifier for the new instance Select appropriate instance specifications (compute, storage, etc.) Configure network and security settings üí° Pro Tip: When restoring a production database, consider restoring to a smaller instance class first for testing, then scale up as needed to minimize costs during validation.\nInitiate the restore process:\nReview your configuration settings Click Restore DB instance to begin the process ‚ö†Ô∏è **‚ö†Ô∏è\\ Warning**: The restore process creates a completely new database instance with its own endpoint. You will need to update your application connection strings to point to this new instance if you intend to use it for production.\nVerifying the Restored Database Confirm successful restoration:\nCheck that the new DB instance appears in your RDS console Verify the status shows as \u0026ldquo;Available\u0026rdquo; when the restore completes Test connectivity and data integrity before directing production traffic to the restored instance üîí Security Note: Remember to configure the same security groups and parameter groups as your original instance if you want identical access controls and database settings.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/5-workshop/7-cleanup/","title":"Clean up resources","tags":[],"description":"","content":"Resource Cleanup ‚ÑπÔ∏è **‚ÑπÔ∏è\\ Information**: After completing your lab, it\u0026rsquo;s important to clean up all AWS resources to avoid ongoing charges. Follow these steps to properly remove all resources created during this workshop.\nDelete Database Resources Delete the DB subnet group:\nNavigate to the Amazon RDS console In the navigation pane, select Subnet groups Select the DB subnet group related to the lab Choose Delete, then confirm by selecting Delete in the confirmation window Delete DB Instance:\nAccess the RDS Management Console In the left navigation bar, select Databases Select the DB Instance related to the lab Click Actions, then Delete Uncheck Create final snapshot? and acknowledge that automated backups will no longer be available Enter delete me in the confirmation field Click Delete Delete DB Snapshots:\nIn the RDS Management Console, select Snapshots from the navigation bar Select all snapshots related to the lab Click Actions, then Delete snapshot Confirm by clicking Delete Delete Network Resources Delete security groups:\nOpen the Amazon VPC console Choose Security Groups from the navigation pane Select the security group related to the lab Choose Actions, select Delete security groups, then confirm Delete NAT gateway:\nIn the VPC console, select NAT Gateways Select the NAT Gateway related to the lab Choose Actions, select Delete NAT gateway Confirm the deletion Release Elastic IP addresses:\nOpen the Amazon EC2 console Select Elastic IPs from the navigation pane Select the Elastic IP address related to the lab From Actions, select Release Elastic IP addresses Confirm by choosing Release Delete the VPC:\nIn the VPC console, select Your VPCs Select the VPC you created for this lab From Actions, select Delete VPC On the confirmation page, enter delete and choose Delete Delete Compute Resources Terminate EC2 instances: Access the EC2 Management Console Select Instances from the navigation pane Select all EC2 Instances related to the lab Click Instance state, then Terminate instance Confirm by clicking Terminate ‚ö†Ô∏è **‚ö†Ô∏è\\ Warning**: Terminating resources is permanent and cannot be undone. Ensure you have backed up any important data before proceeding with cleanup.\nüí° Pro Tip: To verify all resources have been properly deleted, check your AWS Billing dashboard or use AWS Cost Explorer to ensure no unexpected charges appear after cleanup.\n"},{"uri":"https://alicealicek2304.github.io/fcj-workshop/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://alicealicek2304.github.io/fcj-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://alicealicek2304.github.io/fcj-workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]